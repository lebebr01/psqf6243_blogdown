<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lecture Notes | PSQF 6243</title>
    <link>https://psqf6243.brandonlebeau.org/lectures/</link>
      <atom:link href="https://psqf6243.brandonlebeau.org/lectures/index.xml" rel="self" type="application/rss+xml" />
    <description>Lecture Notes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 22 Aug 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://psqf6243.brandonlebeau.org/media/blue-balloon.jpg</url>
      <title>Lecture Notes</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/</link>
    </image>
    
    <item>
      <title>Review</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/01-review/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/01-review/</guid>
      <description>&lt;h1 id=&#34;review-for-psqf-6243&#34;&gt;Review for PSQF 6243&lt;/h1&gt;
&lt;p&gt;This serves as a non-exhaustive review for the course. These are elements that I assume you have knowledge of prior to starting the course.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variable vs constant attributes&lt;/li&gt;
&lt;li&gt;Types of variables (ie., nominal, ordinal, integer, ratio)&lt;/li&gt;
&lt;li&gt;Descriptive Statistics (eg., mean, median, standard deviation, variance, percentiles)&lt;/li&gt;
&lt;li&gt;Higher order moments (eg., skewness and kurtosis)&lt;/li&gt;
&lt;li&gt;Exploring/summarizing univariate distributions (eg., histogram or density figure)&lt;/li&gt;
&lt;li&gt;What is a statistical model? Why do we use them?&lt;/li&gt;
&lt;li&gt;Population vs Sample&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Mario Kart 64 world record data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;track&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Track name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;type&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single or three lap record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;shortcut&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Shortcut or non-shortcut record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;player&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Player’s name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;system_played&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Used system (NTSC or PAL)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;World record date&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time_period&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;period&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time as &lt;code&gt;hms&lt;/code&gt; period&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time in seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;record_duration&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Record duration in days&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load some libraries
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(lubridate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;lubridate&#39;
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     date, intersect, setdiff, union
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following object is masked from &#39;package:scales&#39;:
## 
##     rescale
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(e1071)

theme_set(theme_bw(base_size = 18))

# load in some data
mariokart &amp;lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv&#39;) %&amp;gt;%
    mutate(year = year(date),
           month = month(date),
           day = month(date))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2334 Columns: 9
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): track, type, shortcut, player, system_played, time_period
## dbl  (2): time, record_duration
## date (1): date
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(mariokart)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 12
##   track      type  short…¹ player syste…² date       time_…³  time recor…⁴  year
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Luigi Rac… Thre… No      Salam  NTSC    1997-02-15 2M 12.…  133.       1  1997
## 2 Luigi Rac… Thre… No      Booth  NTSC    1997-02-16 2M 9.9…  130.       0  1997
## 3 Luigi Rac… Thre… No      Salam  NTSC    1997-02-16 2M 8.9…  129.      12  1997
## 4 Luigi Rac… Thre… No      Salam  NTSC    1997-02-28 2M 6.9…  127.       7  1997
## 5 Luigi Rac… Thre… No      Gregg… NTSC    1997-03-07 2M 4.5…  125.      54  1997
## 6 Luigi Rac… Thre… No      Rocky… NTSC    1997-04-30 2M 2.8…  123.       0  1997
## # … with 2 more variables: month &amp;lt;dbl&amp;gt;, day &amp;lt;dbl&amp;gt;, and abbreviated variable
## #   names ¹​shortcut, ²​system_played, ³​time_period, ⁴​record_duration
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# univariate distribution of time
gf_histogram(~ time, data = mariokart, bins = 30) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ time, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_stats(~ time, data = mariokart, mean, median, sd, skewness, kurtosis, quantile(probs = c(0.1, 0.5, 0.9)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response     mean median      sd skewness kurtosis   10%   50%     90%
## 1     time 90.62383  86.19 66.6721 1.771732 3.844745 31.31 86.19 171.961
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;bivariate-association&#34;&gt;Bivariate Association&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(time ~ record_duration, data = mariokart)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.06736739
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(time ~ record_duration, data = mariokart) %&amp;gt;%
  gf_labs(x = &amp;quot;How long the record was held&amp;quot;,
          y = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;What is problematic about the analyses above? Why?&lt;/li&gt;
&lt;li&gt;What could be done to improve the analyses above?&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Review - Class</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/01-review-class/</link>
      <pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/01-review-class/</guid>
      <description>&lt;h1 id=&#34;review-for-psqf-6243&#34;&gt;Review for PSQF 6243&lt;/h1&gt;
&lt;p&gt;This serves as a non-exhaustive review for the course. These are elements that I assume you have knowledge of prior to starting the course.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variable vs constant attributes&lt;/li&gt;
&lt;li&gt;Types of variables (ie., nominal, ordinal, integer, ratio)&lt;/li&gt;
&lt;li&gt;Descriptive Statistics (eg., mean, median, standard deviation, variance, percentiles)&lt;/li&gt;
&lt;li&gt;Higher order moments (eg., skewness and kurtosis)&lt;/li&gt;
&lt;li&gt;Exploring/summarizing univariate distributions (eg., histogram or density figure)&lt;/li&gt;
&lt;li&gt;What is a statistical model? Why do we use them?&lt;/li&gt;
&lt;li&gt;Population vs Sample&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Mario Kart 64 world record data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;track&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Track name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;type&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Single or three lap record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;shortcut&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;factor&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Shortcut or non-shortcut record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;player&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Player’s name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;system_played&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Used system (NTSC or PAL)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;World record date&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time_period&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;period&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time as &lt;code&gt;hms&lt;/code&gt; period&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time in seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;record_duration&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Record duration in days&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# load some libraries
library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(lubridate)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;lubridate&#39;
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     date, intersect, setdiff, union
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following object is masked from &#39;package:scales&#39;:
## 
##     rescale
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(e1071)

theme_set(theme_bw(base_size = 18))

# load in some data
mariokart &amp;lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv&#39;) %&amp;gt;%
    mutate(year = year(date),
           month = month(date),
           day = month(date))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2334 Columns: 9
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): track, type, shortcut, player, system_played, time_period
## dbl  (2): time, record_duration
## date (1): date
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(mariokart)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 12
##   track      type  short…¹ player syste…² date       time_…³  time recor…⁴  year
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Luigi Rac… Thre… No      Salam  NTSC    1997-02-15 2M 12.…  133.       1  1997
## 2 Luigi Rac… Thre… No      Booth  NTSC    1997-02-16 2M 9.9…  130.       0  1997
## 3 Luigi Rac… Thre… No      Salam  NTSC    1997-02-16 2M 8.9…  129.      12  1997
## 4 Luigi Rac… Thre… No      Salam  NTSC    1997-02-28 2M 6.9…  127.       7  1997
## 5 Luigi Rac… Thre… No      Gregg… NTSC    1997-03-07 2M 4.5…  125.      54  1997
## 6 Luigi Rac… Thre… No      Rocky… NTSC    1997-04-30 2M 2.8…  123.       0  1997
## # … with 2 more variables: month &amp;lt;dbl&amp;gt;, day &amp;lt;dbl&amp;gt;, and abbreviated variable
## #   names ¹​shortcut, ²​system_played, ³​time_period, ⁴​record_duration
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# univariate distribution of time
gf_histogram(~ time, data = mariokart, bins = 30, color = &#39;black&#39;) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_histogram(~ time, fill = ~ shortcut, data = mariokart, bins = 30, color = &#39;black&#39;) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ time, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ time, color = &#39;black&#39;, fill = ~ shortcut, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_stats(~ time, data = mariokart, mean, median, sd, skewness, kurtosis, quantile(probs = c(0.1, 0.5, 0.9)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response     mean median      sd skewness kurtosis   10%   50%     90%
## 1     time 90.62383  86.19 66.6721 1.771732 3.844745 31.31 86.19 171.961
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;count(mariokart, track, shortcut)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 28 × 3
##    track                 shortcut     n
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
##  1 Banshee Boardwalk     No          83
##  2 Bowser&#39;s Castle       No          69
##  3 Choco Mountain        No          77
##  4 Choco Mountain        Yes         71
##  5 D.K.&#39;s Jungle Parkway No         106
##  6 D.K.&#39;s Jungle Parkway Yes         74
##  7 Frappe Snowland       No          93
##  8 Frappe Snowland       Yes         87
##  9 Kalimari Desert       No         102
## 10 Kalimari Desert       Yes         67
## # … with 18 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mariokart |&amp;gt; 
  filter(track == &#39;Choco Mountain&#39;) |&amp;gt; 
  gf_density(~ time, color = &#39;black&#39;, fill = ~ shortcut, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;) |&amp;gt;
   gf_facet_wrap(~ type, scale = &#39;free_x&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;count(mariokart, track, system_played)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 × 3
##    track                 system_played     n
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt;
##  1 Banshee Boardwalk     NTSC             21
##  2 Banshee Boardwalk     PAL              62
##  3 Bowser&#39;s Castle       NTSC             18
##  4 Bowser&#39;s Castle       PAL              51
##  5 Choco Mountain        NTSC             56
##  6 Choco Mountain        PAL              92
##  7 D.K.&#39;s Jungle Parkway NTSC             47
##  8 D.K.&#39;s Jungle Parkway PAL             133
##  9 Frappe Snowland       NTSC             65
## 10 Frappe Snowland       PAL             115
## # … with 22 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mariokart |&amp;gt; 
  filter(track == &#39;Moo Moo Farm&#39;, type == &#39;Single Lap&#39;) |&amp;gt; 
  gf_density(~ time, color = &#39;black&#39;, fill = ~ system_played, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;count(mariokart, type)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   type           n
##   &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
## 1 Single Lap  1123
## 2 Three Lap   1211
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(mariokart)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [2,334 × 12] (S3: tbl_df/tbl/data.frame)
##  $ track          : chr [1:2334] &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; ...
##  $ type           : chr [1:2334] &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; ...
##  $ shortcut       : chr [1:2334] &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; ...
##  $ player         : chr [1:2334] &amp;quot;Salam&amp;quot; &amp;quot;Booth&amp;quot; &amp;quot;Salam&amp;quot; &amp;quot;Salam&amp;quot; ...
##  $ system_played  : chr [1:2334] &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; ...
##  $ date           : Date[1:2334], format: &amp;quot;1997-02-15&amp;quot; &amp;quot;1997-02-16&amp;quot; ...
##  $ time_period    : chr [1:2334] &amp;quot;2M 12.99S&amp;quot; &amp;quot;2M 9.99S&amp;quot; &amp;quot;2M 8.99S&amp;quot; &amp;quot;2M 6.99S&amp;quot; ...
##  $ time           : num [1:2334] 133 130 129 127 125 ...
##  $ record_duration: num [1:2334] 1 0 12 7 54 0 0 27 0 64 ...
##  $ year           : num [1:2334] 1997 1997 1997 1997 1997 ...
##  $ month          : num [1:2334] 2 2 2 2 3 4 4 4 5 5 ...
##  $ day            : num [1:2334] 2 2 2 2 3 4 4 4 5 5 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;bivariate-association&#34;&gt;Bivariate Association&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(time ~ record_duration, data = mariokart) |&amp;gt;
  round(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.07
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(time ~ record_duration, data = mariokart) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;) |&amp;gt;
  gf_labs(x = &amp;quot;How long the record was held&amp;quot;,
          y = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mariokart %&amp;gt;%
  group_by(track, type, shortcut, system_played) %&amp;gt;%
summarise(correlation = cor(time ~ record_duration),
         num = n()) %&amp;gt;%
arrange(correlation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &#39;track&#39;, &#39;type&#39;, &#39;shortcut&#39;. You can
## override using the `.groups` argument.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 112 × 6
## # Groups:   track, type, shortcut [56]
##    track                 type       shortcut system_played correlation   num
##    &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1 Rainbow Road          Three Lap  No       NTSC               -1.00      3
##  2 Rainbow Road          Single Lap No       NTSC               -0.914     4
##  3 Rainbow Road          Single Lap Yes      NTSC               -0.914     4
##  4 Sherbet Land          Three Lap  Yes      NTSC               -0.779     5
##  5 Wario Stadium         Three Lap  Yes      PAL                -0.682     7
##  6 Luigi Raceway         Three Lap  Yes      PAL                -0.630     9
##  7 Moo Moo Farm          Single Lap No       NTSC               -0.582    19
##  8 Choco Mountain        Three Lap  No       PAL                -0.527    31
##  9 D.K.&#39;s Jungle Parkway Three Lap  No       NTSC               -0.495     5
## 10 Yoshi Valley          Three Lap  Yes      PAL                -0.469    10
## # … with 102 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mariokart %&amp;gt;%
filter(track == &#39;Mario Raceway&#39;,
       type == &#39;Three Lap&#39;,
       shortcut == &#39;No&#39;,
       system_played == &#39;NTSC&#39;) %&amp;gt;%
       gf_point(time ~ record_duration) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;) |&amp;gt;
  gf_labs(x = &amp;quot;How long the record was held&amp;quot;,
          y = &amp;quot;Time (in seconds)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review-class_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;str(mariokart)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [2,334 × 12] (S3: tbl_df/tbl/data.frame)
##  $ track          : chr [1:2334] &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; &amp;quot;Luigi Raceway&amp;quot; ...
##  $ type           : chr [1:2334] &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; &amp;quot;Three Lap&amp;quot; ...
##  $ shortcut       : chr [1:2334] &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; ...
##  $ player         : chr [1:2334] &amp;quot;Salam&amp;quot; &amp;quot;Booth&amp;quot; &amp;quot;Salam&amp;quot; &amp;quot;Salam&amp;quot; ...
##  $ system_played  : chr [1:2334] &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; &amp;quot;NTSC&amp;quot; ...
##  $ date           : Date[1:2334], format: &amp;quot;1997-02-15&amp;quot; &amp;quot;1997-02-16&amp;quot; ...
##  $ time_period    : chr [1:2334] &amp;quot;2M 12.99S&amp;quot; &amp;quot;2M 9.99S&amp;quot; &amp;quot;2M 8.99S&amp;quot; &amp;quot;2M 6.99S&amp;quot; ...
##  $ time           : num [1:2334] 133 130 129 127 125 ...
##  $ record_duration: num [1:2334] 1 0 12 7 54 0 0 27 0 64 ...
##  $ year           : num [1:2334] 1997 1997 1997 1997 1997 ...
##  $ month          : num [1:2334] 2 2 2 2 3 4 4 4 5 5 ...
##  $ day            : num [1:2334] 2 2 2 2 3 4 4 4 5 5 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;What is problematic about the analyses above? Why?&lt;/li&gt;
&lt;li&gt;What could be done to improve the analyses above?&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Linear Regression</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/</guid>
      <description>&lt;h1 id=&#34;introduction-to-linear-regression&#34;&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 &lt;strong&gt;continuous&lt;/strong&gt; attributes. One of those attributes will be the &lt;em&gt;outcome&lt;/em&gt; or &lt;em&gt;attribute of interest&lt;/em&gt; whereas the other will used as a &lt;em&gt;predictor&lt;/em&gt;. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable &lt;strong&gt;explains variation&lt;/strong&gt; in the dependent variable (more on this later).&lt;/p&gt;
&lt;p&gt;Of note, variable is a typical word used in statistics, I&amp;rsquo;ve come to like the word &lt;strong&gt;attribute&lt;/strong&gt; instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.&lt;/p&gt;
&lt;p&gt;We may write this general model as:&lt;/p&gt;
&lt;p&gt;$$
Y = \beta_{0} + \beta_{1} X + \epsilon
$$&lt;/p&gt;
&lt;p&gt;Where &lt;code&gt;\(Y\)&lt;/code&gt; is the outcome attribute. It is also known as the dependent variable. The &lt;code&gt;\(X\)&lt;/code&gt; term is the predictor/covariate attribute. It is also known as the independent variable. The &lt;code&gt;\(\epsilon\)&lt;/code&gt; is a random error term, more on this later. Finally, &lt;code&gt;\(\beta_{0}\)&lt;/code&gt; and &lt;code&gt;\(\beta_{1}\)&lt;/code&gt; are unknown population coefficients that we are interested in estimating. More on this later too.&lt;/p&gt;
&lt;h2 id=&#34;specific-example&#34;&gt;Specific example&lt;/h2&gt;
&lt;p&gt;The data used for this section of the course is from the 2019 WNBA season. These data are part of the &lt;a href=&#34;https://www.bayesrulesbook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;bayesrules&lt;/em&gt; package/book&lt;/a&gt;. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)

basketball &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 146 Columns: 32
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (2): player_name, team
## dbl (29): height, weight, year, age, games_played, games_started, avg_minute...
## lgl  (1): starter
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(basketball)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 32
##   player_name    height weight  year team    age games…¹ games…² avg_m…³ avg_f…⁴
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Natalie Achon…     75    190  2019 IND      26      30      18    21.2     3.3
## 2 Kayla Alexand…     76    195  2019 CHI      28       3       0     6.7     1  
## 3 Rebecca Allen      74    162  2019 NYL      26      24       2    17.2     2.7
## 4 Jillian Alley…     74    193  2019 MIN      24       5       0     2.8     0.4
## 5 Kristine Anig…     76    200  2019 TOT      22      27       0     9.3     0.8
## 6 Kristine Anig…     76    200  2019 CON      22      17       0     7.1     0.6
## # … with 22 more variables: avg_field_goal_attempts &amp;lt;dbl&amp;gt;,
## #   field_goal_pct &amp;lt;dbl&amp;gt;, avg_three_pointers &amp;lt;dbl&amp;gt;,
## #   avg_three_pointer_attempts &amp;lt;dbl&amp;gt;, three_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_two_pointers &amp;lt;dbl&amp;gt;, avg_two_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   two_pointer_pct &amp;lt;dbl&amp;gt;, avg_free_throws &amp;lt;dbl&amp;gt;,
## #   avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.&lt;/p&gt;
&lt;p&gt;One first step in an analysis would be to explore each distribution independently first. I&amp;rsquo;m going to leave that as an exercise for you to do on your own.&lt;/p&gt;
&lt;p&gt;The next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;questions-to-consider&#34;&gt;Questions to consider&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What can be noticed about the relationship between these two attributes?&lt;/li&gt;
&lt;li&gt;Does there appear to be a relationship between the two?&lt;/li&gt;
&lt;li&gt;Is this relationship perfect?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;adding-a-smoother-line&#34;&gt;Adding a smoother line&lt;/h2&gt;
&lt;p&gt;Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.&lt;/p&gt;
&lt;p&gt;Does there appear to be much difference in the relationship across the two lines?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;estimating-linear-regression-coefficients&#34;&gt;Estimating linear regression coefficients&lt;/h2&gt;
&lt;p&gt;The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is &lt;code&gt;lm()&lt;/code&gt; to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.&lt;/p&gt;
&lt;p&gt;This equation could be written more directly for our specific problem.&lt;/p&gt;
&lt;p&gt;$$
Avg_points = \beta_{0} + \beta_{1} Minutes_Played + \epsilon
$$&lt;/p&gt;
&lt;p&gt;One way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.&lt;/p&gt;
&lt;p&gt;For the R formula, instead of an $ = $, you could insert a ~.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wnba_reg &amp;lt;- lm(avg_points ~ total_minutes, data = basketball)
coef(wnba_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) total_minutes 
##    1.13562456    0.01014207
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;interpretting-linear-regression-terms&#34;&gt;Interpretting linear regression terms&lt;/h2&gt;
&lt;p&gt;Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:&lt;/p&gt;
&lt;p&gt;$$
\hat{avg_points} = 1.1356 + .0101 min_played
$$&lt;/p&gt;
&lt;p&gt;Where instead of &lt;code&gt;\(\beta_{0}\)&lt;/code&gt; or &lt;code&gt;\(\beta_{1}\)&lt;/code&gt;, the estimated values from this single season were inserted. Note the &lt;code&gt;\(\hat{avg\_points}\)&lt;/code&gt;, which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1356
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1457
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 100
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.1456
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * mean(basketball$avg_points)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.189732
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 5000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.6356
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * -50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6306
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.&lt;/p&gt;
&lt;p&gt;First, for the intercept ($\beta_{0}$), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.&lt;/p&gt;
&lt;p&gt;The slope, ($\beta_{1}$), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.&lt;/p&gt;
&lt;p&gt;The predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-about-the-error&#34;&gt;What about the error?&lt;/h2&gt;
&lt;p&gt;So far the error has been disregarded, but where did it go? The error didn&amp;rsquo;t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?&lt;/p&gt;
&lt;p&gt;The short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, &lt;code&gt;\(\sigma^2\)&lt;/code&gt;. Sometimes this is written in math notation as:&lt;/p&gt;
&lt;p&gt;$$
\epsilon \sim N(0, \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;From this notation, can you see why the error was disregarded earlier when generating predictions?&lt;/p&gt;
&lt;p&gt;In short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.&lt;/p&gt;
&lt;h3 id=&#34;estimating-error-in-linear-regression&#34;&gt;Estimating error in linear regression&lt;/h3&gt;
&lt;p&gt;This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the &lt;em&gt;sum of squares regression&lt;/em&gt; (SSR), the portion that is unexplained is referred to as the &lt;em&gt;sum of squares error&lt;/em&gt; (SSE). This could be written in math notation as:&lt;/p&gt;
&lt;p&gt;$$
\sum (Y - \bar{Y})^2 = \sum (Y - \hat{Y})^2 + \sum (\hat{Y} - \bar{Y})^2
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to visualize what this means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = &#39;#FF7F7F&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &#39;#65a765&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &#39;#FFD580&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;another-related-measure-of-error&#34;&gt;Another related measure of error&lt;/h2&gt;
&lt;p&gt;Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.&lt;/p&gt;
&lt;p&gt;$$
R^{2} = 1 - \frac{SS_{res}}{SS_{total}}
$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
R^{2} = \frac{SS_{reg}}{SS_{total}}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the sum of square and get a value for &lt;code&gt;\(R^2\)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;basketball %&amp;gt;%
summarise(ss_total = sum((avg_points - mean(avg_points))^2),
          ss_error = sum((avg_points - fitted(wnba_reg))^2),
          ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %&amp;gt;%
mutate(r_square = 1 - ss_error / ss_total,
       r_square2 = ss_reg / ss_total)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   ss_total ss_error ss_reg r_square r_square2
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    2004.     564.  1440.    0.719     0.719
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(wnba_reg)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7185315
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(wnba_reg)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sigma_hat_square &amp;lt;- 563.9929 / (nrow(basketball) - 2)
sigma_hat &amp;lt;- sqrt(sigma_hat_square)

sigma_hat_square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916617
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sigma_hat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Linear Regression - Class Notes</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class/</link>
      <pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class/</guid>
      <description>&lt;h1 id=&#34;introduction-to-linear-regression&#34;&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 &lt;strong&gt;continuous&lt;/strong&gt; attributes. One of those attributes will be the &lt;em&gt;outcome&lt;/em&gt; or &lt;em&gt;attribute of interest&lt;/em&gt; whereas the other will used as a &lt;em&gt;predictor&lt;/em&gt;. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable &lt;strong&gt;explains variation&lt;/strong&gt; in the dependent variable (more on this later).&lt;/p&gt;
&lt;p&gt;Of note, variable is a typical word used in statistics, I&amp;rsquo;ve come to like the word &lt;strong&gt;attribute&lt;/strong&gt; instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.&lt;/p&gt;
&lt;p&gt;We may write this general model as:&lt;/p&gt;
&lt;p&gt;$$
Y = \beta_{0} + \beta_{1} X + \epsilon
$$&lt;/p&gt;
&lt;p&gt;Where &lt;code&gt;\(Y\)&lt;/code&gt; is the outcome attribute. It is also known as the dependent variable. The &lt;code&gt;\(X\)&lt;/code&gt; term is the predictor/covariate attribute. It is also known as the independent variable. The &lt;code&gt;\(\epsilon\)&lt;/code&gt; is a random error term, more on this later. Finally, &lt;code&gt;\(\beta_{0}\)&lt;/code&gt; and &lt;code&gt;\(\beta_{1}\)&lt;/code&gt; are unknown population coefficients that we are interested in estimating. More on this later too.&lt;/p&gt;
&lt;h2 id=&#34;specific-example&#34;&gt;Specific example&lt;/h2&gt;
&lt;p&gt;The data used for this section of the course is from the 2019 WNBA season. These data are part of the &lt;a href=&#34;https://www.bayesrulesbook.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;bayesrules&lt;/em&gt; package/book&lt;/a&gt;. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)

basketball &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 146 Columns: 32
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (2): player_name, team
## dbl (29): height, weight, year, age, games_played, games_started, avg_minute...
## lgl  (1): starter
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(basketball)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 32
##   player_name    height weight  year team    age games…¹ games…² avg_m…³ avg_f…⁴
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Natalie Achon…     75    190  2019 IND      26      30      18    21.2     3.3
## 2 Kayla Alexand…     76    195  2019 CHI      28       3       0     6.7     1  
## 3 Rebecca Allen      74    162  2019 NYL      26      24       2    17.2     2.7
## 4 Jillian Alley…     74    193  2019 MIN      24       5       0     2.8     0.4
## 5 Kristine Anig…     76    200  2019 TOT      22      27       0     9.3     0.8
## 6 Kristine Anig…     76    200  2019 CON      22      17       0     7.1     0.6
## # … with 22 more variables: avg_field_goal_attempts &amp;lt;dbl&amp;gt;,
## #   field_goal_pct &amp;lt;dbl&amp;gt;, avg_three_pointers &amp;lt;dbl&amp;gt;,
## #   avg_three_pointer_attempts &amp;lt;dbl&amp;gt;, three_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_two_pointers &amp;lt;dbl&amp;gt;, avg_two_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   two_pointer_pct &amp;lt;dbl&amp;gt;, avg_free_throws &amp;lt;dbl&amp;gt;,
## #   avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.&lt;/p&gt;
&lt;p&gt;One first step in an analysis would be to explore each distribution independently first. I&amp;rsquo;m going to leave that as an exercise for you to do on your own.&lt;/p&gt;
&lt;p&gt;The next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) |&amp;gt;
  gf_smooth(method = &#39;lm&#39;, size = 4) |&amp;gt;
  gf_smooth(linetype = 2, color = &#39;black&#39;, size = 4) |&amp;gt;
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;loess&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;filter(basketball, avg_points &amp;gt; 15)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 32
##   player_name    height weight  year team    age games…¹ games…² avg_m…³ avg_f…⁴
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Arike Ogunbow…     68    165  2019 DAL      22      33      28    32.1     6.5
## # … with 22 more variables: avg_field_goal_attempts &amp;lt;dbl&amp;gt;,
## #   field_goal_pct &amp;lt;dbl&amp;gt;, avg_three_pointers &amp;lt;dbl&amp;gt;,
## #   avg_three_pointer_attempts &amp;lt;dbl&amp;gt;, three_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_two_pointers &amp;lt;dbl&amp;gt;, avg_two_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   two_pointer_pct &amp;lt;dbl&amp;gt;, avg_free_throws &amp;lt;dbl&amp;gt;,
## #   avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;questions-to-consider&#34;&gt;Questions to consider&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;What can be noticed about the relationship between these two attributes?&lt;/li&gt;
&lt;li&gt;Does there appear to be a relationship between the two?&lt;/li&gt;
&lt;li&gt;Is this relationship perfect?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;adding-a-smoother-line&#34;&gt;Adding a smoother line&lt;/h2&gt;
&lt;p&gt;Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.&lt;/p&gt;
&lt;p&gt;Does there appear to be much difference in the relationship across the two lines?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  #gf_smooth() %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;,  color = &#39;lightblue&#39;, size = 4) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(avg_points ~ total_minutes, data = basketball)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8476624
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(color = &#39;black&#39;, size = 3) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, size = 4, color = &#39;blue&#39;)
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(simglm)

sim_args &amp;lt;- list(
    formula = y ~ 1 + poly(x1, degree = 2),
    fixed = list(x1 = list(var_type = &#39;continuous&#39;)),
    sample_size = 2000,
    reg_weights = c(1, 2, -200)
)

poly_data &amp;lt;- simulate_fixed(data = NULL, sim_args) |&amp;gt;
simulate_error(sim_args) |&amp;gt;
generate_response(sim_args)

head(poly_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X.Intercept.         x1_1         x1_2          x1 level1_id       error
## 1            1 -0.023738970  0.001264975 -1.05732784         1 -0.72911136
## 2            1  0.001855997 -0.015605511  0.05808737         2 -0.89038148
## 3            1  0.012546200 -0.010416012  0.52396082         3  0.35416269
## 4            1 -0.005732009 -0.014914073 -0.27259394         4 -0.95894820
## 5            1  0.001278362 -0.015680610  0.03291437         5 -0.04970669
## 6            1 -0.013338978 -0.010574816 -0.60410166         6  0.59216136
##   fixed_outcome random_effects           y
## 1      0.699527              0 -0.02958439
## 2      4.124814              0  3.23443267
## 3      3.108295              0  3.46245751
## 4      3.971351              0  3.01240240
## 5      4.138679              0  4.08897209
## 6      3.088285              0  3.68044656
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(y ~ x1, data = poly_data, size = 3) |&amp;gt;
  gf_smooth(method = &#39;lm&#39;, color = &#39;lightblue&#39;) |&amp;gt;
  gf_smooth(method = &#39;loess&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;estimating-linear-regression-coefficients&#34;&gt;Estimating linear regression coefficients&lt;/h2&gt;
&lt;p&gt;The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is &lt;code&gt;lm()&lt;/code&gt; to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.&lt;/p&gt;
&lt;p&gt;This equation could be written more directly for our specific problem.&lt;/p&gt;
&lt;p&gt;$$
Avg_points = \beta_{0} + \beta_{1} Minutes_Played + \epsilon
$$&lt;/p&gt;
&lt;p&gt;$$
Avg_points = 1.14 + .01 Minutes_Played + \epsilon
$$&lt;/p&gt;
&lt;p&gt;One way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.&lt;/p&gt;
&lt;p&gt;For the R formula, instead of an $ = $, you could insert a ~.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wnba_reg &amp;lt;- lm(avg_points ~ total_minutes, data = basketball)
coef(wnba_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) total_minutes 
##    1.13562456    0.01014207
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;basketball &amp;lt;- basketball |&amp;gt;
  mutate(minutes_100 = total_minutes / 100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ minutes_100, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  #gf_smooth() %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;,  color = &#39;lightblue&#39;, size = 4) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wnba_reg &amp;lt;- lm(avg_points ~ minutes_100, data = basketball)
coef(wnba_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) minutes_100 
##    1.135625    1.014207
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;interpretting-linear-regression-terms&#34;&gt;Interpretting linear regression terms&lt;/h2&gt;
&lt;p&gt;Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:&lt;/p&gt;
&lt;p&gt;$$
\hat{avg_points} = 1.1356 + .0101 min_played
$$&lt;/p&gt;
&lt;p&gt;Where instead of &lt;code&gt;\(\beta_{0}\)&lt;/code&gt; or &lt;code&gt;\(\beta_{1}\)&lt;/code&gt;, the estimated values from this single season were inserted. Note the &lt;code&gt;\(\hat{avg\_points}\)&lt;/code&gt;, which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1356
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1457
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 100
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.1456
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * mean(basketball$avg_points)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.189732
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * 5000
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.6356
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.1356 + .0101 * -50
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6306
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.&lt;/p&gt;
&lt;p&gt;First, for the intercept ($\beta_{0}$), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.&lt;/p&gt;
&lt;p&gt;The slope, ($\beta_{1}$), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.&lt;/p&gt;
&lt;p&gt;The predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-about-the-error&#34;&gt;What about the error?&lt;/h2&gt;
&lt;p&gt;So far the error has been disregarded, but where did it go? The error didn&amp;rsquo;t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?&lt;/p&gt;
&lt;p&gt;The short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, &lt;code&gt;\(\sigma^2\)&lt;/code&gt;. Sometimes this is written in math notation as:&lt;/p&gt;
&lt;p&gt;$$
\epsilon \sim N(0, \sigma^2)
$$&lt;/p&gt;
&lt;p&gt;From this notation, can you see why the error was disregarded earlier when generating predictions?&lt;/p&gt;
&lt;p&gt;In short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.&lt;/p&gt;
&lt;h3 id=&#34;estimating-error-in-linear-regression&#34;&gt;Estimating error in linear regression&lt;/h3&gt;
&lt;p&gt;This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the &lt;em&gt;sum of squares regression&lt;/em&gt; (SSR), the portion that is unexplained is referred to as the &lt;em&gt;sum of squares error&lt;/em&gt; (SSE). This could be written in math notation as:&lt;/p&gt;
&lt;p&gt;$$
\sum (Y - \bar{Y})^2 = \sum (Y - \hat{Y})^2 + \sum (\hat{Y} - \bar{Y})^2
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to visualize what this means.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = &#39;#FF7F7F&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &#39;#65a765&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, linetype = 2, color = &#39;lightblue&#39;) %&amp;gt;%
  gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &#39;#FFD580&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;another-related-measure-of-error&#34;&gt;Another related measure of error&lt;/h2&gt;
&lt;p&gt;Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.&lt;/p&gt;
&lt;p&gt;$$
R^{2} = 1 - \frac{SS_{res}}{SS_{total}}
$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
R^{2} = \frac{SS_{reg}}{SS_{total}}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute the sum of square and get a value for &lt;code&gt;\(R^2\)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;basketball %&amp;gt;%
summarise(ss_total = sum((avg_points - mean(avg_points))^2),
          ss_error = sum((avg_points - fitted(wnba_reg))^2),
          ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %&amp;gt;%
mutate(r_square = 1 - ss_error / ss_total,
       r_square2 = ss_reg / ss_total)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   ss_total ss_error ss_reg r_square r_square2
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    2004.     564.  1440.    0.719     0.719
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(wnba_reg)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7185315
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(wnba_reg)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sigma_hat_square &amp;lt;- 563.9929 / (nrow(basketball) - 2)
sigma_hat &amp;lt;- sqrt(sigma_hat_square)

sigma_hat_square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916617
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sigma_hat
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$$
\sigma^2 = \frac{SS_{error}}{n - 2}
$$&lt;/p&gt;
&lt;p&gt;$$
\sigma = \sqrt{\sigma^2}
$$&lt;/p&gt;
&lt;p&gt;$$
\hat{\sigma}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression Estimates</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/</guid>
      <description>&lt;h1 id=&#34;understanding-regression-parameters&#34;&gt;Understanding Regression Parameters&lt;/h1&gt;
&lt;p&gt;This section of notes aims to dig a bit more into what the simple linear regression (i.e., regression with a single continuous covariate/attribute) parameter estimates mean. We will consider the estimation formulas in part of this to gain a sense of how these can be computed.&lt;/p&gt;
&lt;h2 id=&#34;new-example-data&#34;&gt;New Example Data&lt;/h2&gt;
&lt;p&gt;The new data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;id&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;poc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;county&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h3&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;h2 id=&#34;bivariate-figure&#34;&gt;Bivariate Figure&lt;/h2&gt;
&lt;p&gt;Note, below I do a bit of post-processing to combine data from different POC values within a single CBSA.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following object is masked from &#39;package:scales&#39;:
## 
##     rescale
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&#39;cbsa_name&#39;, &#39;date&#39;)) %&amp;gt;% 
   drop_na()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 13
##   date           id   poc pm2.5 daily_aqi site_…¹ aqs_p…² cbsa_…³ cbsa_…⁴ county
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; 
## 1 1/1/21  190130009     1  15.1        57 Water … PM2.5 …   47940 Waterl… Black…
## 2 1/4/21  190130009     1  13.3        54 Water … PM2.5 …   47940 Waterl… Black…
## 3 1/7/21  190130009     1  20.5        69 Water … PM2.5 …   47940 Waterl… Black…
## 4 1/10/21 190130009     1  14.3        56 Water … PM2.5 …   47940 Waterl… Black…
## 5 1/13/21 190130009     1  13.7        54 Water … PM2.5 …   47940 Waterl… Black…
## 6 1/16/21 190130009     1   5.3        22 Water … PM2.5 …   47940 Waterl… Black…
## # … with 3 more variables: avg_wind &amp;lt;dbl&amp;gt;, max_wind &amp;lt;dbl&amp;gt;,
## #   max_wind_hours &amp;lt;dbl&amp;gt;, and abbreviated variable names ¹​site_name,
## #   ²​aqs_parameter_desc, ³​cbsa_code, ⁴​cbsa_name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4821   13
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(daily_aqi ~ avg_wind, data = airquality, size = 4, alpha = .15) %&amp;gt;%
  gf_labs(x = &amp;quot;Average daily wind speed (in knots)&amp;quot;,
          y = &amp;quot;Daily Air Quality&amp;quot;) %&amp;gt;%
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, color = &#39;lightblue&#39;, linetype = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;gam&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(daily_aqi ~ avg_wind, data = airquality) |&amp;gt; round(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.292
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm) |&amp;gt; round(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##      48.223      -2.212
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$r.square |&amp;gt; round(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.085
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$sigma |&amp;gt; round(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.055
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;interpreting-these-estimates&#34;&gt;Interpreting these estimates&lt;/h3&gt;
&lt;p&gt;What do these parameter estimates mean in this context?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; This is the model implied ______ when the ________ equals 0.&lt;br&gt;
&lt;strong&gt;Slope:&lt;/strong&gt; For each 1 unit change in ____ there is a -2.2 unit decrease in ___.&lt;br&gt;
&lt;strong&gt;R-Square:&lt;/strong&gt; The _____ in ____ is explained by ____. &lt;br&gt;
&lt;strong&gt;Sigma:&lt;/strong&gt; The _____ distance each point is from ____.&lt;/p&gt;
&lt;h2 id=&#34;centering-predictors&#34;&gt;Centering predictors&lt;/h2&gt;
&lt;p&gt;There are times when centering of predictors can be helpful for interpretation of the model parameters. This can be helpful when 0 is not a practically useful characteristic of the attribute or for more specific tests of certain elements of the X attribute.&lt;/p&gt;
&lt;h3 id=&#34;mean-centering&#34;&gt;Mean Centering&lt;/h3&gt;
&lt;p&gt;Mean centering is where the mean of the attribute is subtracted from each value. This is a linear transformation where each data point is subtracted by a constant, the mean. This means that the distance between points do not change.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality &amp;lt;- airquality %&amp;gt;%
  mutate(avg_wind_mc = avg_wind - mean(avg_wind),
         avg_wind_maxc = avg_wind - max(avg_wind),
         avg_wind_10 = avg_wind - 10)

gf_point(daily_aqi ~ avg_wind_mc, data = airquality, size = 4, alpha = .15) %&amp;gt;%
  gf_labs(x = &amp;quot;Average daily wind speed (in knots)&amp;quot;,
          y = &amp;quot;Daily Air Quality&amp;quot;) %&amp;gt;%
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;, color = &#39;lightblue&#39;, linetype = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &#39;gam&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm_mc &amp;lt;- lm(daily_aqi ~ avg_wind_mc, data = airquality)
coef(air_lm_mc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) avg_wind_mc 
##   38.788011   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_mc)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_mc)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm_maxc &amp;lt;- lm(daily_aqi ~ avg_wind_maxc, data = airquality)
coef(air_lm_maxc)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) avg_wind_maxc 
##      5.968391     -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_maxc)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_maxc)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm_10 &amp;lt;- lm(daily_aqi ~ avg_wind_10, data = airquality)
coef(air_lm_10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) avg_wind_10 
##   26.104968   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_10)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_10)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;standardized-regression&#34;&gt;Standardized Regression&lt;/h2&gt;
&lt;p&gt;Another type of regression that can be done is one in which the attributes are standardized prior to estimating the linear regression. What is meant by standardizing? This is converting the attributes into z-scores:&lt;/p&gt;
&lt;p&gt;$$
Z_{api} = \frac{(aqi - \bar{aqi})}{s_{aqi}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality &amp;lt;- airquality %&amp;gt;%
  mutate(z_aqi = scale(daily_aqi),
         z_aqi2 = (daily_aqi - mean(daily_aqi)) / sd(daily_aqi),
         z_wind = scale(avg_wind))

head(airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 19
##   date           id   poc pm2.5 daily_aqi site_…¹ aqs_p…² cbsa_…³ cbsa_…⁴ county
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; 
## 1 1/1/21  190130009     1  15.1        57 Water … PM2.5 …   47940 Waterl… Black…
## 2 1/4/21  190130009     1  13.3        54 Water … PM2.5 …   47940 Waterl… Black…
## 3 1/7/21  190130009     1  20.5        69 Water … PM2.5 …   47940 Waterl… Black…
## 4 1/10/21 190130009     1  14.3        56 Water … PM2.5 …   47940 Waterl… Black…
## 5 1/13/21 190130009     1  13.7        54 Water … PM2.5 …   47940 Waterl… Black…
## 6 1/16/21 190130009     1   5.3        22 Water … PM2.5 …   47940 Waterl… Black…
## # … with 9 more variables: avg_wind &amp;lt;dbl&amp;gt;, max_wind &amp;lt;dbl&amp;gt;,
## #   max_wind_hours &amp;lt;dbl&amp;gt;, avg_wind_mc &amp;lt;dbl&amp;gt;, avg_wind_maxc &amp;lt;dbl&amp;gt;,
## #   avg_wind_10 &amp;lt;dbl&amp;gt;, z_aqi &amp;lt;dbl[,1]&amp;gt;, z_aqi2 &amp;lt;dbl&amp;gt;, z_wind &amp;lt;dbl[,1]&amp;gt;, and
## #   abbreviated variable names ¹​site_name, ²​aqs_parameter_desc, ³​cbsa_code,
## #   ⁴​cbsa_name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm_s &amp;lt;- lm(z_aqi ~ z_wind, data = airquality)
coef(air_lm_s)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)        z_wind 
## -6.712023e-16 -2.920277e-01
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_s)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm_s)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9565091
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use this formula to convert any unstandardized regression coefficients into a standardized metric.&lt;/p&gt;
&lt;p&gt;$$
b^{&#39;}&lt;em&gt;{k} = b&lt;/em&gt;{k} * \frac{s_{x_{k}}}{s_{y}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;-2.211 * sd(airquality$avg_wind) / sd(airquality$daily_aqi)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2919224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cor(daily_aqi ~ avg_wind, data = airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2920277
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;parameter-estimation&#34;&gt;Parameter Estimation&lt;/h2&gt;
&lt;p&gt;Now that we looked how the parameters are impacted by some changes in the model specification, how are these parameters actually estimated? I will show two ways, one is general, the other is specific to this simple case with a single predictor/covariate attribute. In general, linear regression (or more generally the general linear model) uses least square estimation. This means that the the parameters in the model minimize the squared distance between the observed and predicted values. That is, least squares estimates minimize this criterion:&lt;/p&gt;
&lt;p&gt;$$
\sum (Y - \hat{Y})^2
$$&lt;/p&gt;
&lt;h3 id=&#34;specific-example&#34;&gt;Specific example&lt;/h3&gt;
&lt;p&gt;Calculus can be used to show that these two equations can be solved simultanuously to get estimates for &lt;code&gt;\(\beta_{0}\)&lt;/code&gt; and &lt;code&gt;\(\beta_{1}\)&lt;/code&gt; that minimize the criterion above. These formulas are:&lt;/p&gt;
&lt;p&gt;$$
b_{1} = \frac{\sum(X - \bar{X})(Y - \bar{Y})}{\sum(X - \bar{X})^2}
$$
$$
b_{0} = \bar{Y} - b_{1}\bar{X}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use R to get these quantities.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;b1 &amp;lt;- with(airquality, 
      sum((avg_wind - mean(avg_wind)) * (daily_aqi - mean(daily_aqi))) / sum((avg_wind - mean(avg_wind))^2)
)
b0 &amp;lt;- with(airquality, 
      mean(daily_aqi) - b1 * mean(avg_wind)
)
b0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 48.22295
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;b1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coef(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;general-approach&#34;&gt;General Approach&lt;/h3&gt;
&lt;p&gt;When there are more than one predictor, the number of equations gets a bit unyieldy, therefore, there is a general analytic approach that works for any set of predictor attributes. The general approach uses matrix algebra (anyone take linear algebra?), to achieve their estimates. This general form is:&lt;/p&gt;
&lt;p&gt;$$
\mathbf{b} = \left( \mathbf{X}^{&lt;code&gt;}\mathbf{X} \right)^{-1} \left( \mathbf{X}^{&lt;/code&gt;} \mathbf{Y} \right).
$$
Where &lt;code&gt;\(\mathbf{b}\)&lt;/code&gt; is a vector of estimated regression coefficients, &lt;code&gt;\(\mathbf{X}\)&lt;/code&gt; is a matrix of covariate/predictor attributes (called the design matrix), and &lt;code&gt;\(\mathbf{Y}\)&lt;/code&gt; is a vector of the outcome attribute.&lt;/p&gt;
&lt;p&gt;Below, I show what these would look like for the air quality example that has been used and solve for the regression coefficients.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;X &amp;lt;- model.matrix(air_lm)
head(X)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) avg_wind
## 1           1 2.941667
## 2           1 2.445833
## 3           1 1.995833
## 4           1 3.445833
## 5           1 1.116667
## 6           1 6.091667
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Y &amp;lt;- as.matrix(airquality$daily_aqi)
head(Y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   57
## [2,]   54
## [3,]   69
## [4,]   56
## [5,]   54
## [6,]   22
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;X_X &amp;lt;- solve(t(X) %*% X)
X_X
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               (Intercept)      avg_wind
## (Intercept)  0.0008152474 -1.424894e-04
## avg_wind    -0.0001424894  3.340328e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;X_Y &amp;lt;- t(X) %*% Y
X_Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]
## (Intercept) 186997
## avg_wind    731464
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;X_X %*% X_Y
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  [,1]
## (Intercept) 48.222946
## avg_wind    -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coef(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Least Squares Simulation</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/</guid>
      <description>&lt;h2 id=&#34;example-to-show-least-squares-minimization&#34;&gt;Example to show least squares minimization&lt;/h2&gt;
&lt;p&gt;This little example is meant as a way to show the least square really minimizes the criterion, $ \sum \left( Y - \hat{Y} \right)^2 $.&lt;/p&gt;
&lt;p&gt;In this example, we will generate some data so that we know what the truth is. Then, upon data generation, we will compute a bunch of different values for the linear slope and y-intercept. For each combination of the y-intercept and slope, I will compute the sum of squares error depicted above.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(simglm)

theme_set(theme_bw(base_size = 18))

set.seed(2021)

sim_arguments &amp;lt;- list(
    formula = y ~ x,
    fixed = list(x = list(var_type = &#39;continuous&#39;, mean = 100, sd = 20)),
    error = list(variance = 100),
    sample_size = 1000,
    reg_weights = c(5, .5)
)

sim_data &amp;lt;- simulate_fixed(data = NULL, sim_arguments) %&amp;gt;%
  simulate_error(sim_arguments) %&amp;gt;%
  generate_response(sim_arguments)

head(sim_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X.Intercept.         x level1_id       error fixed_outcome random_effects
## 1            1  97.55080         1   8.0959540      53.77540              0
## 2            1 111.04913         2   0.9941633      60.52457              0
## 3            1 106.97299         3 -23.4302131      58.48650              0
## 4            1 107.19264         4   6.7652598      58.59632              0
## 5            1 117.96107         5 -36.1147374      63.98054              0
## 6            1  61.54861         6  -1.6416799      35.77430              0
##          y
## 1 61.87135
## 2 61.51873
## 3 35.05628
## 4 65.36158
## 5 27.86580
## 6 34.13262
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;ggstance&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: scales
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;scales&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggridges
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(y ~ x, data = sim_data, size = 4) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim_lm &amp;lt;- lm (y ~ x, data = sim_data)
coef(sim_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           x 
##   5.9653484   0.4941165
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;y_intercept &amp;lt;- seq(0, 15, by = .25)
slope &amp;lt;- seq(0, 1.5, by = .01)

conditions &amp;lt;- rbind(expand.grid(y_intercept = y_intercept, 
                          slope = slope),
                          coef(sim_lm))

tail(conditions)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      y_intercept     slope
## 9207   14.000000 1.5000000
## 9208   14.250000 1.5000000
## 9209   14.500000 1.5000000
## 9210   14.750000 1.5000000
## 9211   15.000000 1.5000000
## 9212    5.965348 0.4941165
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(conditions)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9212    2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(y ~ x, data = sim_data, size = 4) %&amp;gt;%
  gf_smooth(method = &#39;lm&#39;) %&amp;gt;%
  gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 1), linetype = 2, size = 2) %&amp;gt;%
  gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 855), linetype = 2, color = &#39;lightgreen&#39;, size = 2) %&amp;gt;%
  gf_refine(coord_cartesian(xlim = c(0, 160), ylim = c(0, 120)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum_square_error &amp;lt;- function(conditions, sim_data) {
    fitted &amp;lt;- conditions[[&#39;y_intercept&#39;]] + conditions[[&#39;slope&#39;]] * sim_data[[&#39;x&#39;]]

    deviation &amp;lt;- sim_data[[&#39;y&#39;]] - fitted

    sqrt((sum(deviation^2) / (nrow(sim_data) - 2)))
}

sum_square_error(conditions[1892, ], sim_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 26.74862
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(sim_lm)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.18153
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(future)

plan(multisession)

conditions$sse &amp;lt;- unlist(lapply(1:nrow(conditions), function(xx) sum_square_error(conditions[xx, ], sim_data)))

head(conditions)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   y_intercept slope      sse
## 1        0.00     0 57.37574
## 2        0.25     0 57.13345
## 3        0.50     0 56.89123
## 4        0.75     0 56.64908
## 5        1.00     0 56.40699
## 6        1.25     0 56.16498
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plan(sequential)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regression Inference</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/</link>
      <pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/</guid>
      <description>&lt;h1 id=&#34;inference-for-regression-parameters&#34;&gt;Inference for regression parameters&lt;/h1&gt;
&lt;p&gt;It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population. Therefore, a subset of the population (the sample) is used to estimate the population parameters that are of most interest. As such, our estimates come with error and this uncertainty we can quantify when making inference about the parameter estimates.&lt;/p&gt;
&lt;p&gt;In this course, I plan to show you two ways to perform this inference. One of those frameworks will be the classical approach which uses classical statistical theory to estimate the amount of uncertainty in the parameter estimates. The second approach will use the bootstrap as a way to computationally estimate the uncertainty. The benefit of the bootstrap is that it comes with fewer assumptions than the classical approach. We will build up to these arguments.&lt;/p&gt;
&lt;h2 id=&#34;classical-inferential-framework&#34;&gt;Classical Inferential Framework&lt;/h2&gt;
&lt;p&gt;The classical inferential framework, sometimes referred to as the null hypothesis significance test (NHST) has been around for more than 100 years. This framework builds off of the idea of a null hypothesis.&lt;/p&gt;
&lt;p&gt;A null hypothesis is typically thought as a hypothesis that assumes there is no relationship or a null effect. Framing this in the regression concept that we have been working with, we could define the following null hypotheses.&lt;/p&gt;
&lt;p&gt;$$
H_{0}: \beta_{0} = 0.\ The\  population\  yintercept\  equals\  0.
$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
H_{0}: \beta_{1} = 0.\ The\  population\  slope\  equals\  0.
$$&lt;/p&gt;
&lt;p&gt;In the following two null hypotheses, represented with &lt;code&gt;\(H_{0}\)&lt;/code&gt;, the population parameters are being assumed to be 0 in the population. This is one definition of a null effect, but is not the only null effect we can define (more on this later). The value defined as a null effect is important as it centers the sampling distribution used for evaluating where the sample estimate falls in that distribution.&lt;/p&gt;
&lt;p&gt;Another hypothesis is typically defined with the null hypothesis, called the alternative hypothesis. This hypothesis states that there is an effect. Within the linear regression framework, we could write the alternative hypotheses as:&lt;/p&gt;
&lt;p&gt;$$
H_{A}: \beta_{0} \neq 0.\ The\  population\  yintercept\  is\  not\  equal\  to\  0.
$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
H_{A}: \beta_{1} \neq 0.\ The\  population\  slope\  is\  not\  equal\  to\  0.
$$&lt;/p&gt;
&lt;p&gt;In the following two alternative hypotheses, represented with &lt;code&gt;\(H_{A}\)&lt;/code&gt;, the population parameters are assumed to be not equal to 0. These can also be one-sided, more on this with an example later.&lt;/p&gt;
&lt;h3 id=&#34;estimating-uncertainty-in-parameter-estimates&#34;&gt;Estimating uncertainty in parameter estimates&lt;/h3&gt;
&lt;p&gt;The standard error is used to estimate uncertainty or error in the parameter estimates due to having a sample from the population. More specifically, this means that the entire population is not used to estimate the parameter, therefore the estimate we have is very likely not equal exactly to the parameter. Instead, there is some sort of sampling error involved that we want to quantify. If the sample was collected well, ideally randomly, then the estimate should be unbiased. Unbiased here doesn&amp;rsquo;t mean that the estimate equals the population parameter, rather, that through repeated sampling, the average of our sample estimates would equal the population parameter.&lt;/p&gt;
&lt;p&gt;As mentioned, standard errors are used to quantify this uncertainty. In the linear regression case we have explored so far, there are mathematical formula for the standard errors. These are shown below.&lt;/p&gt;
&lt;p&gt;$$
SE\left( \hat{\beta}_{0} \right) = \sqrt{\hat{\sigma}^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum \left( X - \bar{X} \right)^2} \right)}
$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$
SE\left( \hat{\beta}_{1} \right) = \sqrt{\frac{\hat{\sigma}^2}{\sum \left( X - \bar{X} \right)^2}}
$$&lt;/p&gt;
&lt;p&gt;In the equation above, &lt;code&gt;\(\hat{\sigma}^2\)&lt;/code&gt;, is equal to &lt;code&gt;\(\sqrt{\frac{SS_{error}}{n - 2}}\)&lt;/code&gt;, and &lt;code&gt;\(n\)&lt;/code&gt; is the sample size (ie, number of rows of data in the model).&lt;/p&gt;
&lt;h4 id=&#34;matrix-representation&#34;&gt;Matrix Representation&lt;/h4&gt;
&lt;p&gt;It is also possible, and more easily extendable, to write the standard error computations or the variance of the estimated parameters in matrix representation. This framework extends beyond the single predictor case (ie. one &lt;code&gt;\(X\)&lt;/code&gt;), therefore is more readily used in practice.&lt;/p&gt;
&lt;p&gt;$$
\hat{var}\left({\hat{\beta}}\right) = \hat{\sigma}^2 \left( \mathbf{X}^{`}\mathbf{X} \right)^{-1}
$$&lt;/p&gt;
&lt;p&gt;In the equation above, &lt;code&gt;\(\hat{\sigma}^2\)&lt;/code&gt;, is equal to &lt;code&gt;\(\sqrt{\frac{SS_{error}}{n - 2}}\)&lt;/code&gt;, and &lt;code&gt;\(\mathbf{X}\)&lt;/code&gt; is the design matrix from the regression analysis. Finally, to get the standard errors back, you take the square root of the diagonal elements.&lt;/p&gt;
&lt;p&gt;$$
SE\left( \hat{\beta} \right) = \sqrt{diag\left(\hat{\sigma}^2 \left( \mathbf{X}^{`}\mathbf{X} \right)^{-1}\right)}
$$&lt;/p&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;The  data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;id&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;poc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;county&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h4&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following object is masked from &#39;package:scales&#39;:
## 
##     rescale
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&#39;cbsa_name&#39;, &#39;date&#39;)) %&amp;gt;% 
   drop_na()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$r.square
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$sigma
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sum_x &amp;lt;- airquality %&amp;gt;%
    summarise(mean_wind = mean(avg_wind),
              sum_dev_x_sq = sum( (avg_wind - mean_wind) ^ 2))
sum_x
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   mean_wind sum_dev_x_sq
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1      4.27       29937.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;se_b0 &amp;lt;- sqrt(summary(air_lm)$sigma^2 * ((1 / nrow(airquality)) + ( sum_x[[&#39;mean_wind&#39;]]^2 / sum_x[[&#39;sum_dev_x_sq&#39;]]) ))
se_b1 &amp;lt;- sqrt(summary(air_lm)$sigma^2 / sum_x[[&#39;sum_dev_x_sq&#39;]])

se_b0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.51551
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;se_b1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1043487
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;X &amp;lt;- model.matrix(air_lm)

var_b &amp;lt;- summary(air_lm)$sigma^2 * solve(t(X) %*% X)
var_b 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             (Intercept)    avg_wind
## (Intercept)  0.26575054 -0.04644803
## avg_wind    -0.04644803  0.01088865
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sqrt(diag(var_b))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$coefficients[,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;moving-toward-inference&#34;&gt;Moving toward inference&lt;/h3&gt;
&lt;p&gt;Now that there the parameters are estimated and the amount of uncertainty is quantified, inference is possible. There are two related pieces that can be computed now, a confidence interval and/or the test-statistic and p-value. Let&amp;rsquo;s go through both.&lt;/p&gt;
&lt;p&gt;First, a confidence interval can be computed. Confidence intervals take the following general form:&lt;/p&gt;
&lt;p&gt;$$
\hat{\beta} \pm C * SE
$$&lt;/p&gt;
&lt;p&gt;Where, &lt;code&gt;\(\hat{\beta}\)&lt;/code&gt; is the parameter estimate, &lt;code&gt;\(C\)&lt;/code&gt; is the confidence level, and &lt;code&gt;\(SE\)&lt;/code&gt; is the standard error. The parameter estimates and standard errors are what we have already established, the &lt;code&gt;\(C\)&lt;/code&gt; is the confidence level. This indicates the percentage of times, over the long run/repeated sampling, that the interval will capture the population parameter. Historically, this value is often specified as 95%, but any value is theoretically possible.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;\(C\)&lt;/code&gt; value represents a quantile from a mathematical distribution that separates the middle percetage desired (ie, 95%) from the rest of the distribution. The mathematical distribution is most often the t-distribution, but the difference between a t-distribution and normal distribution are modest once the sample size is greater than 30 or so.&lt;/p&gt;
&lt;p&gt;The figure below tries to highlight the &lt;code&gt;\(C\)&lt;/code&gt; value.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t_30 &amp;lt;- data.frame(value = seq(-5, 5, .01), density = dt(seq(-5, 5, .01), df = 30))
gf_line(density ~ value, data = t_30) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.025, df = 30), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.975, df = 30), linetype = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05-regression-inference_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coef(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)$coefficients[,2]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;abs(qt(.025, df = nrow(airquality) -2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.960456
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;48.2 + c(-1, 1) * 1.96 * .5155
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 47.18962 49.21038
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;-2.211 + c(-1, 1) * 1.96 * .1043
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.415428 -2.006572
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;inference-with-test-statistics&#34;&gt;Inference with test statistics&lt;/h3&gt;
&lt;p&gt;It is also possible to do inference with a test statistic and computation of a p-value. Inference in this framework can be summarized into the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generate hypotheses, ie null and alternative hypotheses.&lt;/li&gt;
&lt;li&gt;Establish an &lt;code&gt;\(\alpha\)&lt;/code&gt; value&lt;/li&gt;
&lt;li&gt;Estimate parameters&lt;/li&gt;
&lt;li&gt;Compute test statistic&lt;/li&gt;
&lt;li&gt;Generate p-value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An &lt;code&gt;\(\alpha\)&lt;/code&gt; value is the level of significance and represents the probability of obtaining the results due to chance. This is a value that the researcher can select. For a 5% &lt;code&gt;\(\alpha\)&lt;/code&gt; value, this is what was used above to compute the confidence intervals.&lt;/p&gt;
&lt;p&gt;The test statistic is computed as follows:&lt;/p&gt;
&lt;p&gt;$$
test\ stat = \frac{\hat{\beta} - hypothesized\ value}{SE(\hat{\beta})}
$$&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;\(\hat{\beta}\)&lt;/code&gt; is the estimated parameter, &lt;code&gt;\(SE(\hat{\beta})\)&lt;/code&gt; is the standard error of the parameter estimate, and the &lt;code&gt;\(hypothesized\ value\)&lt;/code&gt; is the hypothesized value from the null hypothesis. This is often 0, but does not need to be zero.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume the following null/alternative hypotheses:&lt;/p&gt;
&lt;p&gt;$$
H_{0}: \beta_{1} = 0  \&lt;br&gt;
H_{A}: \beta_{1} \neq 0
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use R to compute this test statistic.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t = (-2.211 - 0) / .1043
t
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -21.19847
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pt(-21.198, df = nrow(airquality) -2, lower.tail = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.003866e-95
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t_dist &amp;lt;- data.frame(value = seq(-25, 25, .05), density = dt(seq(-25, 25, .05), df = (nrow(airquality) - 2)))
gf_line(density ~ value, data = t_dist) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.025, df = (nrow(airquality) - 2)), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.975, df = (nrow(airquality) - 2)), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ -21.198, color = &#39;red&#39;) %&amp;gt;%
  gf_vline(xintercept = ~ 21.198, color = &#39;red&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05-regression-inference_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = daily_aqi ~ avg_wind, data = airquality)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -41.71 -14.38  -0.73  12.43  86.84 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  48.2229     0.5155   93.54   &amp;lt;2e-16 ***
## avg_wind     -2.2118     0.1043  -21.20   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 18.05 on 4819 degrees of freedom
## Multiple R-squared:  0.08528,	Adjusted R-squared:  0.08509 
## F-statistic: 449.3 on 1 and 4819 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>t-distribution vs Normal distribution</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/06-t-vs-norm/</link>
      <pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/06-t-vs-norm/</guid>
      <description>&lt;h1 id=&#34;comparing-t-distribution-with-normal-distribution&#34;&gt;Comparing t-distribution with normal distribution&lt;/h1&gt;
&lt;p&gt;This short set of notes will show some differences in the t-distribution with varying degrees of freedom with a normal distribution. This&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;- c(2, 10, 15, 20, 30, 50, 100, 1000)

t_dist &amp;lt;- do.call(&#39;rbind&#39;, lapply(seq_along(df), function(xx) data.frame(value = seq(-5, 5, .01), density = dt(seq(-5, 5, .01), df = df[xx]), df = df[xx], normal = FALSE)))

z_dist &amp;lt;- data.frame(value = seq(-5, 5, .01), density = dnorm(seq(-5, 5, .01)), normal = TRUE, df = 0)

dist &amp;lt;- rbind(t_dist, z_dist)

head(dist)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   value     density df normal
## 1 -5.00 0.007127781  2  FALSE
## 2 -4.99 0.007167524  2  FALSE
## 3 -4.98 0.007207557  2  FALSE
## 4 -4.97 0.007247883  2  FALSE
## 5 -4.96 0.007288503  2  FALSE
## 6 -4.95 0.007329422  2  FALSE
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(patchwork)

theme_set(theme_bw(base_size = 18))

dist %&amp;gt;%
filter(df %in% c(0, 2)) %&amp;gt;%
gf_point(density ~ value, color = ~ normal, size = 1) %&amp;gt;%
  gf_refine(theme(legend.position = &#39;none&#39;)) /
dist %&amp;gt;%
filter(df %in% c(0, 10)) %&amp;gt;%
gf_point(density ~ value, color = ~ normal, size = 1) %&amp;gt;%
  gf_refine(theme(legend.position = &#39;none&#39;)) / 
  dist %&amp;gt;%
filter(df %in% c(0, 100)) %&amp;gt;%
gf_point(density ~ value, color = ~ normal, size = 1) %&amp;gt;%
  gf_refine(theme(legend.position = &#39;none&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-t-vs-norm_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Conditions for Valid Inference</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions/</guid>
      <description>&lt;h1 id=&#34;conditions-for-linear-regression&#34;&gt;Conditions for Linear Regression&lt;/h1&gt;
&lt;p&gt;The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:&lt;/p&gt;
&lt;p&gt;$$
Y - \hat{Y}
$$&lt;/p&gt;
&lt;p&gt;These are the deviations in the observed scores from the predicted scores from the linear regression. Recall, through least square estimation that these residauls will sum to 0, therefore, their mean would also be equal to 0. However, there are certain conditions about these residuals that are made for the linear regression model to have the inferences be appropriate. We&amp;rsquo;ll talk more about what the implications for violating these conditions will have on the linear regression model, but first, the conditions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Approximately Normally distributed residuals&lt;/li&gt;
&lt;li&gt;Homogeneity of variance&lt;/li&gt;
&lt;li&gt;Uncorrelated residuals&lt;/li&gt;
&lt;li&gt;Error term is uncorrelated with the predictor attribute&lt;/li&gt;
&lt;li&gt;Linearity and additivity of the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these will be discussed in turn.&lt;/p&gt;
&lt;p&gt;Data conditions that are not directly testable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Validity&lt;/li&gt;
&lt;li&gt;Representativeness&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;approximately-normally-distributed-residuals&#34;&gt;Approximately Normally distributed residuals&lt;/h2&gt;
&lt;p&gt;The first assumption is that the residuals are at least approximately Normally distributed. This assumption is really only much of a concern when the sample size is small. If the sample size is larger, the Central Limit Thereom (CLT) states that the distribution of the statstics will be approximately normally distributed. The threshold for the CLT to be properly invoked is about 30. Larger then this, the residuals do not need to be approximately normally distributed. Even still, exploring the distribution of the residuals can still be helpful and can also be helpful to identify potential extreme values.&lt;/p&gt;
&lt;p&gt;This example will make use of the air quality data one more time.&lt;/p&gt;
&lt;h3 id=&#34;data&#34;&gt;Data&lt;/h3&gt;
&lt;p&gt;The  data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;id&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;poc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;site_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;county&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h4&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(mosaic)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;mosaic&#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &#39;mosaic&#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &#39;mosaic&#39;
## 
## The following object is masked from &#39;package:Matrix&#39;:
## 
##     mean
## 
## The following object is masked from &#39;package:scales&#39;:
## 
##     rescale
## 
## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     count, do, tally
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross
## 
## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     stat
## 
## The following objects are masked from &#39;package:stats&#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &#39;package:base&#39;:
## 
##     max, mean, min, prod, range, sample, sum
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&#39;cbsa_name&#39;, &#39;date&#39;)) %&amp;gt;% 
   drop_na()

air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The residuals can be saved with the &lt;code&gt;resid()&lt;/code&gt; function. These can also be added to the original data, which are particularly helpful.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(resid(air_lm))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  15.283427  11.186742  25.191433  15.398540   8.246896 -12.749410
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality$residuals &amp;lt;- resid(air_lm)
head(airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 14
##   date           id   poc pm2.5 daily_aqi site_…¹ aqs_p…² cbsa_…³ cbsa_…⁴ county
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; 
## 1 1/1/21  190130009     1  15.1        57 Water … PM2.5 …   47940 Waterl… Black…
## 2 1/4/21  190130009     1  13.3        54 Water … PM2.5 …   47940 Waterl… Black…
## 3 1/7/21  190130009     1  20.5        69 Water … PM2.5 …   47940 Waterl… Black…
## 4 1/10/21 190130009     1  14.3        56 Water … PM2.5 …   47940 Waterl… Black…
## 5 1/13/21 190130009     1  13.7        54 Water … PM2.5 …   47940 Waterl… Black…
## 6 1/16/21 190130009     1   5.3        22 Water … PM2.5 …   47940 Waterl… Black…
## # … with 4 more variables: avg_wind &amp;lt;dbl&amp;gt;, max_wind &amp;lt;dbl&amp;gt;,
## #   max_wind_hours &amp;lt;dbl&amp;gt;, residuals &amp;lt;dbl&amp;gt;, and abbreviated variable names
## #   ¹​site_name, ²​aqs_parameter_desc, ³​cbsa_code, ⁴​cbsa_name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ residuals, data = airquality) %&amp;gt;%
  gf_labs(x = &amp;quot;Residuals&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(airquality, aes(sample = residuals)) + 
  stat_qq(size = 5) + 
  stat_qq_line(size = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h3 id=&#34;standardized-residuals&#34;&gt;Standardized Residuals&lt;/h3&gt;
&lt;p&gt;Standardized residuals can be another way to explore the residuals. These will now be standardized to have a variance of 1, similar to that of a Z-score. These can be computed as:&lt;/p&gt;
&lt;p&gt;$$
standardized\ residuals = \frac{\epsilon_{i}}{SD_{\epsilon}}
$$&lt;/p&gt;
&lt;p&gt;Within R, these can be computed using the function &lt;code&gt;rstandard()&lt;/code&gt;. Furthermore, these can be computed from another package called broom with the &lt;code&gt;augment()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(rstandard(air_lm))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  0.8466153  0.6196983  1.3955421  0.8529766  0.4568937 -0.7062638
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(broom)

resid_diagnostics &amp;lt;- augment(air_lm)
head(resid_diagnostics)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 8
##   daily_aqi avg_wind .fitted .resid     .hat .sigma   .cooksd .std.resid
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1        57     2.94    41.7  15.3  0.000266   18.1 0.0000953      0.847
## 2        54     2.45    42.8  11.2  0.000318   18.1 0.0000611      0.620
## 3        69     2.00    43.8  25.2  0.000380   18.1 0.000370       1.40 
## 4        56     3.45    40.6  15.4  0.000230   18.1 0.0000836      0.853
## 5        54     1.12    45.8   8.25 0.000539   18.1 0.0000563      0.457
## 6        22     6.09    34.7 -12.7  0.000319   18.1 0.0000795     -0.706
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ .std.resid, data = resid_diagnostics) %&amp;gt;%
  gf_labs(x = &amp;quot;Standardized Residuals&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(resid_diagnostics, aes(sample = .std.resid)) + 
  stat_qq(size = 5) + 
  stat_qq_line(size = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;homogeneity-of-variance&#34;&gt;Homogeneity of variance&lt;/h2&gt;
&lt;p&gt;Homoegeneity of variance is an assumption that is of larger concern compared to normality of the residuals. Homoogeneity of variance is an assumption that states that the variance of the residuals are similar across the predicted or fitted values from the regression line. This assumption can be explored by looking at the residuals (standardized or raw residuals), by the fitted or predicted values. Within this plot, the range of residuals should be similar across the range of fitted or predicted values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &#39;loess&#39;, size = 2) %&amp;gt;%
  gf_labs(x = &#39;Fitted Values&#39;,
          y = &#39;Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(.std.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &#39;loess&#39;, size = 2) %&amp;gt;%
  gf_labs(x = &#39;Fitted Values&#39;,
          y = &#39;Standardized Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another figure that can also be helpful for the homogeneity of variance assumption is one that rescales the residuals on the y-axis. The rescaling makes all the standardized residuals positive (takes the absolute value) and then takes the square root of this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;resid_diagnostics %&amp;gt;%
  mutate(sqrt_abs_sresid = sqrt(abs(.std.resid))) %&amp;gt;%
  gf_point(sqrt_abs_sresid ~ .fitted, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &#39;loess&#39;, size = 2) %&amp;gt;%
  gf_labs(x = &#39;Fitted Values&#39;,
          y = &#39;Sqrt Abs Standardized Residuals&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;data-with-high-leverage&#34;&gt;Data with high leverage&lt;/h2&gt;
&lt;p&gt;Data with high leverage are extreme values that may significantly impact the regression estimates. These statistics include extreme values for the outcome or predictor attributes. Cook&amp;rsquo;s distance is one statistic that can help to identify points with high impact/leverage for the regression estimates. Cook&amp;rsquo;s distance is a statistic that represents how much change there would be in the fitted values if the point was removed when estimating the regression coefficients. There is some disagreement between what type of thresholds to use for Cook&amp;rsquo;s distance, but one rule of thumb is Cook&amp;rsquo;s distance greater than 1. There has also been some research showing that Cook&amp;rsquo;s distance follows an F-distribution, so a more specific value could be computed. The rule of thumb for greater than 1 comes from the F distribution for large samples.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;resid_diagnostics %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_col(.cooksd ~ obs_num, fill = &#39;black&#39;, color = &#39;black&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Cook&#39;s Distance&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another leave one out statistic is the studentized deleted residuals. These are computed by removing a data point, refitting the regression model, then generate a predicted value for the X value for the data point removed. Then the residual is computed the same as before and is standardized like the standardized residuals above. The function in R to compute these is &lt;code&gt;rstudent()&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(rstudent(air_lm))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  0.8465904  0.6196587  1.3956793  0.8529524  0.4568561 -0.7062271
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality$student_residuals &amp;lt;- rstudent(air_lm)
head(airquality)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 15
##   date           id   poc pm2.5 daily_aqi site_…¹ aqs_p…² cbsa_…³ cbsa_…⁴ county
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; 
## 1 1/1/21  190130009     1  15.1        57 Water … PM2.5 …   47940 Waterl… Black…
## 2 1/4/21  190130009     1  13.3        54 Water … PM2.5 …   47940 Waterl… Black…
## 3 1/7/21  190130009     1  20.5        69 Water … PM2.5 …   47940 Waterl… Black…
## 4 1/10/21 190130009     1  14.3        56 Water … PM2.5 …   47940 Waterl… Black…
## 5 1/13/21 190130009     1  13.7        54 Water … PM2.5 …   47940 Waterl… Black…
## 6 1/16/21 190130009     1   5.3        22 Water … PM2.5 …   47940 Waterl… Black…
## # … with 5 more variables: avg_wind &amp;lt;dbl&amp;gt;, max_wind &amp;lt;dbl&amp;gt;,
## #   max_wind_hours &amp;lt;dbl&amp;gt;, residuals &amp;lt;dbl&amp;gt;, student_residuals &amp;lt;dbl&amp;gt;, and
## #   abbreviated variable names ¹​site_name, ²​aqs_parameter_desc, ³​cbsa_code,
## #   ⁴​cbsa_name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;airquality %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_point(student_residuals ~ obs_num, size = 5, alpha = .15) %&amp;gt;%
  gf_hline(yintercept = ~ 3, color = &#39;blue&#39;, size = 2) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Studentized Residuals&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Leverage can be another measure to help detect outliers in X values. The hat values that were computed from the &lt;code&gt;augment()&lt;/code&gt; function above and can be interpreted as the distance the X scores are from the center of all X predictors. In the case of a single predictor, the hat values are the distance the X score is from the mean of X. The hat values will also sum up to the number of predictors and will always range between 0 and 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;resid_diagnostics %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_col(.hat ~ obs_num, fill = &#39;black&#39;, color = &#39;black&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Hat Values (leverage)&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(air_lm, which = 1:5)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/07-regression-conditions_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;solutions-to-overcome-data-conditions-not-being-met&#34;&gt;Solutions to overcome data conditions not being met&lt;/h2&gt;
&lt;p&gt;There are some strategies that can be done when a variety of data conditions for linear regression have not been met. In general, the strategies stem around generalizing the model and adding some complexity.&lt;/p&gt;
&lt;p&gt;The following are options that would be possible:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add interactions or other non-linear terms&lt;/li&gt;
&lt;li&gt;Use more complicated model
&lt;ul&gt;
&lt;li&gt;weighted least squares for homogeneity of variance concerns&lt;/li&gt;
&lt;li&gt;models that include measurement error&lt;/li&gt;
&lt;li&gt;mixed models for correlated data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transform the data.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrap</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/08-bootstrap/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/08-bootstrap/</guid>
      <description>&lt;h1 id=&#34;bootstrap&#34;&gt;Bootstrap&lt;/h1&gt;
&lt;p&gt;The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework. The only real assumption when doing a bootstrap approach is that the sample is obtained randomly from the population, an assumption already made in the NHST framework. The primary drawback of the bootstrap approach is that it is computationally expensive, therefore, it can take time to peform the procedure.&lt;/p&gt;
&lt;h2 id=&#34;bootstrapping-steps&#34;&gt;Bootstrapping Steps&lt;/h2&gt;
&lt;p&gt;The following are the steps when performing a bootstrap.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Treat the sample data as the population.&lt;/li&gt;
&lt;li&gt;Resample, with replacement, from the sample data, ensuring the new sample is the same size as the original.&lt;/li&gt;
&lt;li&gt;Estimate the model using the resampled data from step 2.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 many many times (eg, 10,000 or more).&lt;/li&gt;
&lt;li&gt;Visualize distribution of effect of interest&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;resampling-with-replacement&#34;&gt;Resampling with replacement&lt;/h2&gt;
&lt;p&gt;What is meant by sampling with replacement? Let&amp;rsquo;s do an example.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fruit &amp;lt;- data.frame(name = c(&#39;watermelon&#39;, &#39;apple&#39;, &#39;orange&#39;, &#39;kumquat&#39;, &#39;grapes&#39;, &#39;canteloupe&#39;, &#39;kiwi&#39;, &#39;banana&#39;)) %&amp;gt;%
    mutate(obs_num = 1:n())

fruit
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1 watermelon       1
## 2      apple       2
## 3     orange       3
## 4    kumquat       4
## 5     grapes       5
## 6 canteloupe       6
## 7       kiwi       7
## 8     banana       8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;slice_sample(fruit, n = nrow(fruit), replace = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1     orange       3
## 2       kiwi       7
## 3 watermelon       1
## 4    kumquat       4
## 5 canteloupe       6
## 6     grapes       5
## 7      apple       2
## 8     banana       8
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;slice_sample(fruit, n = nrow(fruit), replace = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1 watermelon       1
## 2     banana       8
## 3       kiwi       7
## 4 canteloupe       6
## 5       kiwi       7
## 6       kiwi       7
## 7       kiwi       7
## 8 watermelon       1
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;more-practical-example&#34;&gt;More practical example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s load some data to do a more practical example. The following data come from a &lt;a href=&#34;https://nces.ed.gov/timss/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TIMSS&lt;/a&gt; on science achievement. The data provided is a subset of the available data and is not intended to be representative. Below is a short description of the data. Please don’t hesitate to send any data related questions, happy to provide additional help on interpreting the data appropriately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IDSTUD&lt;/strong&gt;: A unique student ID&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITBIRTHM&lt;/strong&gt;: The birth month of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITBIRTHY&lt;/strong&gt;: The birth year of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITSEX&lt;/strong&gt;: The birth sex of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASDAGE&lt;/strong&gt;: The age of the student (at time of test)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSSCI01&lt;/strong&gt;: Overall science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSEAR01&lt;/strong&gt;: Earth science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSLIF01&lt;/strong&gt;: Life science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSPHY01&lt;/strong&gt;: Physics scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSKNO01&lt;/strong&gt;: Science knowing scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSAPP01&lt;/strong&gt;: Science applying scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSREA01&lt;/strong&gt;: Science reasoning scale score&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)

timss &amp;lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/timss_grade4_science.csv&#39;) |&amp;gt; 
   filter(ASDAGE &amp;lt; 15)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 10029 Columns: 12
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## dbl (12): IDSTUD, ITBIRTHM, ITBIRTHY, ITSEX, ASDAGE, ASSSCI01, ASSEAR01, ASS...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(timss)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 12
##   IDSTUD ITBIRTHM ITBIRTHY ITSEX ASDAGE ASSSCI01 ASSEA…¹ ASSLI…² ASSPH…³ ASSKN…⁴
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1  10101        6     2005     2   9.92     505.    445.    408.    458.    530.
## 2  10102       10     2005     2   9.58     401.    291.    323.    297.    379.
## 3  10103        1     2005     1  10.3      542.    440.    529.    529.    521.
## 4  10104        6     2005     2   9.92     544.    534.    552.    557.    538.
## 5  10105        6     2005     2   9.92     588.    555.    500.    559.    533.
## 6  10106       12     2005     1   9.42     583.    556.    585.    562.    526.
## # … with 2 more variables: ASSAPP01 &amp;lt;dbl&amp;gt;, ASSREA01 &amp;lt;dbl&amp;gt;, and abbreviated
## #   variable names ¹​ASSEAR01, ²​ASSLIF01, ³​ASSPHY01, ⁴​ASSKNO01
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;two-guiding-questions&#34;&gt;Two Guiding Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Is the age of a student related to their overall science scale score?&lt;/li&gt;
&lt;li&gt;Is the life science scale score related to the overall science scale score?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bivariate-exploration&#34;&gt;Bivariate exploration&lt;/h2&gt;
&lt;p&gt;It is important to explore relationships bivariately before going to the model phase. To do this, fill in the outcome of interest in place of &amp;ldquo;%%&amp;rdquo; below and fill in the appropriate predictor in place of &amp;ldquo;^^&amp;rdquo;. You may also want to fill in an appropriate axis labels in place of &amp;ldquo;@@&amp;rdquo; below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Summarize the bivariate association&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;ggstance&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: scales
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &#39;scales&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggridges
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

gf_point(ASSSCI01 ~ ASDAGE, data = timss, size = 4) |&amp;gt; 
  gf_smooth(method = &#39;lm&#39;, size = 1.5) |&amp;gt;
  gf_smooth(method = &#39;loess&#39;, size = 1.5, color = &#39;green&#39;) |&amp;gt;
  gf_labs(x = &amp;quot;Age&amp;quot;,
          y = &amp;quot;Overall Science Score&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/08-bootstrap_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;estimate-parameters&#34;&gt;Estimate Parameters&lt;/h2&gt;
&lt;p&gt;To do this, fill in the outcome of interest in place of &amp;ldquo;%%&amp;rdquo; below and fill in the appropriate predictor in place of &amp;ldquo;^^&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interpret the following parameter estimates in the context of the current problem. (ie., what do these parameter estimates mean?)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;timss_model &amp;lt;- lm(ASSSCI01 ~ ASDAGE, data = timss)

summary(timss_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ASSSCI01 ~ ASDAGE, data = timss)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -305.769  -51.969    4.919   56.707  272.217 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  613.563     17.064  35.956  &amp;lt; 2e-16 ***
## ASDAGE        -6.687      1.669  -4.007 6.18e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 80.89 on 10025 degrees of freedom
## Multiple R-squared:  0.001599,	Adjusted R-squared:  0.0015 
## F-statistic: 16.06 on 1 and 10025 DF,  p-value: 6.183e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;confint(timss_model)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  2.5 %     97.5 %
## (Intercept) 580.114047 647.012616
## ASDAGE       -9.957345  -3.415915
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;resample-these-data&#34;&gt;Resample these data&lt;/h2&gt;
&lt;p&gt;For the astronauts data, to resample, a similar idea can be made. Essentially, we are treating these data as a random sample of some population of space missions. We again, would resample, with replacement, which means that multiple missions would likely show up in the resampling procedure.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;slice_sample(timss, n = nrow(timss), replace = TRUE) %&amp;gt;% 
  count(IDSTUD)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,356 × 2
##    IDSTUD     n
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;
##  1  10105     3
##  2  10108     1
##  3  10110     1
##  4  10111     1
##  5  10112     2
##  6  10113     1
##  7  10116     2
##  8  10117     1
##  9  10118     1
## 10  10119     1
## # … with 6,346 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;resamp_lm &amp;lt;- slice_sample(timss, n = nrow(timss), replace = TRUE) %&amp;gt;% 
  lm(ASSSCI01 ~ ASDAGE, data = .)

summary(resamp_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = ASSSCI01 ~ ASDAGE, data = .)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -303.996  -51.161    4.522   57.275  271.654 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  580.886     17.169  33.834   &amp;lt;2e-16 ***
## ASDAGE        -3.571      1.680  -2.126   0.0335 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 80.45 on 10025 degrees of freedom
## Multiple R-squared:  0.0004508,	Adjusted R-squared:  0.0003511 
## F-statistic: 4.521 on 1 and 10025 DF,  p-value: 0.03351
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;confint(resamp_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  2.5 %      97.5 %
## (Intercept) 547.232078 614.5395568
## ASDAGE       -6.863595  -0.2788938
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;questions-to-consider&#34;&gt;Questions to consider&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Do the parameter estimates differ from before? Why or why not?&lt;/li&gt;
&lt;li&gt;Would you come to substantially different conclusions from the original analysis? Why or why not?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s now repeat this a bunch of times.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2022)

resample_timss &amp;lt;- function(data, model_equation) {
  timss_resample &amp;lt;- slice_sample(data, n = nrow(data), replace = TRUE)

  
  lm(model_equation, data = timss_resample) %&amp;gt;%
    coef()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run this function a bunch of times manually, what happens to the estimates? Why is this happening?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;resample_timss(data = timss, model_equation = ASSSCI01 ~ ASDAGE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)      ASDAGE 
##  593.768824   -4.636762
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;make-sure-futureapply-is-installed&#34;&gt;Make sure future.apply is installed&lt;/h2&gt;
&lt;p&gt;The following code chunk makes sure the future.apply function is installed for parallel processing in R. If you get an error, you can uncomment (delete the &lt;code&gt;#&lt;/code&gt; symbol) in the first line of code to (hopefully) install the package yourself.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# install.packages(&amp;quot;future.apply&amp;quot;)
library(future.apply)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: future
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plan(multisession)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;replicate&#34;&gt;Replicate&lt;/h2&gt;
&lt;p&gt;The following code replicates the analysis many times. Pick an initial value for N and fill in the model equation to match your code above. I will ask you to change this N value later.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;timss_coef &amp;lt;- future.apply::future_replicate(10000, resample_timss(data = timss, 
                        model_equation = ASSSCI01 ~ ASDAGE), simplify = FALSE) |&amp;gt;
                        dplyr::bind_rows() |&amp;gt; 
                        tidyr::pivot_longer(cols = everything())

head(timss_coef)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 2
##   name         value
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 (Intercept) 610.  
## 2 ASDAGE       -6.56
## 3 (Intercept) 603.  
## 4 ASDAGE       -5.67
## 5 (Intercept) 620.  
## 6 ASDAGE       -7.25
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;visualize-results&#34;&gt;Visualize results&lt;/h2&gt;
&lt;p&gt;The following code visualizes the results of the analysis above. Explore the following questions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What does this distribution show/represent?&lt;/li&gt;
&lt;li&gt;What are key features of this distribution?&lt;/li&gt;
&lt;li&gt;How do these values compare to the original linear regression results?
&lt;ul&gt;
&lt;li&gt;Are there comparable statistics here compared to those?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ value, data = timss_coef) %&amp;gt;%
  gf_facet_wrap(~ name, scales = &#39;free&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Regression Estimates&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/08-bootstrap_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;change-the-n-value&#34;&gt;Change the N value&lt;/h2&gt;
&lt;p&gt;Now, change the N value for the replicate step (you can either add a new cell to copy/paste the code or just change it in the code above).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens to the resulting figure when the N increases?&lt;/li&gt;
&lt;li&gt;What value for N seems to be reasonable? Why?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Regression</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/09-multiple-regression/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/09-multiple-regression/</guid>
      <description>&lt;h1 id=&#34;multiple-regression&#34;&gt;Multiple Regression&lt;/h1&gt;
&lt;p&gt;So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single &lt;code&gt;\(X\)&lt;/code&gt; attribute.&lt;/p&gt;
&lt;p&gt;$$
Y = \beta_{0} + \beta_{1} X + \epsilon
$$&lt;/p&gt;
&lt;p&gt;However, there are many situations where we would want more than one predictor attribute in the data. We can simply add another predictor attribute and this would look like the following regression equation.&lt;/p&gt;
&lt;p&gt;$$
Y = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \epsilon
$$&lt;/p&gt;
&lt;p&gt;where &lt;code&gt;\(X_{1}\)&lt;/code&gt; and &lt;code&gt;\(X_{2}\)&lt;/code&gt; are two different predictors attributes. Let&amp;rsquo;s dive right into some data to explore this a bit more.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;For this portion, using data from the college scorecard representing information about higher education institutions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

college &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7058 Columns: 16
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): instnm, city, stabbr, preddeg, region, locale
## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(college)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 16
##   instnm        city  stabbr preddeg region locale adm_r…¹ actcm…²  ugds costt…³
##   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Alabama A &amp;amp; … Norm… AL     Bachel… South… City:…   0.903      18  4824   22886
## 2 University o… Birm… AL     Bachel… South… City:…   0.918      25 12866   24129
## 3 Amridge Univ… Mont… AL     Bachel… South… City:…  NA          NA   322   15080
## 4 University o… Hunt… AL     Bachel… South… City:…   0.812      28  6917   22108
## 5 Alabama Stat… Mont… AL     Bachel… South… City:…   0.979      18  4189   19413
## 6 The Universi… Tusc… AL     Bachel… South… City:…   0.533      28 32387   28836
## # … with 6 more variables: costt4_p &amp;lt;dbl&amp;gt;, tuitionfee_in &amp;lt;dbl&amp;gt;,
## #   tuitionfee_out &amp;lt;dbl&amp;gt;, debt_mdn &amp;lt;dbl&amp;gt;, grad_debt_mdn &amp;lt;dbl&amp;gt;, female &amp;lt;dbl&amp;gt;,
## #   and abbreviated variable names ¹​adm_rate, ²​actcmmid, ³​costt4_a
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;question&#34;&gt;Question&lt;/h2&gt;
&lt;p&gt;Suppose we were interested in exploring admission rates and which attributes helped to explain variation in admission rates.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ adm_rate, data = college) %&amp;gt;%
  gf_labs(x = &amp;quot;Admission Rate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5039 rows containing non-finite values (stat_density).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/09-multiple-regression_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_point(adm_rate ~ actcmmid, data = college) %&amp;gt;%
  gf_smooth(method = &#39;loess&#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Median ACT Score&amp;quot;,
          y = &amp;quot;Admission Rate&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing non-finite values (stat_smooth).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/09-multiple-regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(GGally)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggpairs(college[c(&#39;adm_rate&#39;, &#39;actcmmid&#39;, &#39;costt4_a&#39;)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5039 rows containing non-finite values (stat_density).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5769 rows containing missing values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5236 rows containing missing values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing non-finite values (stat_density).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5777 rows containing missing values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5236 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5777 rows containing missing values (geom_point).
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 3486 rows containing non-finite values (stat_density).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/09-multiple-regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now fit a multiple regression model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adm_mult_reg &amp;lt;- lm(adm_rate ~ actcmmid + costt4_a, data = college)

summary(adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ actcmmid + costt4_a, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.135e+00  3.326e-02  34.130  &amp;lt; 2e-16 ***
## actcmmid    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## costt4_a    -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
##   (5777 observations deleted due to missingness)
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model is now the following form:&lt;/p&gt;
&lt;p&gt;$$
Admission\ Rate = 1.1 + -0.017 ACT + -0.000002 cost + \epsilon
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Why are these parameter estimates so small?&lt;/li&gt;
&lt;li&gt;How well is the overall model doing?&lt;/li&gt;
&lt;li&gt;Are both terms important in understanding variation in admission rates? How can you tell?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;variance-decomposition&#34;&gt;Variance decomposition&lt;/h2&gt;
&lt;p&gt;Ultimately, multiple regression is a decomposition of variance. Recall,&lt;/p&gt;
&lt;p&gt;$$
\sum (Y - \bar{Y})^2 =  \sum (\hat{Y} - \bar{Y})^2 + \sum (Y - \hat{Y})^2  \[10pt]
SS_{Total} = SS_{Regression} + SS_{Error}
$$&lt;/p&gt;
&lt;p&gt;Multiple regression still does this, but now, there are two attributes going into helping explain variation in the regression portion of the variance decomposition. How is this variance decomposed by default? For linear regression, the variance decomposition is commonly done using type I sum of square decomposition. What does this mean? Essentially, this means that additional terms are added to determine if they help explain variation over and above the other terms in the model. This is a conditional variance added. For example, given the model above, the variance decomposition could be broken down into the following.&lt;/p&gt;
&lt;p&gt;$$
SS_{Total} = SS_{Regression} + SS_{Error}   \[10pt]
SS_{Total} = SS_{X_{1}} + SS_{X_{2} | X_{1}} + SS_{Error}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;act_lm &amp;lt;- lm(adm_rate ~ actcmmid, data = college)

summary(act_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ actcmmid, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.71615 -0.12521  0.02024  0.14490  0.37314 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.180914   0.032988   35.80   &amp;lt;2e-16 ***
## actcmmid    -0.022162   0.001391  -15.93   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1844 on 1287 degrees of freedom
##   (5769 observations deleted due to missingness)
## Multiple R-squared:  0.1647,	Adjusted R-squared:  0.1641 
## F-statistic: 253.8 on 1 and 1287 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(act_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## actcmmid     1  8.635  8.6351  253.84 &amp;lt; 2.2e-16 ***
## Residuals 1287 43.781  0.0340                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## actcmmid     1  8.386  8.3861 255.092 &amp;lt; 2.2e-16 ***
## costt4_a     1  1.065  1.0655  32.411 1.546e-08 ***
## Residuals 1278 42.014  0.0329                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cost_lm &amp;lt;- lm(adm_rate ~ costt4_a, data = college)

summary(cost_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ costt4_a, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72108 -0.12656  0.02474  0.14459  0.38059 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  8.233e-01  1.151e-02   71.52   &amp;lt;2e-16 ***
## costt4_a    -4.147e-06  3.022e-07  -13.73   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1965 on 1820 degrees of freedom
##   (5236 observations deleted due to missingness)
## Multiple R-squared:  0.0938,	Adjusted R-squared:  0.09331 
## F-statistic: 188.4 on 1 and 1820 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(cost_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## costt4_a     1  7.278  7.2781   188.4 &amp;lt; 2.2e-16 ***
## Residuals 1820 70.309  0.0386                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Model Comparisons</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/10-model-comparison/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/10-model-comparison/</guid>
      <description>&lt;h1 id=&#34;model-comparison&#34;&gt;Model Comparison&lt;/h1&gt;
&lt;p&gt;This section of multiple regression is going to explore model comparison, to guide the selection of the best fitting model from a set of competing models.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

college &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv&amp;quot;) %&amp;gt;%
  mutate(act_mean = actcmmid - mean(actcmmid, na.rm = TRUE),
         cost_mean = costt4_a - mean(costt4_a, na.rm = TRUE)) %&amp;gt;%
  drop_na(act_mean, cost_mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7058 Columns: 16
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): instnm, city, stabbr, preddeg, region, locale
## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(college)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 18
##   instnm        city  stabbr preddeg region locale adm_r…¹ actcm…²  ugds costt…³
##   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Alabama A &amp;amp; … Norm… AL     Bachel… South… City:…   0.903      18  4824   22886
## 2 University o… Birm… AL     Bachel… South… City:…   0.918      25 12866   24129
## 3 University o… Hunt… AL     Bachel… South… City:…   0.812      28  6917   22108
## 4 Alabama Stat… Mont… AL     Bachel… South… City:…   0.979      18  4189   19413
## 5 The Universi… Tusc… AL     Bachel… South… City:…   0.533      28 32387   28836
## 6 Auburn Unive… Mont… AL     Bachel… South… City:…   0.825      22  4211   19892
## # … with 8 more variables: costt4_p &amp;lt;dbl&amp;gt;, tuitionfee_in &amp;lt;dbl&amp;gt;,
## #   tuitionfee_out &amp;lt;dbl&amp;gt;, debt_mdn &amp;lt;dbl&amp;gt;, grad_debt_mdn &amp;lt;dbl&amp;gt;, female &amp;lt;dbl&amp;gt;,
## #   act_mean &amp;lt;dbl&amp;gt;, cost_mean &amp;lt;dbl&amp;gt;, and abbreviated variable names ¹​adm_rate,
## #   ²​actcmmid, ³​costt4_a
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dim(college)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1281   18
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;adm_mult_reg &amp;lt;- lm(adm_rate ~ act_mean + cost_mean, data = college)

summary(adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean + cost_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  6.834e-01  6.346e-03 107.681  &amp;lt; 2e-16 ***
## act_mean    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## cost_mean   -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;omnibus-hypothesis&#34;&gt;Omnibus Hypothesis&lt;/h2&gt;
&lt;p&gt;In linear regression, I omitted a portion of the output from above that typically comes with most statistical output. That is, there is a test statistic that aims to test if the model is explaining variation over and above a simple mean. More specifically, this omnibus hypothesis tests the following:&lt;/p&gt;
&lt;p&gt;$$
H_{0}: All\ \beta = 0 \[10pt]
H_{A}: Any\ \beta \neq 0
$$&lt;/p&gt;
&lt;p&gt;This hypothesis can be formally tested with an F-statistic which is distributed as an F distribution with &lt;code&gt;\(p\)&lt;/code&gt; predictor attributes and &lt;code&gt;\(n - p - 1\)&lt;/code&gt; degrees of freedom.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f_data &amp;lt;- data.frame(value = seq(0, 5, by = .01)) %&amp;gt;%
   mutate(dens = df(value, 2, 1278))

gf_line(dens ~ value, data = f_data, size = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-model-comparison_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;f_data &amp;lt;- data.frame(value = seq(0, 5, by = .01)) %&amp;gt;%
   mutate(dens = df(value, 5, 50))

gf_line(dens ~ value, data = f_data, size = 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-model-comparison_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;adjusted-r-squared&#34;&gt;Adjusted R-squared&lt;/h2&gt;
&lt;p&gt;The adjusted R-squared is typically used when comparing models. This statistic is commonly used as R-square represents the ratio between explained and total variance, therefore, this will always increase, even if the new attribute entered is not helpful. The adjusted R-squared tries to adjust for model complexity. There are many ways to do this, but the most common will be defined here.&lt;/p&gt;
&lt;p&gt;$$
\bar{R}^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}
$$&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;$$
\bar{R}^2 = 1 - \frac{SS_{res} / df_{e}}{SS_{tot} / df_{t}}
$$
where &lt;code&gt;\(p\)&lt;/code&gt; is the number of predictors (excluding the intercept), &lt;code&gt;\(n\)&lt;/code&gt; is the sample size, &lt;code&gt;\(SS_{e}\)&lt;/code&gt; and &lt;code&gt;\(SS_{tot}\)&lt;/code&gt; are sum of square residual and total respectively, and &lt;code&gt;\(df_{e}\)&lt;/code&gt; and &lt;code&gt;\(df_{t}\)&lt;/code&gt; are degrees of freedom for the error ($n - p - 1$) and total ($n - 1$) respectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean + cost_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  6.834e-01  6.346e-03 107.681  &amp;lt; 2e-16 ***
## act_mean    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## cost_mean   -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1 - (1 - .1836) * (1281 - 1) / (1281 - 2 - 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1823224
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## act_mean     1  8.386  8.3861 255.092 &amp;lt; 2.2e-16 ***
## cost_mean    1  1.065  1.0655  32.411 1.546e-08 ***
## Residuals 1278 42.014  0.0329                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1 - (42.01 / 1278) / ((8.39 + 1.066 + 42.014) / 1280)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1825191
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;model-comparison-1&#34;&gt;Model Comparison&lt;/h2&gt;
&lt;p&gt;There are a variety of statistics used to provide statistical evidence for competing models. If the models are nested, then the variance decomposition can be used to determine if the added predictors helped to explain significant variation over and above the simpler model.&lt;/p&gt;
&lt;p&gt;In this situation, another F statistic can be derived where&lt;/p&gt;
&lt;p&gt;$$
F = \frac{SS_{res}^{R} - SS_{res}^{F} / \Delta p}{SS_{res}^{F} / df_{F}}
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;act_lm &amp;lt;- lm(adm_rate ~ act_mean, data = college)

summary(act_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.71229 -0.12483  0.01999  0.14317  0.37289 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.661593   0.005128  129.02   &amp;lt;2e-16 ***
## act_mean    -0.021905   0.001388  -15.78   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1835 on 1279 degrees of freedom
## Multiple R-squared:  0.1629,	Adjusted R-squared:  0.1623 
## F-statistic:   249 on 1 and 1279 DF,  p-value: &amp;lt; 2.2e-16
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;anova(act_lm, adm_mult_reg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: adm_rate ~ act_mean
## Model 2: adm_rate ~ act_mean + cost_mean
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1   1279 43.079                                  
## 2   1278 42.014  1    1.0655 32.411 1.546e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;((43.08 - 42.01) / 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.07
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;1.07 / (42.01 / 1278)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 32.55082
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;non-nested-models&#34;&gt;Non-nested models&lt;/h3&gt;
&lt;p&gt;For non-nested models, the F-statistic defined above will not work. Instead other statistics are needed to evaluate which model is the best. The one that I prefer for this is the AIC (Akaike information criteria) or the related small sample form, AICc. The equations for these aren&amp;rsquo;t all that useful, utilizing software is the best way to compute these statistics. In general, smaller AIC values indicate a better fitting model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(AICcmodavg)

cost_lm &amp;lt;- lm(adm_rate ~ cost_mean, data = college)

aictab(list(cost_lm, act_lm), 
       modnames = c(&#39;cost&#39;, &#39;act&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model selection based on AICc:
## 
##      K    AICc Delta_AICc AICcWt Cum.Wt     LL
## act  3 -704.26       0.00      1      1 355.14
## cost 3 -637.70      66.56      0      1 321.86
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Regression Categorical</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical/</guid>
      <description>&lt;h1 id=&#34;regression-with-categorical-predictors&#34;&gt;Regression with Categorical Predictors&lt;/h1&gt;
&lt;p&gt;This set of notes will explore using linear regression for a single predictor attribute that is categorical instead of continuous. To explore this first, let&amp;rsquo;s explore some data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(Lahman)
library(ggformula)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &#39;ggstance&#39;
## 
## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &#39;scales&#39;
## 
## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard
## 
## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &#39;playerID&#39;. You can override using the
## `.groups` argument.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  dplyr::select(-playerID)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## ℹ Please use `tibble::as_tibble()` instead.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(career)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   name               lgID      H    AB average
##   &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Jeff Abbott        AL      127   459   0.277
## 2 Kurt Abbott        AL       33   123   0.268
## 3 Kurt Abbott        NL      455  1780   0.256
## 4 Reggie Abercrombie NL       54   255   0.212
## 5 Brent Abernathy    AL      194   767   0.253
## 6 Shawn Abner        AL       81   309   0.262
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;question&#34;&gt;Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the batting average of baseball players since 1990, that is, the average is:&lt;/p&gt;
&lt;p&gt;$$
average = \frac{number\ of\ hits}{number\ of\ atbats}
$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first visualize this.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_density(~ average, data = career) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we hypothesized that the batting average will differ based on the league that players played in.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_violin(lgID ~ average, data = career, fill = &#39;gray80&#39;, draw_quantiles = c(&#39;0.1&#39;, &#39;0.5&#39;, &#39;0.9&#39;)) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;,
          y = &amp;quot;League&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distributions seem similar, but what if we wanted to go a step further and estimate a model to explore if there are really differences or not. For example, suppose we were interested in:&lt;/p&gt;
&lt;p&gt;$$
H_{0}: \mu_{NL} = \mu_{AL}
$$&lt;/p&gt;
&lt;p&gt;What type of model could we use? What about linear regression?&lt;/p&gt;
&lt;h2 id=&#34;linear-regression-with-categorical-attributes&#34;&gt;Linear Regression with Categorical Attributes&lt;/h2&gt;
&lt;p&gt;Since these notes are happening, you can assume it is possible. But how can a categorical attribute with categories rather than numbers be included in the linear regression model?&lt;/p&gt;
&lt;p&gt;The answer is that they can&amp;rsquo;t. We need a new representation of the categorical attribute, enter dummy or indicator coding.&lt;/p&gt;
&lt;h3 id=&#34;dummyindicator-coding&#34;&gt;Dummy/Indicator Coding&lt;/h3&gt;
&lt;p&gt;Suppose we use the following logic:&lt;/p&gt;
&lt;p&gt;If NL, then give a value of 1, else give a value of 0.&lt;/p&gt;
&lt;p&gt;Does this give the same information as before?&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;League ID&lt;/th&gt;
&lt;th&gt;Dummy League ID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;AL&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NL&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What would this look like for the actual data?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &#39;NL&#39;, 1, 0))

head(career, n = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 6
##    name               lgID      H    AB average league_dummy
##    &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Jeff Abbott        AL      127   459   0.277            0
##  2 Kurt Abbott        AL       33   123   0.268            0
##  3 Kurt Abbott        NL      455  1780   0.256            1
##  4 Reggie Abercrombie NL       54   255   0.212            1
##  5 Brent Abernathy    AL      194   767   0.253            0
##  6 Shawn Abner        AL       81   309   0.262            0
##  7 Shawn Abner        NL       19   115   0.165            1
##  8 Bobby Abreu        AL      858  3061   0.280            0
##  9 Bobby Abreu        NL     1602  5373   0.298            1
## 10 Jose Abreu         AL     1262  4353   0.290            0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that there is a numeric attribute, these can be added into the linear regression model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;average_lm &amp;lt;- lm(average ~ league_dummy, data = career)

broom::tidy(average_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term         estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.253    0.000761   332.      0    
## 2 league_dummy  0.00102  0.00107      0.949   0.343
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How are these terms interpreted now?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_stats(average ~ league_dummy, data = career, mean, sd, length)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response league_dummy      mean         sd length
## 1  average            0 0.2525899 0.02876352   1431
## 2  average            1 0.2536090 0.02879563   1440
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;average_lm2 &amp;lt;- lm(average ~ lgID, data = career)

broom::tidy(average_lm2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.253    0.000761   332.      0    
## 2 lgIDNL       0.00102  0.00107      0.949   0.343
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;t.test(average ~ lgID, data = career, var.equal = TRUE)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Two Sample t-test
## 
## data:  average by lgID
## t = -0.9487, df = 2869, p-value = 0.3429
## alternative hypothesis: true difference in means between group AL and group NL is not equal to 0
## 95 percent confidence interval:
##  -0.003125489  0.001087227
## sample estimates:
## mean in group AL mean in group NL 
##        0.2525899        0.2536090
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;values-other-than-01&#34;&gt;Values other than 0/1&lt;/h2&gt;
&lt;p&gt;First, I want to build off of the first part of the notes on regression with categorical predictors. Before generalizing to more than two groups, let&amp;rsquo;s first explore what happens when values other than 0/1 are used for the categorical attribute. The following three dummy/indicator attributes will be used:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;1 = NL, 0 = AL&lt;/li&gt;
&lt;li&gt;1 = NL, 2 = AL&lt;/li&gt;
&lt;li&gt;100 = NL, 0 = AL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Make some predictions about what you think will happen in the three separate regressions?&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(Lahman)
library(ggformula)

theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &#39;playerID&#39;. You can override using the
## `.groups` argument.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  dplyr::select(-playerID)

career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &#39;NL&#39;, 1, 0),
         league_dummy_12 = ifelse(lgID == &#39;NL&#39;, 1, 2),
         league_dummy_100 = ifelse(lgID == &#39;NL&#39;, 100, 0))

head(career, n = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 8
##    name               lgID      H    AB average league_dummy league_du…¹ leagu…²
##    &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 Jeff Abbott        AL      127   459   0.277            0           2       0
##  2 Kurt Abbott        AL       33   123   0.268            0           2       0
##  3 Kurt Abbott        NL      455  1780   0.256            1           1     100
##  4 Reggie Abercrombie NL       54   255   0.212            1           1     100
##  5 Brent Abernathy    AL      194   767   0.253            0           2       0
##  6 Shawn Abner        AL       81   309   0.262            0           2       0
##  7 Shawn Abner        NL       19   115   0.165            1           1     100
##  8 Bobby Abreu        AL      858  3061   0.280            0           2       0
##  9 Bobby Abreu        NL     1602  5373   0.298            1           1     100
## 10 Jose Abreu         AL     1262  4353   0.290            0           2       0
## # … with abbreviated variable names ¹​league_dummy_12, ²​league_dummy_100
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;average_lm &amp;lt;- lm(average ~ league_dummy, data = career)

broom::tidy(average_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term         estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)   0.253    0.000761   332.      0    
## 2 league_dummy  0.00102  0.00107      0.949   0.343
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;average_lm_12 &amp;lt;- lm(average ~ league_dummy_12, data = career)

broom::tidy(average_lm_12)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term            estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)      0.255     0.00170   150.      0    
## 2 league_dummy_12 -0.00102   0.00107    -0.949   0.343
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;average_lm_100 &amp;lt;- lm(average ~ league_dummy_100, data = career)

broom::tidy(average_lm_100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term              estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)      0.253     0.000761    332.      0    
## 2 league_dummy_100 0.0000102 0.0000107     0.949   0.343
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before moving to more than 2 groups, any thoughts on how we could run a one-sample t-test using a linear regression? For example, suppose this null hypothesis wanted to be explored.&lt;/p&gt;
&lt;p&gt;$$
H_{0}: \mu_{BA} = .2
$$&lt;/p&gt;
&lt;p&gt;$$
H_{A}: \mu_{BA} \neq .2
$$&lt;/p&gt;
&lt;h2 id=&#34;generalize-to-more-than-2-groups&#34;&gt;Generalize to more than 2 groups&lt;/h2&gt;
&lt;p&gt;The ability to use regression with categorical attributes of more than 2 groups is possible and an extension of the 2 groups model shown above. First, let&amp;rsquo;s think about how we could represent three categories as numeric attributes. Suppose we had the following 4 categories of baseball players.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Position&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Outfield&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Infield&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Catcher&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Designated Hitter&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(GeomMLBStadiums)

ggplot() + 
  geom_mlb_stadium(stadium_segments = &amp;quot;all&amp;quot;) + 
  facet_wrap(~team) + 
  coord_fixed() + 
  theme_void()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(tidyverse)
library(Lahman)
library(ggformula)

theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &#39;playerID&#39;. You can override using the
## `.groups` argument.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- Appearances %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  select(-GS, -G_ph, -G_pr, -G_batting, -G_defense, -G_p, -G_lf, -G_cf, -G_rf) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(g_inf = sum(c_across(G_1b:G_ss))) %&amp;gt;%
  select(-G_1b, -G_2b, -G_3b, -G_ss) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(catcher = sum(G_c),
            outfield = sum(G_of),
            dh = sum(G_dh),
            infield = sum(g_inf),
            total_games = sum(G_all)) %&amp;gt;%
  pivot_longer(catcher:infield,
               names_to = &amp;quot;position&amp;quot;) %&amp;gt;%
  filter(value &amp;gt; 0) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  slice_max(value) %&amp;gt;%
  select(playerID, lgID, position) %&amp;gt;%
  inner_join(career)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &#39;playerID&#39;. You can override using the
## `.groups` argument.
## Joining, by = c(&amp;quot;playerID&amp;quot;, &amp;quot;lgID&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;)

career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &#39;NL&#39;, 1, 0))

count(career, position)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 2
##   position     n
##   &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1 catcher    410
## 2 dh          81
## 3 infield   1248
## 4 outfield  1136
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gf_violin(position ~ average, data = career, fill = &#39;gray85&#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;,
          y = &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/11-regression-categorical_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;career &amp;lt;- career %&amp;gt;%
  mutate(outfield = ifelse(position == &#39;outfield&#39;, 1, 0),
         infield = ifelse(position == &#39;infield&#39;, 1, 0),
         catcher = ifelse(position == &#39;catcher&#39;, 1, 0))

head(career)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 11
##   playerID  name       lgID  posit…¹     H    AB average leagu…² outfi…³ infield
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 abbotje01 Jeff Abbo… AL    outfie…   127   459   0.277       0       1       0
## 2 abbotku01 Kurt Abbo… AL    infield    33   123   0.268       0       0       1
## 3 abbotku01 Kurt Abbo… NL    infield   455  1780   0.256       1       0       1
## 4 abercre01 Reggie Ab… NL    outfie…    54   255   0.212       1       1       0
## 5 abernbr01 Brent Abe… AL    infield   194   767   0.253       0       0       1
## 6 abnersh01 Shawn Abn… AL    outfie…    81   309   0.262       0       1       0
## # … with 1 more variable: catcher &amp;lt;dbl&amp;gt;, and abbreviated variable names
## #   ¹​position, ²​league_dummy, ³​outfield
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;position_lm &amp;lt;- lm(average ~ 1 + outfield + infield + catcher, data = career)

broom::tidy(position_lm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term        estimate std.error statistic    p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.257     0.00315    81.7   0         
## 2 outfield    -0.00182   0.00326    -0.557 0.578     
## 3 infield     -0.00289   0.00325    -0.888 0.375     
## 4 catcher     -0.0165    0.00345    -4.79  0.00000175
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_stats(average ~ position, data = career, mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response position      mean
## 1  average  catcher 0.2408859
## 2  average       dh 0.2574041
## 3  average  infield 0.2545163
## 4  average outfield 0.2555881
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Pairwise Contrasts</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/12-pairwise/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/12-pairwise/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;more-details-on-pairwise-contrasts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More details on pairwise contrasts&lt;/h1&gt;
&lt;p&gt;Last week the idea of pairwise contrasts were introduced. Here is a more formal discussion of pairwise contrasts and controlling of familywise type I error rates.&lt;/p&gt;
&lt;div id=&#34;bonferroni-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonferroni Method&lt;/h2&gt;
&lt;p&gt;This method is introduced first, primarily due to its simplicity. This is not the best measure to use however as will be discussed below. The Bonferroni method makes the following adjustment:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha_{new} = \frac{\alpha}{m}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of hypotheses being tested. Let’s use a specific example to frame this based on the baseball data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
##  learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
##  learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- Appearances %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  select(-GS, -G_ph, -G_pr, -G_batting, -G_defense, -G_p, -G_lf, -G_cf, -G_rf) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(g_inf = sum(c_across(G_1b:G_ss))) %&amp;gt;%
  select(-G_1b, -G_2b, -G_3b, -G_ss) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(catcher = sum(G_c),
            outfield = sum(G_of),
            dh = sum(G_dh),
            infield = sum(g_inf),
            total_games = sum(G_all)) %&amp;gt;%
  pivot_longer(catcher:infield,
               names_to = &amp;quot;position&amp;quot;) %&amp;gt;%
  filter(value &amp;gt; 0) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  slice_max(value) %&amp;gt;%
  select(playerID, lgID, position) %&amp;gt;%
  inner_join(career)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.
## Joining, by = c(&amp;quot;playerID&amp;quot;, &amp;quot;lgID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## ℹ Please use `tibble::as_tibble()` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &amp;#39;NL&amp;#39;, 1, 0))

head(career)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 8
##   playerID  name               lgID  position     H    AB average league_dummy
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 abbotje01 Jeff Abbott        AL    outfield   127   459   0.277            0
## 2 abbotku01 Kurt Abbott        AL    infield     33   123   0.268            0
## 3 abbotku01 Kurt Abbott        NL    infield    455  1780   0.256            1
## 4 abercre01 Reggie Abercrombie NL    outfield    54   255   0.212            1
## 5 abernbr01 Brent Abernathy    AL    infield    194   767   0.253            0
## 6 abnersh01 Shawn Abner        AL    outfield    81   309   0.262            0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_lm &amp;lt;- lm(average ~ 1 + position, data = career)

broom::tidy(position_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term             estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)        0.241    0.00140    172.   0       
## 2 positiondh         0.0165   0.00345      4.79 1.75e- 6
## 3 positioninfield    0.0136   0.00161      8.44 4.80e-17
## 4 positionoutfield   0.0147   0.00163      9.00 4.05e-19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count(career, position)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 2
##   position     n
##   &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1 catcher    410
## 2 dh          81
## 3 infield   1248
## 4 outfield  1136&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given that there are 4 groups, if all pairwise comparisons were of interest, the following would be all possible &lt;strong&gt;NULL&lt;/strong&gt; hypotheses. Note, I am assuming that each of these has a matching alternative hypothesis that states the groups differences are &lt;em&gt;different&lt;/em&gt; from 0.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \mu_{catcher} - \mu_{dh} = 0 \\
H_{0}: \mu_{catcher} - \mu_{infield} = 0 \\
H_{0}: \mu_{catcher} - \mu_{outfield} = 0 \\
H_{0}: \mu_{dh} - \mu_{infield} = 0 \\
H_{0}: \mu_{dh} - \mu_{outfield} = 0 \\
H_{0}: \mu_{infield} - \mu_{outfield} = 0 \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, to find the total number of hypotheses, you can use the combinations formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
C(n, r) = \binom{n}{r} = \frac{n!}{r!(n - r)!}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For our example this would lead to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\binom{4}{2} = \frac{4!}{2!(4 - 2)!} = \frac{4 * 3 * 2 * 1}{2 * 1 * (2 * 1)} = \frac{24}{4} = 6
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, using bonferroni’s correction, our new alpha would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha_{new} = \frac{.05}{6} = .0083
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, you could also adjust the p-values to make them smaller and still use .05 (or any other predetermined familywise &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; value).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p_{new} = \frac{p_{original}}{m}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is what most software programs do automatically for you.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;bonf&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  career$average and career$position 
## 
##          catcher dh infield
## dh       1.0e-05 -  -      
## infield  2.9e-16 1  -      
## outfield &amp;lt; 2e-16 1  1      
## 
## P value adjustment method: bonferroni&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-type-i-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is a type I error?&lt;/h2&gt;
&lt;p&gt;Before moving to why you should care, let’s more formally talk about type I errors. Suppose we have the following table of possible outcomes:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;31%&#34; /&gt;
&lt;col width=&#34;33%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt; true&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt; false&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Reject &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Type I Error (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Correct - Power (&lt;span class=&#34;math inline&#34;&gt;\(1 - \beta\)&lt;/span&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Retain &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Correct (&lt;span class=&#34;math inline&#34;&gt;\(1 - \alpha\)&lt;/span&gt;)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Type II Error (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Type I error&lt;/strong&gt; is when the null hypothesis is true in the population, but the statistical evidence &lt;em&gt;supports the alternative hypothesis&lt;/em&gt;.
&lt;strong&gt;Type II error&lt;/strong&gt; is when the null hypothesis is false in the population, but the statistical evidence &lt;em&gt;supports the null hypothesis&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here is a good link for an interactive app that shows many of these terms visually: &lt;a href=&#34;https://rpsychologist.com/d3/nhst/&#34; class=&#34;uri&#34;&gt;https://rpsychologist.com/d3/nhst/&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;type-m-and-type-s-errors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type M and Type S errors&lt;/h3&gt;
&lt;p&gt;These were first defined by Gelman and Tuerlinckx (2000) and is also in section 4.4 of &lt;em&gt;Regression and Other Stories&lt;/em&gt; textbook.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type S (sign) occurs when the sign of the estimated effect is in the opposite direction as the true effect.&lt;/li&gt;
&lt;li&gt;Type M (magnitude) occurse when the magnitude of the estimated effect is much different (often larger) than the true effect.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These really shift the narrative away from achieving statistical signficiance and moving toward proper estimation of effects or precision, this is a better goal in my opinion.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;holm-bonferroni-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Holm-Bonferroni Procedure&lt;/h2&gt;
&lt;p&gt;The boneferroni procedure will be overly conservative and I wouldn’t recommend it’s use in practice. If you want a simple approach to p-value adjustment, I’d recommend just setting a specific value for the alpha value to be more conservative, for example setting it to 0.01.&lt;/p&gt;
&lt;p&gt;The Holm-Bonferroni approach is an adjustment method that is more powerful then the original bonferroni procedure, but does not come with onerous assumptions. The Holm-Boneferroni procedure uses the following steps for adjustment.&lt;/p&gt;
&lt;p&gt;Suppose there are &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; p-values (ie, &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; null hypotheses). First, order these from smallest to largest, be sure to keep track of which hypothese the p-value is associated with. Then,
1. Is the smallest p-value &lt;strong&gt;less than&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha / m\)&lt;/span&gt;, if yes, provide evidence for the alternative hypothesis and proceed to the next p-value, it not stop.
2. Is the second smallest p-value &lt;strong&gt;less than&lt;/strong&gt; $ / m - 1$, if eys, provide evidence for the alternative hypothesis and proceed to the next p-value, if not stop.
3. Continue, comparing the p-values in order to the following adjust alpha,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha_{new_{k}} = \frac{\alpha}{m + 1 - k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where is &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the rank of the p-values when ordered from smallest to largest.&lt;/p&gt;
&lt;p&gt;Similar to the bonferroni, the p-values can be adjusted with the following formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p_{adj} = min(1, max((m + 1 - k) * p_{k} ))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where is &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the rank of the p-values when ordered from smallest to largest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;holm&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  career$average and career$position 
## 
##          catcher dh infield
## dh       7.0e-06 -  -      
## infield  2.4e-16 1  -      
## outfield &amp;lt; 2e-16 1  1      
## 
## P value adjustment method: holm&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               catcher        dh   infield
## dh       1.747816e-06        NA        NA
## infield  4.802093e-17 0.3745381        NA
## outfield 4.051053e-19 0.5776818 0.3567626&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[3] * (6 + 1 - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.430632e-18&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[2] * (6 + 1 - 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.401046e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[1] * (6 + 1 - 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6.991264e-06&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[9] * (6 + 1 - 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.070288&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[5] * (6 + 1 - 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7490762&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairwise.t.test(career$average, career$position, p.adjust = &amp;#39;none&amp;#39;)$p.value[6] * (6 + 1 - 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5776818&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-assumptions-for-model-with-categorical-attributes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring assumptions for model with categorical attributes&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(position_lm, which = 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/12-pairwise_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/12-pairwise_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/12-pairwise_files/figure-html/unnamed-chunk-8-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/12-pairwise_files/figure-html/unnamed-chunk-8-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/12-pairwise_files/figure-html/unnamed-chunk-8-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Categorical Attributes</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;add-more-than-one-categorical-predictor&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Add more than one categorical predictor&lt;/h1&gt;
&lt;p&gt;Let’s explore the ability to add more than one categorical predictor to our model. This will then build into thinking through interactions. Let’s load some new data. These are bike data from Philadelphia program, called &lt;a href=&#34;https://www.rideindego.com/about/data/&#34;&gt;Indego&lt;/a&gt;. The data loaded below are from Q3 of 2021.&lt;/p&gt;
&lt;p&gt;Here is information about the columns in the data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;trip_id:&lt;/strong&gt; Locally unique integer that identifies the trip&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;duration:&lt;/strong&gt; Length of trip in minutes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start_time:&lt;/strong&gt; The date/time when the trip began, presented in ISO 8601 format in local time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;end_time:&lt;/strong&gt; The date/time when the trip ended, presented in ISO 8601 format in local time&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start_station:&lt;/strong&gt; The station ID where the trip originated (for station name and more information on each station see the Station Table)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start_lat:&lt;/strong&gt; The latitude of the station where the trip originated&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start_lon:&lt;/strong&gt; The longitude of the station where the trip originated&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;end_station:&lt;/strong&gt; The station ID where the trip terminated (for station name and more information on each station see the Station Table)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;end_lat:&lt;/strong&gt; The latitude of the station where the trip terminated&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;end_lon:&lt;/strong&gt; The longitude of the station where the trip terminated&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bike_id:&lt;/strong&gt; Locally unique integer that identifies the bike&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;plan_duration:&lt;/strong&gt; The number of days that the plan the passholder is using entitles them to ride; 0 is used for a single ride plan (Walk-up)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;trip_route_category:&lt;/strong&gt; “Round Trip” for trips starting and ending at the same station or “One Way” for all other trips&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;passholder_type:&lt;/strong&gt; The name of the passholder’s plan&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bike_type:&lt;/strong&gt; The kind of bike used on the trip, including standard pedal-powered bikes or electric assist bikes&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
##  learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
##  learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

temp &amp;lt;- tempfile()
download.file(&amp;quot;https://u626n26h74f16ig1p3pt0f2g-wpengine.netdna-ssl.com/wp-content/uploads/2021/10/indego-trips-2021-q3.zip&amp;quot;, temp)
bike &amp;lt;- readr::read_csv(unz(temp, &amp;quot;indego-trips-2021-q3.csv&amp;quot;)) %&amp;gt;%
   filter(duration &amp;lt;= 120 &amp;amp; passholder_type != &amp;#39;Walk-up&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: One or more parsing issues, see `problems()` for details&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 300432 Columns: 15
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (5): start_time, end_time, trip_route_category, passholder_type, bike_type
## dbl (10): trip_id, duration, start_station, start_lat, start_lon, end_statio...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unlink(temp)

head(bike)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 15
##     trip_id duration start_time  end_t…¹ start…² start…³ start…⁴ end_s…⁵ end_lat
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 398698761       11 7/1/2021 0… 7/1/20…    3045    39.9   -75.2    3030    39.9
## 2 398698759        4 7/1/2021 0… 7/1/20…    3052    39.9   -75.2    3238    39.9
## 3 398698757       56 7/1/2021 0… 7/1/20…    3192    40.0   -75.1    3161    40.0
## 4 398698755       55 7/1/2021 0… 7/1/20…    3192    40.0   -75.1    3161    40.0
## 5 398698753        5 7/1/2021 0… 7/1/20…    3052    39.9   -75.2    3046    40.0
## 6 398698750       13 7/1/2021 0… 7/1/20…    3213    39.9   -75.2    3039    40.0
## # … with 6 more variables: end_lon &amp;lt;dbl&amp;gt;, bike_id &amp;lt;dbl&amp;gt;, plan_duration &amp;lt;dbl&amp;gt;,
## #   trip_route_category &amp;lt;chr&amp;gt;, passholder_type &amp;lt;chr&amp;gt;, bike_type &amp;lt;chr&amp;gt;, and
## #   abbreviated variable names ¹​end_time, ²​start_station, ³​start_lat,
## #   ⁴​start_lon, ⁵​end_station&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(bike)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 297792     15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose we were interested in the following research questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the trip route, type of pass, and bike type explain variation in the duration the bike was rented?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s explore this descriptively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ duration, data = bike) %&amp;gt;%
  gf_labs(x = &amp;quot;Duration, in minutes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_violin(bike_type ~ duration, data = bike, fill = &amp;#39;gray85&amp;#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&amp;gt;%
  gf_labs(x = &amp;quot;Duration, in minutes&amp;quot;, y = &amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_violin(passholder_type ~ duration, data = bike, fill = &amp;#39;gray85&amp;#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&amp;gt;%
  gf_labs(x = &amp;quot;Duration, in minutes&amp;quot;, y = &amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_violin(trip_route_category ~ duration, data = bike, fill = &amp;#39;gray85&amp;#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&amp;gt;%
  gf_labs(x = &amp;quot;Duration, in minutes&amp;quot;, y = &amp;quot;&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;regression-model-with-multiple-categorical-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression model with multiple categorical predictors&lt;/h2&gt;
&lt;p&gt;Similar to multiple linear regression, including multiple categorical predictors is similar to that case. The simplest model is the additive model, also commonly referred to as the main effect model. Let’s start with two categorical attributes that take on two different values.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
duration = \beta_{0} + \beta_{1} bike\_type + \beta_{2} trip\_route\_category + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_lm &amp;lt;- lm(duration ~ bike_type + trip_route_category, data = bike)

broom::glance(bike_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    0.0177     0.0176  15.2   2676.       0     2 -1.23e6 2.47e6 2.47e6  6.90e7
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(bike_lm) |&amp;gt;
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term                          estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                            &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                     14.9       0.048     310.        0
## 2 bike_typestandard                0.375     0.059       6.4       0
## 3 trip_route_categoryRound Trip    7.25      0.099      73.0       0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_way_predict &amp;lt;- broom::augment(bike_lm) %&amp;gt;%
   distinct(bike_type, trip_route_category, .fitted)

two_way_predict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 3
##   bike_type trip_route_category .fitted
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;
## 1 standard  One Way                15.3
## 2 electric  One Way                14.9
## 3 electric  Round Trip             22.2
## 4 standard  Round Trip             22.6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_duration &amp;lt;- df_stats(duration ~ bike_type + trip_route_category, data =bike, mean, length)
mean_duration&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response bike_type trip_route_category     mean length
## 1 duration  electric             One Way 15.21481  93807
## 2 duration  standard             One Way 15.16518 178219
## 3 duration  electric          Round Trip 19.52470   9819
## 4 duration  standard          Round Trip 24.19370  15947&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = two_way_predict, size = 5) %&amp;gt;%
  gf_line(size = 1.5, group = ~ trip_route_category) %&amp;gt;%
  gf_labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Model Predicted Values&amp;quot;, color = &amp;#39;Trip Route&amp;#39;) %&amp;gt;%
  gf_point(mean ~ bike_type, color = ~ trip_route_category, data = mean_duration, shape = 15, size = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Back to the coefficients to really understand what these are.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(bike_lm) |&amp;gt;
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term                          estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                            &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                     14.9       0.048     310.        0
## 2 bike_typestandard                0.375     0.059       6.4       0
## 3 trip_route_categoryRound Trip    7.25      0.099      73.0       0&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Intercept&lt;/em&gt;: The average bike rental duration for the reference group (electric bikes and one-way trips).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bike_typestandard&lt;/em&gt;: This is like a slope, so for a one unit change in bike type (ie, moving from an electric bike to a standard bike), the estimated mean change in bike duration decreased by 0.375 minutes, holding other attributes constant.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;trip_route_categoryRound Trip&lt;/em&gt;: This is again like a slope, for a one unit change in trip route (ie, moving from a one-way trip to a round trip), the estimated mean change in bike duraction increased by 7.246 minutes, holder other attributes constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are like weighted marginal means, where the weighting is occuring due to different sample sizes among the groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count(bike, bike_type, trip_route_category)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 3
##   bike_type trip_route_category      n
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;
## 1 electric  One Way              93807
## 2 electric  Round Trip            9819
## 3 standard  One Way             178219
## 4 standard  Round Trip           15947&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(duration ~ bike_type + trip_route_category, data = bike, mean) %&amp;gt;%
  pivot_wider(names_from = &amp;#39;trip_route_category&amp;#39;, values_from = &amp;quot;mean&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 4
##   response bike_type `One Way` `Round Trip`
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 duration electric       15.2         19.5
## 2 duration standard       15.2         24.2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;interaction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interaction Model&lt;/h3&gt;
&lt;p&gt;The interaction model expands on the main effect only model, by allowing the effect of one category to differ based on elements of the other category. One way to think about this in the case with two categorical attributes, is that the mean change differs based on levels of the second attribute. This model, in the case where the attributes are each two groups, adds one additional term.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
duration = \beta_{0} + \beta_{1} bike\_type + \beta_{2} trip\_route\_category + \beta_{3} bike\_type:trip\_route\_category + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_lm_int &amp;lt;- lm(duration ~ bike_type + trip_route_category + bike_type:trip_route_category, data = bike)

broom::glance(bike_lm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    0.0194     0.0194  15.2   1964.       0     3 -1.23e6 2.47e6 2.47e6  6.89e7
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(bike_lm_int)  |&amp;gt;
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term                                           estim…¹ std.e…² stati…³ p.value
##   &amp;lt;chr&amp;gt;                                            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                                      15.2    0.05  306.      0    
## 2 bike_typestandard                                -0.05   0.061  -0.809   0.419
## 3 trip_route_categoryRound Trip                     4.31   0.161  26.7     0    
## 4 bike_typestandard:trip_route_categoryRound Tr…    4.72   0.205  23.1     0    
## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_way_predict_int &amp;lt;- broom::augment(bike_lm_int) %&amp;gt;%
   distinct(bike_type, trip_route_category, .fitted)

two_way_predict_int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 3
##   bike_type trip_route_category .fitted
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;
## 1 standard  One Way                15.2
## 2 electric  One Way                15.2
## 3 electric  Round Trip             19.5
## 4 standard  Round Trip             24.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = two_way_predict_int, size = 5) %&amp;gt;%
  gf_line(size = 1.5, group = ~ trip_route_category) %&amp;gt;%
  gf_labs(x = &amp;quot;&amp;quot;, y = &amp;quot;Model Predicted Values&amp;quot;, color = &amp;#39;Trip Route&amp;#39;) %&amp;gt;%
  gf_point(mean ~ bike_type, color = ~ trip_route_category, data = mean_duration, shape = 15, size = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Back to the coefficients and what do these mean here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(bike_lm_int)  |&amp;gt;
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term                                           estim…¹ std.e…² stati…³ p.value
##   &amp;lt;chr&amp;gt;                                            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                                      15.2    0.05  306.      0    
## 2 bike_typestandard                                -0.05   0.061  -0.809   0.419
## 3 trip_route_categoryRound Trip                     4.31   0.161  26.7     0    
## 4 bike_typestandard:trip_route_categoryRound Tr…    4.72   0.205  23.1     0    
## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Intercept&lt;/em&gt;: The average bike rental duration for the reference group (electric bikes and one-way trips).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bike_typestandard&lt;/em&gt;: This is like a slope, so for a one unit change in bike type (ie, moving from an electric bike to a standard bike), the estimated mean change in bike duration decreased by .05 minutes, holding other attributes constant.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;trip_route_categoryRound Trip&lt;/em&gt;: This is again like a slope, for a one unit change in trip route (ie, moving from a one-way trip to a round trip), the estimated mean change in bike duraction increased by 4.3 minutes, holder other attributes constant.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bike_typestandard:trip_route_categoryRound Trip&lt;/em&gt;: This is the interaction effect and is the &lt;strong&gt;additional&lt;/strong&gt; mean level change for standard bikes &lt;em&gt;and&lt;/em&gt; round trips, holding other attributes constant.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can get the estimated means for the 4 groups as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_{elec-1way} = 15.2
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_{elec-RT} = 15.2 + 4.3
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_{stand-1way} = 15.2 - .05
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_{stand-RT} = 15.2 - 0.05 + 4.3 + 4.7
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;What about the example with two categorical attributes bike type and passholder type? Fit this model below and interpret the parameter estimates, both one without and with interaction effects.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;second-order-three-way-interaction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second order (three-way) interaction&lt;/h2&gt;
&lt;p&gt;The more attributes that interact with one another makes the model more complicated and difficult to interpret. Still, let’s try a second order or three way interaction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bike_lm_3way &amp;lt;- lm(duration ~ bike_type * trip_route_category * passholder_type, data = bike)

broom::glance(bike_lm_3way)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.sq…¹ sigma stati…² p.value    df  logLik    AIC    BIC devia…³
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    0.0617     0.0617  14.9   1780.       0    11 -1.23e6 2.45e6 2.45e6  6.59e7
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(bike_lm_3way) |&amp;gt;
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 5
##    term                                          estim…¹ std.e…² stati…³ p.value
##    &amp;lt;chr&amp;gt;                                           &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 (Intercept)                                    23.9     0.132 181.      0    
##  2 bike_typestandard                              -0.8     0.173  -4.63    0    
##  3 trip_route_categoryRound Trip                   0.426   0.314   1.36    0.174
##  4 passholder_typeIndego30                        -9.67    0.144 -67.3     0    
##  5 passholder_typeIndego365                      -11.9     0.185 -64.0     0    
##  6 bike_typestandard:trip_route_categoryRound T…   8.04    0.424  19.0     0    
##  7 bike_typestandard:passholder_typeIndego30       1.42    0.187   7.60    0    
##  8 bike_typestandard:passholder_typeIndego365      0.738   0.233   3.17    0.002
##  9 trip_route_categoryRound Trip:passholder_typ…   3.64    0.369   9.86    0    
## 10 trip_route_categoryRound Trip:passholder_typ…  -0.249   0.661  -0.377   0.706
## 11 bike_typestandard:trip_route_categoryRound T…  -3.78    0.49   -7.72    0    
## 12 bike_typestandard:trip_route_categoryRound T…  -2.47    0.801  -3.08    0.002
## # … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Interpreting these coefficients can be challenging, visualizing them can be a more effective way to undersand the impact these may have. The following steps will be used to visualize these model results:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate model-implied or predicted values for each combination of model values&lt;/li&gt;
&lt;li&gt;Plot those model implied means&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;three_way_predict_int &amp;lt;- broom::augment(bike_lm_3way) %&amp;gt;%
   distinct(bike_type, trip_route_category, passholder_type, .fitted)

three_way_predict_int&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 4
##    bike_type trip_route_category passholder_type .fitted
##    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;
##  1 standard  One Way             Indego30           14.8
##  2 electric  One Way             Indego30           14.2
##  3 standard  One Way             Indego365          12.0
##  4 electric  Round Trip          Day Pass           24.3
##  5 standard  One Way             Day Pass           23.1
##  6 standard  Round Trip          Indego30           23.2
##  7 standard  Round Trip          Day Pass           31.5
##  8 electric  One Way             Day Pass           23.9
##  9 electric  Round Trip          Indego30           18.3
## 10 electric  One Way             Indego365          12.0
## 11 standard  Round Trip          Indego365          17.7
## 12 electric  Round Trip          Indego365          12.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = three_way_predict_int, size = 5) %&amp;gt;%
  gf_line(size = 1.5, group = ~ trip_route_category) %&amp;gt;%
  gf_facet_wrap(~ passholder_type) %&amp;gt;%
  gf_labs(y = &amp;quot;&amp;quot;, x = &amp;quot;Model Predicted Values&amp;quot;, color = &amp;#39;Trip Route&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/13-multiple-categorical_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of Covariance</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/14-ancova/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/14-ancova/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;analysis-of-covariance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis of Covariance&lt;/h1&gt;
&lt;p&gt;This section of notes will explore an analysis procedure classically known as Analysis of Covariance (ANCOVA). This methodology is aimed to answer the following question, &lt;em&gt;Is there a difference in groups, after controlling for other covariates&lt;/em&gt;. Here, covariates refer to another attribute in the data that is thought to influence or explain variation in the outcome, therefore, this method is meant to remove, control, or partition the variance associated with the covariate (or multiple covariates) to evaluate the conditional or adjusted means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.4.1 
## ✔ readr   2.1.2      ✔ forcats 0.5.2 
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
##  learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
##  learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)

theme_set(theme_bw(base_size = 18))

baby &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/baby.csv&amp;quot;) %&amp;gt;%
   mutate(gestational_mean = gestational_days - mean(gestational_days),
          maternal_weight_mean = maternal_pregnancy_weight - mean(maternal_pregnancy_weight),
          maternal_age_mean = maternal_age - mean(maternal_age),
          maternal_height_mean = maternal_height - mean(maternal_height))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1174 Columns: 6
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## dbl (5): birth_weight, gestational_days, maternal_age, maternal_height, mate...
## lgl (1): maternal_smoker
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(baby)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 10
##   birth_weight gestati…¹ mater…² mater…³ mater…⁴ mater…⁵ gesta…⁶ mater…⁷ mater…⁸
##          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1          120       284      27      62     100 FALSE     4.90   -28.5   -0.228
## 2          113       282      33      64     135 FALSE     2.90     6.52   5.77 
## 3          128       279      28      64     115 TRUE     -0.101  -13.5    0.772
## 4          108       282      23      67     125 TRUE      2.90    -3.48  -4.23 
## 5          136       286      25      62      93 FALSE     6.90   -35.5   -2.23 
## 6          138       244      33      62     178 FALSE   -35.1     49.5    5.77 
## # … with 1 more variable: maternal_height_mean &amp;lt;dbl&amp;gt;, and abbreviated variable
## #   names ¹​gestational_days, ²​maternal_age, ³​maternal_height,
## #   ⁴​maternal_pregnancy_weight, ⁵​maternal_smoker, ⁶​gestational_mean,
## #   ⁷​maternal_weight_mean, ⁸​maternal_age_mean&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s first explore the model that only includes gestational days in the model, that is, this is a simple linear regression model. &lt;em&gt;Note&lt;/em&gt;: I mean centered gestational days for this model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight = \beta_{0} + \beta_{1} days + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baby_lm &amp;lt;- lm(birth_weight ~ gestational_mean, data = baby)

glance(baby_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     0.166        0.165  16.7    233. 3.40e-48     1 -4973. 9953. 9968. 328608.
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(baby_lm) |&amp;gt; 
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term             estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)       119.        0.489     244.        0
## 2 gestational_mean    0.467     0.031      15.3       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s next explore another simple linear regression model that only includes whether a mother smoked or not. This model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight = \beta_{0} + \beta_{2} smoker + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baby_smoker &amp;lt;- lm(birth_weight ~ maternal_smoker, data = baby)

glance(baby_smoker)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.sq…¹ sigma stati…²  p.value    df logLik    AIC    BIC devia…³
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    0.0609     0.0601  17.8    76.0 9.46e-18     1 -5043. 10092. 10107. 370056.
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(baby_smoker) |&amp;gt; 
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term                estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)           123.       0.665    185.         0
## 2 maternal_smokerTRUE    -9.27     1.06      -8.72       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea behind ANCOVA is that we know that gestational days is an important predictor in understanding variation in the birth weight of the baby. This may not be a predictor that is of most interest to our research question of knowing whether or not there are differences in birth weight between those that smoked vs did not smoke when the baby was in the womb. There may be some sampling bias occurring, that is, those that smoked may have fewer gestational days compared to those that did not smoke. This fact may drive the differences in the mean difference of birth weight. As a results, we would want to adjust for the effect of gestational days prior to evaluating the mean difference of smoker status.&lt;/p&gt;
&lt;p&gt;Below is a descriptive figure trying to show the relationship between birth weight and gestational days by smoker status.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(birth_weight ~ gestational_days, data = baby, color = ~maternal_smoker, size = 4) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, size = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/14-ancova_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below, let’s fit the traditional ANCOVA model, that contains a single continuous covariate (gestational days) and the categorical predictor. The model would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight = \beta_{0} + \beta_{1} days + \beta_{2} smoker + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baby_ancova &amp;lt;- lm(birth_weight ~ gestational_mean + maternal_smoker, data = baby)

glance(baby_ancova)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     0.216        0.214  16.2    161. 1.71e-62     2 -4937. 9883. 9903. 309075.
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(baby_ancova) |&amp;gt; 
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term                estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)          123.        0.608    202.         0
## 2 gestational_mean       0.451     0.03      15.2        0
## 3 maternal_smokerTRUE   -8.37      0.973     -8.60       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model above can be partioned to better show the specific terms in the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{smoker} = (\beta_{0} + \beta_{2}) + \beta_{1} days + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{non-smoker} = \beta_{0} + \beta_{1} days + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More explicitly, the term, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; above reflects a mean level change on the intercept after controlling for the effects of gestational days.&lt;/p&gt;
&lt;div id=&#34;adjusted-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusted Means&lt;/h2&gt;
&lt;p&gt;Let’s compare the means from this ANCOVA model compared to the unconditional means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;uncond_mean &amp;lt;- df_stats(birth_weight ~ maternal_smoker, data = baby, mean) %&amp;gt;%
   mutate(maternal_smoker = as.character(maternal_smoker))

cond_mean &amp;lt;- data.frame(maternal_smoker = c(&amp;#39;FALSE&amp;#39;, &amp;#39;TRUE&amp;#39;), cond_mean = c(122.74, 122.74 - 8.37))

combine_means &amp;lt;- full_join(uncond_mean, cond_mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;maternal_smoker&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combine_means&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       response maternal_smoker     mean cond_mean
## 1 birth_weight           FALSE 123.0853    122.74
## 2 birth_weight            TRUE 113.8192    114.37&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, the conditional mean used the fact that the gestational days were mean centered, therefore, a value of 0 was included in the above equations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction-between-continuous-and-categorical-predictor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interaction between continuous and categorical predictor&lt;/h2&gt;
&lt;p&gt;One assumption in classical ANCOVA is to assume that the effect of the adjusting covariate is the same for each group. Although, this is a classical assumption, this is an empirical question that we can explore given the data. This can be explored by the introduction of an interaction.&lt;/p&gt;
&lt;p&gt;This new model would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight = \beta_{0} + \beta_{1} days + \beta_{2} smoker + \beta_{3} days:smoker + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the &lt;span class=&#34;math inline&#34;&gt;\(:\)&lt;/span&gt; in the equation above represents the interaction effect between gestational days and smoker status. An interaction is testing if the slope for gestational days is the same for those that smoke vs those that did not smoke.&lt;/p&gt;
&lt;p&gt;More explicitly, we can partition the model above into the following two regression equations.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{smoker} = (\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{3}) days + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{non-smoker} = \beta_{0} + \beta_{1} days + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baby_lm_int &amp;lt;- lm(birth_weight ~ gestational_mean * maternal_smoker, data = baby)
# baby_lm_int &amp;lt;- lm(birth_weight ~ gestational_mean + maternal_smoker + gestational_mean:maternal_smoker, 
#    data = baby)

glance(baby_lm_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     0.225        0.223  16.2    113. 2.41e-64     3 -4930. 9871. 9896. 305427.
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(baby_lm_int) |&amp;gt; 
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term                                 estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                           123.        0.605    203.         0
## 2 gestational_mean                        0.37      0.037     10.1        0
## 3 maternal_smokerTRUE                    -8.26      0.969     -8.52       0
## 4 gestational_mean:maternal_smokerTRUE    0.231     0.062      3.74       0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the 4 terms represent now?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; is the intercept for the reference group (non-smokers) when all other attributes in the model are equal to 0.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; is the slope for the reference group (non-smokers), controlling for the smoker status.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; is the average change for those that smoke, controlling for differences in gestational days.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt; is the average slope difference for those that smoke compared to those that did not smoke.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Often, it is easier to combine the terms to be compared directly. For example, given the example above, we could combine &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; to get the estimated intercept for those that smoke. Then we could do something similar for the slope terms for those that smoke.&lt;/p&gt;
&lt;div id=&#34;hypotheses-being-tested&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypotheses being tested&lt;/h3&gt;
&lt;p&gt;The hypotheses being tested here for the &lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{3}\)&lt;/span&gt; are as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \beta_{2} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \beta_{3} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These can be a little bit untenable, therefore stating them in words can help.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_{0}: \beta_{2} = 0\)&lt;/span&gt;, is really testing, is there a mean difference between the reference and focal groups, controlling for the other terms in the model. This is inherently a conditional or adjusted means question.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(H_{0}: \beta_{3} = 0\)&lt;/span&gt;, is really testing, is there a slope difference comparing the focal and reference groups, controlling for other terms in the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;including-more-than-1-covariate-to-adjustcondition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Including more than 1 covariate to adjust/condition&lt;/h2&gt;
&lt;p&gt;It is also possible to adjust for more than one covariate. For example, maybe we thought adjusting for the weight, height, and age of the mother would also be a good idea. This can be done to remove any variation associated with that attribute prior to making exploring the mean difference in the attributes of interest.&lt;/p&gt;
&lt;p&gt;This model would now be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight = \beta_{0} + \beta_{1} days + \beta_{4} mother\_weight + \beta_{5} mother\_height + \beta_{6} mother\_age + \beta_{2} smoker + \beta_{3} days:smoker + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These can also be partitioned into two separate equations.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{smoker} = (\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{3}) days + \beta_{4} mother\_weight + \beta_{5} mother\_height + \beta_{6} mother\_age + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
weight_{non-smoker} = \beta_{0} + \beta_{1} days + \beta_{4} mother\_weight + \beta_{5} mother\_height + \beta_{6} mother\_age + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baby_lm_int_w &amp;lt;- lm(birth_weight ~ gestational_mean + maternal_pregnancy_weight + maternal_height + maternal_age + maternal_smoker + gestational_mean:maternal_smoker, 
    data = baby)

glance(baby_lm_int_w)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squa…¹ sigma stati…²  p.value    df logLik   AIC   BIC devia…³
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     0.260        0.256  15.8    68.3 6.14e-73     6 -4903. 9822. 9863. 291631.
## # … with 2 more variables: df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;, and abbreviated
## #   variable names ¹​adj.r.squared, ²​statistic, ³​deviance&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(baby_lm_int_w) |&amp;gt; 
  mutate_if(is.double, round, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 5
##   term                                 estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                                   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)                            43.5      12.3       3.54    0    
## 2 gestational_mean                        0.365     0.036    10.1     0    
## 3 maternal_pregnancy_weight               0.052     0.025     2.06    0.039
## 4 maternal_height                         1.10      0.204     5.41    0    
## 5 maternal_age                            0.07      0.081     0.873   0.383
## 6 maternal_smokerTRUE                    -8.20      0.952    -8.62    0    
## 7 gestational_mean:maternal_smokerTRUE    0.208     0.061     3.44    0.001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How could the appropriate conditional means be computed here? Also, what happened to the intercept in this model, why is it so small?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
