[{"authors":["admin"],"categories":null,"content":"\nThe course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.g., Pearson correlation).\nThis course will focus on using statistical methods to answer research questions and make decisions from data. General linear models (i.e., regression) will serve as the fundamental building block that students will be exposed to answer questions about associations of quantitative outcomes with quantitative and categorical predictors. Statistical estimation, model building, and inference using statistical software will serve as primary topics which students will gain a stronger understanding by the end of the course.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://psqf6243.brandonlebeau.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.","tags":null,"title":"","type":"authors"},{"authors":["brandon"],"categories":null,"content":"Brandon LeBeau\nI\u0026rsquo;m interested in computational methods, longitudinal data, statistical software development with R, and quantitative program evaluation. You can see more about my interests on my website: https://brandonlebeau.org/.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a6b366d06474d85d9f788b8d18e8310d","permalink":"https://psqf6243.brandonlebeau.org/authors/brandon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brandon/","section":"authors","summary":"Brandon LeBeau\nI\u0026rsquo;m interested in computational methods, longitudinal data, statistical software development with R, and quantitative program evaluation. You can see more about my interests on my website: https://brandonlebeau.org/.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"A list of the notes used for the semester.\n Review Linear Regression Intro  In Class Activity In Class Notes   Regression Estimates Least Square Simulation Regression Inference  Regression in class activity   Conditions for Regression Inference Bootstrap  Bootstrap - In class   Multiple Regression  Multiple Regression - In Class   Model Comparison Regression with Categorical Attributes Pairwise Comparisons Multiple Categorical Predictors Analysis of Covariance Pre / Post Designs Non-Linear Chi-Square  ","date":1661126400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1661126400,"objectID":"87e0410d95d74ae914d8a58b3f6a1716","permalink":"https://psqf6243.brandonlebeau.org/lectures/","publishdate":"2022-08-22T00:00:00Z","relpermalink":"/lectures/","section":"lectures","summary":"Lecture Notes","tags":null,"title":"Lecture Notes","type":"book"},{"authors":null,"categories":null,"content":"This page contains the syllabus for the course.\n This syllabus is an attempt early in the semester to plan for the course. This syllabus is subject to change at the Instructors discretion.  Course Information PSQF 6243: Intermediate Statistical Methods - Fall 2023\nInstructor Information  Brandon LeBeau, Ph.D. Email: brandon-lebeau at uiowa.edu Virtual Office Hours (Zoom): Tues 9:30 am to 11 am or by appointment  See ICON for office hours zoom link   Department: Psychological and Quantitative Foundations, 361 LC  Interim DEO: Dr. Dennis Martin Kivlighan,, 361 LC    Teaching Assistant Information  Alfonso J. Martinez Email: alfonso-martinez at uiowa.edu Office Hours: M/W 10 am - 12 pm - N476 LC  Course Quote Data does not give up their secrets easily. They must be tortured to confess. \u0026ndash; Jeff Hooper, Bell Labs\nCourse Description The course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.g., Pearson correlation).\nThis course will focus on using statistical methods to answer research questions and make decisions from data. General linear models (i.e., regression) will serve as the fundamental building block that students will be exposed to answer questions about associations of quantitative outcomes with quantitative and categorical predictors. Statistical estimation, model building, and inference using statistical software will serve as primary topics which students will gain a stronger understanding by the end of the course.\nCourse Objectives The course requirements and class materials are aimed to help students achieve the following course objectives:\n become more fluent in statistical terminology, use statistical software, turn research questions into actionable statistical methods, engage with statistical theory, connect model equations with statistical code, summarize uncertainty and variation in estimation.  Textbook  Introduction to Statistical Learning (2nd edition), by James, Witten, Hastie, and Tibshirani: https://www.statlearning.com/ Supplemental Text:  Statistical Reasoning through Computation and R, by LeBeau and Zieffler: https://lebebr01.github.io/stat_thinking/ Introduction to Modern Statistics, by Cetinkaya-Rundel and Hardin. Found online here: https://openintro-ims.netlify.app/ Applied Regression: An Introduction, by Lewis-Beck and Lewis-Beck. Online access via library: https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n1.   Class notes and Jupyter Notebooks used in class will also be shared through the course website.  Course Requirements   Online Quizzes: There will be a total of 10 online quizzes taken through ICON. Each quiz will be available for 5 to 7 days and are open note/book, but should be completed independently and will need to be completed in a single occasion. Expect each quiz to be about 10 questions. The dates will be discussed in class and announcements will be sent by email to communicate when quizzes open and close. Quizzes will account for 40% of the course grade.\n  Assignments: 4 assignments spread throughout the course will be used to improve student understanding and give students practice to work with data. These assignments can be completed within small groups of no more than 3 students. If completed as a group, all students will receive the same score and only one assignment with everyone\u0026rsquo;s name on it needs to be turned in. Assignments will account for 40% of the course grade.\n  Activities: Approximately 6 activities will be posted across the semester. These are complete/incomplete activities, therefore, completing these will give you 20% of the final course grade. Completion of these involve posting answers to the questions that show you interacted and engaged in the activity. Answers that are not on topic or missing will not count as \u0026ldquo;passing\u0026rdquo; for that activity. These activities are meant to be formative assessments to give you time to practice interpreting statistics. Similar to the assignments, you may work in groups of up to 3 to complete these activities, in which you would all receive the same pass/fail grade for the activity.\n  Absences: Absences happen. Therefore, I ask you to be as transparent as possible with me. I promise to be compassionate and understanding. If at any point in the semester you are having difficulties, please reach out to me and I will do my best to be accommodating and provide support, which could include an extension on course deadlines as necessary.\n  Grading: Final grades will be based on the following weighting scheme:\n 10 quizzes, equally weighted, 40% of total grade 4 assignments, equally weighted, 40% of total grade About 6 activities, equally weighted, 20% of total grade    Percentage Breakdown: Guidelines are given below, plus and minus grades will be given as well. Changes may occur to the grade percentage breakdown below, but the percentages will not get higher. For example, the lower percentage to get a B, could drop to 75%, but will never be higher than 80%. I will do my best to communicate any shifts to these grade ranges as the semester progresses.\n A 90% and above B 80% up to 90% C 70% up to 80% D Below 70%    Course and University Policies  Announcements and Communication: Any announcements regarding the course will be communicated via email so please check it daily.  Email Expectations: I want to be transparent regarding course email. Email is not meant to be an instantaneous exchange, rather, email is asynchronous. I typically will respond to an email within one business day. Note, I typically will not respond to email over the weekend or during evening hours, but there could be exceptions to this. If I do send an email during the evening or weekend, there is no expectation that you reply immediately as well. If a reply is warranted, reply on your own time. Please be advised that this applies to all elements of the semester, even around due dates. If you email a question the day an assignment is due, I may not be able to get back to you until the next day   Course Materials: Course materials will be posted to ICON or the course website. Go to icon.uiowa.edu for access to the ICON site and the primary course website for PSQF 6243. Adaptations and Modifications: Please inform me during the first two weeks if you require special adaptations or modifications to any assignment or due dates because of special circumstances such as learning disabilities, religious observances, or other appropriate needs. Contesting a Grade: To contest a grade, please send me an email detailing your reason within 48 hours of receiving the grade. This allows both of us time to think, reflect, and discuss the matter without taking class time from other students. When contesting a grade, provide a copy of the graded assignment. Academic Misconduct: Plagiarism and cheating may result in grade reduction and/or serious penalties. Unless you are otherwise instructed, your work should be entirely your own. Please take care in writing your final project. You should always be writing in your own words, citing others' ideas, and quoting text as appropriate. This link provides the College of Education policy on student academic misconduct. Student Complaint Procedures: The College of Education policy on student complaints about faculty action Use of AI: Students are invited to use AI platforms to help prepare for assignments and projects (e.g., to help with brainstorming or to test your own knowledge about specific course topics). I also welcome you to use AI tools to help revise and edit your work (e.g., to help identify flaws in reasoning, spot confusing or underdeveloped paragraphs, or to simply fix citations). If you use AI tools to aid in writing or editing any written material you submit for the course, please make sure you cite the tool and indicate how you fact checked the material.  University Policies The following University of Iowa course policies are published by the Provost office.\nAt the University of Iowa, we strive for a classroom or laboratory climate that encourages learning while also protecting the freedoms and rights of our students and faculty.\nPlease review the following course policies, expectations, and resources. Visit the Dean of Students website for additional student policies and procedures.\n Free Speech and Expression: The University of Iowa supports and upholds the First Amendment protection of freedom of speech and the principles of academic and artistic freedom. We are committed to open inquiry, vigorous debate, and creative expression inside and outside of the classroom. Visit the Free Speech at Iowa website for more information on the university’s policies on free speech and academic freedom. Accommodations for Students with Disabilities: The University is committed to providing an educational experience that is accessible to all students. If a student has a diagnosed disability or other disabling condition that may impact the student’s ability to complete the course requirements as stated in the syllabus, the student may seek accommodations through Student Disability Services (SDS). SDS is responsible for making Letters of Accommodation (LOA) available to the student. The student must provide a LOA to the instructor as early in the semester as possible, but requests not made at least two weeks prior to the scheduled activity for which an accommodation is sought may not be accommodated. The LOA will specify what reasonable course accommodations the student is eligible for and those the instructor should provide. Additional information can be found on the SDS website. Absences from Class: University regulations require that students be allowed to make up examinations which have been missed due to illness, religious holy days, military service obligations, including service-related medical appointments, jury duty, or other unavoidable circumstances or other University-sponsored activities. Students should work with faculty regarding making up other missed work, such as assignments, quizzes, and classroom attendance. Absences for Religious Holy Days: The University is prepared to make reasonable accommodations for students whose religious holy days coincide with their classroom assignments, test schedules, and classroom attendance expectations. Students must notify their instructors in writing of any such Religious Holy Day conflicts or absences within the first few days of the semester or session, and no later than the third week of the semester. If the conflict or absence will occur within the first three weeks of the semester, the student should notify the instructor as soon as possible. See Operations Manual 8.2 Absences for Religious Holy Days for additional information. Absences for Military Service Obligations: Students absent from class or class-related requirements due to U.S. veteran or U.S. military service obligations (including military service–related medical appointments, military orders, and National Guard Service obligations) shall be excused without any grading adjustment or other penalty. Instructors shall make reasonable accommodations to allow students to make up, without penalty, tests and assignments they missed because of veteran or military service obligations. Reasonable accommodations may include making up missed work following the service obligation; completing work in advance; completing an equivalent assignment; or waiver of the assignment without penalty. In all instances, students bear the responsibility to communicate with their instructors about such veteran or military service obligations, to meet course expectations and requirements. Classroom Expectations: Students are expected to comply with University policies regarding appropriate classroom behavior as outlined in the Code of Student Life. While students have the right to express themselves and participate freely in class, it is expected that students will behave with the same level of courtesy and respect in the virtual class setting (whether asynchronous or synchronous) as they would in an in-person classroom. Failure to follow behavior expectations as outlined in the Code of Student Life may be addressed by the instructor and may also result in discipline under the Code of Student Life policies governing E.5 Disruptive Behavior or E.6 Failure to Comply with University Directive. Non-Discrimination Statement: The University of Iowa prohibits discrimination in employment, educational programs, and activities on the basis of race, creed, color, religion, national origin, age, sex, pregnancy, disability, genetic information, status as a U.S. veteran, service in the U.S. military, sexual orientation, gender identity, associational preferences, or any other classification that deprives the person of consideration as an individual. The university also affirms its commitment to providing equal opportunities and equal access to university facilities. For additional information on nondiscrimination policies, contact the Director, Office of Institutional Equity, the University of Iowa, 202 Jessup Hall, Iowa City, IA 52242-1316, 319-335-0705, oie-ui@uiowa.edu. Students may share their pronouns and chosen/preferred names in MyUI, which is accessible to instructors and advisors. Sexual Harassment/Sexual Misconduct and Supportive Measures: The University of Iowa prohibits all forms of sexual harassment, sexual misconduct, and related retaliation. The Policy on Sexual Harassment and Sexual Misconduct governs actions by students, faculty, staff and visitors. Incidents of sexual harassment or sexual misconduct can be reported to the Title IX and Gender Equity Office or to the Department of Public Safety. Students impacted by sexual harassment or sexual misconduct may be eligible for academic supportive measures and can learn more by contacting the Title IX and Gender Equity Office. Information about confidential resources can be found here. Watch the video for an explanation of these resources. Mental Health: Students are encouraged to be mindful of their mental health and seek help as a preventive measure or if feeling overwhelmed and/or struggling to meet course expectations. Students are encouraged to talk to their instructor for assistance with specific class-related concerns. For additional support and counseling, students are encouraged to contact University Counseling Service (UCS). Information about UCS, including resources and how to schedule an appointment, can be found at counseling.uiowa.edu. Find out more about UI mental health services at: mentalhealth.uiowa.edu. Basic Needs and Support for Students: It can be difficult to maintain focus and be present if you are experiencing challenges with meeting basic needs or navigating personal crisis situations. The Office of the Dean of Students can help. Contact us for one-on-one support, identifying options, and to locate and access basic needs resources (such as food, rent, childcare, etc.). Student Care and Assistance, 132 IMU, dos-assistance@uiowa.edu, 319-335-1162. Basic Needs Info: Food Pantry at Iowa, Clothing Closet, Basic Needs and Support Resource Sharing of Class Recordings: Students may be enrolled in a class where some sessions will be recorded or live-streamed. Such recordings/streaming will only be available to students registered for the class. These recordings are the intellectual property of the faculty and they may not be shared or reproduced without the explicit, written consent of the faculty member. Further, students may not share these sessions with those not in the class or upload them to any other online environment. Doing so would be a breach of the Code of Student Conduct, and, in some cases, a violation of state and federal law, including the Federal Education Rights and Privacy Act (FERPA). This course is provided by the College of Education and cross-listed with the Statistics Department. Policies on matters such as course requirements, grading, and sanctions for academic dishonesty are governed by the College of Education. Students wishing to add or drop this course after the official deadline must receive approval of the Dean of the College of Education. See the College of Education policy on cross enrollments.  ","date":1660694400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1660694400,"objectID":"5538b9800f06ab7b29edaa22ab7e63cb","permalink":"https://psqf6243.brandonlebeau.org/syllabus/","publishdate":"2022-08-17T00:00:00Z","relpermalink":"/syllabus/","section":"syllabus","summary":"Course syllabus","tags":null,"title":"Syllabus","type":"book"},{"authors":null,"categories":null,"content":" R Resources  PSQF 6250 Course Materials - This is my course, usually offered during the spring semester. R for Data Science   SPSS Resources  Regression with SPSS SPSS Tutorials   SAS Resources  Regression with SAS SAS Tutorial   Stata Resources  Regression with Stata Stata Tutorials    ","date":1630022400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1630022400,"objectID":"b3827c9680770e0f03c58ccf7bc48d61","permalink":"https://psqf6243.brandonlebeau.org/code/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/code/","section":"code","summary":"Software Resources","tags":null,"title":"Software Resources","type":"book"},{"authors":null,"categories":null,"content":"A general overview of the semester. This is subject to change and we may not reach all of the course topics planned at the beginning of the semester. Readings/objects for specific topics will be posted to the course content pages.\nPart 1 - Linear Regression  Review / Exploring univariate and multivariate distributions Introduction to statistical software Simple linear regression / correlation Ordinary least squares estimation Inference for linear regression Multiple linear regression Data Conditions for regression  Part 2 - Regression Special Cases 2-group designs (i.e., 2-sample t-tests) More than 2 group designs (i.e., ANOVA) Nonlinear effects of quantitative predictors Interaction effects  Part 3 – Special Topics (as time permits) Chi-square for association and independence Dichotomous Attributes as outcomes  ","date":1629590400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629590400,"objectID":"ee8dfa775d2ca48042bf50fe0819e70c","permalink":"https://psqf6243.brandonlebeau.org/schedule/","publishdate":"2021-08-22T00:00:00Z","relpermalink":"/schedule/","section":"schedule","summary":"Course schedule","tags":null,"title":"Schedule","type":"book"},{"authors":null,"categories":null,"content":"The course content will be organized by weeks. Each week will contain some text to:\n discuss the goals of the week the content to be covered relevant R syntax/notebook files.  Each week may also contain some information about assignments and links directly to those on ICON/IDAS.\n Welcome  IDAS Introduction   Week 1 Week 2 Week 3 Week 4 Week 5 Week 6 Week 7 Week 8 Week 9 Week 10 Week 11 Week 12 Week 13 Week 14 Week 15  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"d5be68294f12f9cfecf81ad87009adc6","permalink":"https://psqf6243.brandonlebeau.org/content/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Course Content","tags":null,"title":"Content","type":"book"},{"authors":null,"categories":null,"content":"Here you can view all of the course assignments for the semester. This will include the hands on assignments and the quizzes. The quizzes will provide a link to ICON to complete.\n Assignments Activities Quizzes  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"5d370553e45c580541e007200292c8d8","permalink":"https://psqf6243.brandonlebeau.org/assignments/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/assignments/","section":"assignments","summary":"Course Requirements","tags":null,"title":"Course Requirements","type":"book"},{"authors":null,"categories":null,"content":"A list of the data used within the course.\nMore details to come soon \u0026hellip; .\n","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"77c2d2ba0979ac9954ceb502de85c1ce","permalink":"https://psqf6243.brandonlebeau.org/data/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/data/","section":"data","summary":"Data for the course","tags":null,"title":"Data","type":"book"},{"authors":null,"categories":null,"content":" Assignment 1 Assignment 2 Assignment 3 Assignment 4  ","date":1664150400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1664150400,"objectID":"148b0563112c2a9006e85ec08153b0ce","permalink":"https://psqf6243.brandonlebeau.org/assignments/assignment/","publishdate":"2022-09-26T00:00:00Z","relpermalink":"/assignments/assignment/","section":"assignments","summary":"Course Assignments","tags":null,"title":"Assignments","type":"book"},{"authors":null,"categories":null,"content":"Below are the course quizzes.\n Quiz 1 Quiz 2 Quiz 3 Quiz 4 Quiz 5 Quiz 6 Quiz 7 Quiz 8 Quiz 9 Quiz 10  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"9ee2f1578158bc73334dba690602f1a2","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/assignments/quizzes/","section":"assignments","summary":"Course Quizzes","tags":null,"title":"Quizzes","type":"book"},{"authors":null,"categories":null,"content":" Activity 1 Activity 2 Activity 3 Activity 4 Activity 5  ","date":1631577600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631577600,"objectID":"58e07d20629c9d63bd16da101769c8d4","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/","publishdate":"2021-09-14T00:00:00Z","relpermalink":"/assignments/activity/","section":"assignments","summary":"Course Activities","tags":null,"title":"Activities","type":"book"},{"authors":null,"categories":null,"content":"  Review for PSQF 6243 This serves as a non-exhaustive review for the course. These are elements that I assume you have knowledge of prior to starting the course.\n Variable vs constant attributes Types of variables (ie., nominal, ordinal, integer, ratio) Descriptive Statistics (eg., mean, median, standard deviation, variance, percentiles) Higher order moments (eg., skewness and kurtosis) Exploring/summarizing univariate distributions (eg., histogram or density figure) What is a statistical model? Why do we use them? Population vs Sample  Examples Mario Kart 64 world record data:\n  variable class description    track character Track name  type factor Single or three lap record  shortcut factor Shortcut or non-shortcut record  player character Player’s name  system_played character Used system (NTSC or PAL)  date date World record date  time_period period Time as hms period  time double Time in seconds  record_duration double Record duration in days    # load some libraries library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(lubridate) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(e1071) theme_set(theme_bw(base_size = 18)) # load in some data mariokart \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv\u0026#39;) %\u0026gt;% mutate(year = year(date), month = month(date), day = month(date)) ## Rows: 2334 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (6): track, type, shortcut, player, system_played, time_period ## dbl (2): time, record_duration ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(mariokart) ## # A tibble: 6 × 12 ## track type shortcut player system_played date time_period time ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Luigi Raceway Thre… No Salam NTSC 1997-02-15 2M 12.99S 133. ## 2 Luigi Raceway Thre… No Booth NTSC 1997-02-16 2M 9.99S 130. ## 3 Luigi Raceway Thre… No Salam NTSC 1997-02-16 2M 8.99S 129. ## 4 Luigi Raceway Thre… No Salam NTSC 1997-02-28 2M 6.99S 127. ## 5 Luigi Raceway Thre… No Gregg… NTSC 1997-03-07 2M 4.51S 125. ## 6 Luigi Raceway Thre… No Rocky… NTSC 1997-04-30 2M 2.89S 123. ## # ℹ 4 more variables: record_duration \u0026lt;dbl\u0026gt;, year \u0026lt;dbl\u0026gt;, month \u0026lt;dbl\u0026gt;, day \u0026lt;dbl\u0026gt; # univariate distribution of time gf_histogram(~ time, data = mariokart, bins = 30) %\u0026gt;% gf_labs(x = \u0026quot;Time (in seconds)\u0026quot;) gf_density(~ time, data = mariokart) %\u0026gt;% gf_labs(x = \u0026quot;Time (in seconds)\u0026quot;) df_stats(~ time, data = mariokart, mean, median, sd, skewness, kurtosis, quantile(probs = c(0.1, 0.5, 0.9))) ## response mean median sd skewness kurtosis 10% 50% 90% ## 1 time 90.62383 86.19 66.6721 1.771732 3.844745 31.31 86.19 171.961   Bivariate Association cor(time ~ record_duration, data = mariokart) ## [1] -0.06736739 gf_point(time ~ record_duration, data = mariokart) %\u0026gt;% gf_labs(x = \u0026quot;How long the record was held\u0026quot;, y = \u0026quot;Time (in seconds)\u0026quot;) Questions What is problematic about the analyses above? Why? What could be done to improve the analyses above?    ","date":1692662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692662400,"objectID":"2824d49626915b3cf1d27b9ea2cf5334","permalink":"https://psqf6243.brandonlebeau.org/lectures/01-review/","publishdate":"2023-08-22T00:00:00Z","relpermalink":"/lectures/01-review/","section":"lectures","summary":"Review","tags":null,"title":"Review","type":"book"},{"authors":null,"categories":null,"content":"Getting Started  Review the syllabus Review the schedule Review, accessing the IDAS page  Jupyter Notebooks version of IDAS RStudio Server, to access you can switch via the \u0026ldquo;New\u0026rdquo; button on Jupyter Notebooks.   Optionally, but strongly encouraged, complete the course survey  ","date":1692316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692316800,"objectID":"1573444049fbe3c4964b394aa4c5174c","permalink":"https://psqf6243.brandonlebeau.org/content/00-getting-started/","publishdate":"2023-08-18T00:00:00Z","relpermalink":"/content/00-getting-started/","section":"content","summary":"Getting Started","tags":null,"title":"Welcome","type":"book"},{"authors":null,"categories":null,"content":"Introduction This page is meant as a way to give some introduction and how to access the IDAS used for the course.\nAccess The IDAS server can be accessed via the following link directly: Access IDAS. Upon going here, you will be prompted to log in with your HawkID. Finally, to use the IDAS, you will need to have DUO authentication activated.\nHere is a good link if you have not set up DUO authentication before from ITS. It may take a few hours or up to a day to be granted access to the IDAS after setting up DUO authentication for the first time, DUO Authentication Docs.\nIDAS Specifics The IDAS by default will launch in Jupyter Lab mode and will sync all course materials that I have posted for the course. The syncing of remote materials will never overwrite anything that you have manually edited by you, the student. For this reason, if I notice a breaking change, I usually post a new updated file with a new name.\nYou can also access RStudio server within the IDAS for the course as well. This can be accessed by clicking on \u0026ldquo;RStudio\u0026rdquo; from the launcher page. You should still have access to all of the course files that are automatically synced for you within the RStudio environment.\nShutting down IDAS When you are finished working in the IDAS, you should go to:\n File \u0026gt; Hub Control Panel \u0026gt; Stop My Server  This disconnects/stops the server instance so that the resources can be allocated to others wishing to use the service.\nCommon issues ITS has established a wiki page for common problems that occur and other documentation related to the IDAS.\n","date":1692316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692316800,"objectID":"4400f965f661bfec3d4ba2892522f833","permalink":"https://psqf6243.brandonlebeau.org/content/00a-idas/","publishdate":"2023-08-18T00:00:00Z","relpermalink":"/content/00a-idas/","section":"content","summary":"TBD","tags":null,"title":"IDAS Introduction","type":"book"},{"authors":null,"categories":null,"content":"  Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes. One of those attributes will be the outcome or attribute of interest whereas the other will used as a predictor. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable explains variation in the dependent variable (more on this later).\nOf note, variable is a typical word used in statistics, I’ve come to like the word attribute instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.\nWe may write this general model as:\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nWhere \\(Y\\) is the outcome attribute. It is also known as the dependent variable. The \\(X\\) term is the predictor/covariate attribute. It is also known as the independent variable. The \\(\\epsilon\\) is a random error term, more on this later. Finally, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown population coefficients that we are interested in estimating. More on this later too.\nSpecific example The data used for this section of the course is from the 2019 WNBA season. These data are part of the bayesrules package/book. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(ggformula) basketball \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv\u0026quot;) ## Rows: 146 Columns: 32 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): player_name, team ## dbl (29): height, weight, year, age, games_played, games_started, avg_minute... ## lgl (1): starter ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. theme_set(theme_bw(base_size = 18)) head(basketball) ## # A tibble: 6 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, …  Guiding Question Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.\nOne first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.\nThe next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) Questions to consider What can be noticed about the relationship between these two attributes? Does there appear to be a relationship between the two? Is this relationship perfect?    Adding a smoother line Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.\nI’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.\nDoes there appear to be much difference in the relationship across the two lines?\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39;  Estimating linear regression coefficients The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is lm() to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.\nThis equation could be written more directly for our specific problem.\n\\[ Avg\\_points = \\beta_{0} + \\beta_{1} Minutes\\_Played + \\epsilon \\]\nOne way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.\nFor the R formula, instead of an $ = $, you could insert a ~.\nwnba_reg \u0026lt;- lm(avg_points ~ total_minutes, data = basketball) coef(wnba_reg) ## (Intercept) total_minutes ## 1.13562456 0.01014207  Interpretting linear regression terms Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:\n\\[ \\hat{avg\\_points} = 1.1356 + .0101 min\\_played \\]\nWhere instead of \\(\\beta_{0}\\) or \\(\\beta_{1}\\), the estimated values from this single season were inserted. Note the \\(\\hat{avg\\_points}\\), which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.\n1.1356 + .0101 * 0 ## [1] 1.1356 1.1356 + .0101 * 1 ## [1] 1.1457 1.1356 + .0101 * 100 ## [1] 2.1456 1.1356 + .0101 * mean(basketball$avg_points) ## [1] 1.189732 1.1356 + .0101 * 5000 ## [1] 51.6356 1.1356 + .0101 * -50 ## [1] 0.6306 Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.\nFirst, for the intercept (\\(\\beta_{0}\\)), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.\nThe slope, (\\(\\beta_{1}\\)), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.\nThe predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)  What about the error? So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?\nThe short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, \\(\\sigma^2\\). Sometimes this is written in math notation as:\n\\[ \\epsilon \\sim N(0, \\sigma^2) \\]\nFrom this notation, can you see why the error was disregarded earlier when generating predictions?\nIn short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.\nEstimating error in linear regression This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the sum of squares regression (SSR), the portion that is unexplained is referred to as the sum of squares error (SSE). This could be written in math notation as:\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (Y - \\hat{Y})^2 + \\sum (\\hat{Y} - \\bar{Y})^2 \\]\nLet’s try to visualize what this means.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FF7F7F\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#65a765\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FFD580\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)   Another related measure of error Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.\n\\[ R^{2} = 1 - \\frac{SS_{res}}{SS_{total}} \\]\nor\n\\[ R^{2} = \\frac{SS_{reg}}{SS_{total}} \\]\nLet’s compute the sum of square and get a value for \\(R^2\\).\nbasketball %\u0026gt;% summarise(ss_total = sum((avg_points - mean(avg_points))^2), ss_error = sum((avg_points - fitted(wnba_reg))^2), ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %\u0026gt;% mutate(r_square = 1 - ss_error / ss_total, r_square2 = ss_reg / ss_total) ## # A tibble: 1 × 5 ## ss_total ss_error ss_reg r_square r_square2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004. 564. 1440. 0.719 0.719 summary(wnba_reg)$r.square ## [1] 0.7185315 summary(wnba_reg)$sigma ## [1] 1.979045 sigma_hat_square \u0026lt;- 563.9929 / (nrow(basketball) - 2) sigma_hat \u0026lt;- sqrt(sigma_hat_square) sigma_hat_square ## [1] 3.916617 sigma_hat ## [1] 1.979045   ","date":1692748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692748800,"objectID":"1bd811c01f9ed8aa0de49dc4fbae8a29","permalink":"https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/","publishdate":"2023-08-23T00:00:00Z","relpermalink":"/lectures/02-linear-regression/","section":"lectures","summary":"Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes.","tags":null,"title":"Introduction to Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to the course. Primary content will be a review to set the basis for the remainder of the semester.\nObjectives  Read syllabus, ask any questions Review introductory statistics content Explore R code from introductory slides  Activities  Review introductory statistics content  Chapters 2 - 4 of Statistical Reasoning through computation and R   Optional, complete course survey  Assignments None this week.\n","date":1692662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692662400,"objectID":"d1fbf25a3d7006b7a626817f4e8207b2","permalink":"https://psqf6243.brandonlebeau.org/content/01-week1/","publishdate":"2023-08-22T00:00:00Z","relpermalink":"/content/01-week1/","section":"content","summary":"TBD","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.\nObjectives  Recognize a linear regression formula Interpret regression parameters Specify a linear regression in statistical software  Activities  Read Chapter 1 of Applied Regression: An Introduction Engage with course notes. Optional, Read Chapter 7 of Statistical Reasoning through Computation and R Optional, Read 7.1 and 7.2 of Introduction to Modern Statistics  Assignments TBA\n","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"52d024887264e25c0a808c70b28505ad","permalink":"https://psqf6243.brandonlebeau.org/content/02-week2/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/content/02-week2/","section":"content","summary":"TBD","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"  In Class Activity - August 29, 2023 This activity is meant to give you some exploration of topics within class. I’ve created the code for you to run the activity and explore some data.\nThe goals of this activity are as follows:\nExplore what happens if the X and Y attributes are flipped in a linear regression.  What happens to the linear regression coefficient estimates (ie., the \\(\\hat{\\beta}\\))? What happens to the sigma and R-Square statistics?  Given what you find in #1, how do you decide which attribute should be an outcome (Y) vs a predictor (X)?  library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(palmerpenguins) library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 16)) # If you get errors, use this line of code too. # penguins \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\u0026quot;) head(penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## # ℹ 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt; model_fit \u0026lt;- function(outcome, predictor, data = penguins) { formula \u0026lt;- as.formula(paste(outcome, predictor, sep = \u0026quot;~\u0026quot;)) model_out \u0026lt;- lm(formula, data = data) model_coef \u0026lt;- data.frame(matrix(c(coef(model_out)), ncol = 2)) names(model_coef) \u0026lt;- c(\u0026quot;Intercept\u0026quot;, \u0026quot;Slope\u0026quot;) data.frame(model_coef, Rsquare = summary(model_out)$r.square, sigma = summary(model_out)$sigma) } visualize_relationship \u0026lt;- function(outcome, predictor, data = penguins, add_regression_line = TRUE, add_smoother_line = FALSE) { formula \u0026lt;- as.formula(paste(outcome, predictor, sep = \u0026quot;~\u0026quot;)) if(add_regression_line \u0026amp; !add_smoother_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; print() } if(add_smoother_line \u0026amp; !add_regression_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5) |\u0026gt; print() } if(add_smoother_line \u0026amp; add_regression_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5, linetype = 2) |\u0026gt; print() } }  Compute Correlation The following code chunk can help you compute correlations between attributes. To use the function, you can replace “outcome” with an attribute name from the data above and “predictor” with another attribute above.\nWhat happens when you flip the outcome / predictor outcomes when computing the correlation? Does the correlation change? Why or why not? Given the correlation computed, how is it interpreted? Given the correlation computed, what information would this tell us when we try estimate the regression coefficients below?  cor(outcome ~ predictor, data = penguins, use = \u0026#39;complete.obs\u0026#39;) |\u0026gt; round(3)  Visualize bivariate distribution The following code creates a scatter plot showing the bivariate association between the two attributes entered. Example code is shown below as an example. You can replace the “outcome” and “predictor” with the two continuous attributes that you are interested in exploring. These need to be entered in quotations, either single or double are fine. You can also add a regression line or smoother line by specifying those arguments as either TRUE (ie., Yes) or FALSE (ie., No).\nDoes the association between the two attributes appear to be linear? What happens to the association if you flip the predictor / outcome attributes? How would you summarize the association in a few sentences?  visualize_relationship(outcome = \u0026#39;bill_length_mm\u0026#39;, predictor = \u0026#39;flipper_length_mm\u0026#39;, data = penguins, add_regression_line = TRUE, add_smoother_line = FALSE) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: Removed 2 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 2 rows containing missing values (`geom_point()`).  Linear Regression Fitting Similar to the bivariate scatterplot, the following function was created to fit a linear regression and extract some information about the model. The output should include the Intercept, Slope, R-square, and sigma estimates. You can specify the outcome and predictor by replacing those as you did in the bivariate scatterplot above to reflect the attributes you are interested in exploring.\nHow are the 4 estimates interpreted, particularly in the context of the problem? What happens to the 4 estimates if you flip the outcome and predictor attributes?  Which one should truly be the outcome and what should guide this?   model_fit(outcome = \u0026#39;bill_length_mm\u0026#39;, predictor = \u0026#39;flipper_length_mm\u0026#39;, data = penguins) ## Intercept Slope Rsquare sigma ## 1 -7.264868 0.2547682 0.430574 4.125874  ","date":1693267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693267200,"objectID":"730e6344c2a80c627178c5d190900d5b","permalink":"https://psqf6243.brandonlebeau.org/lectures/02a-in-class-activity/","publishdate":"2023-08-29T00:00:00Z","relpermalink":"/lectures/02a-in-class-activity/","section":"lectures","summary":"In Class Activity - August 29, 2023 This activity is meant to give you some exploration of topics within class. I’ve created the code for you to run the activity and explore some data.","tags":null,"title":"Linear Regression - In Class","type":"book"},{"authors":null,"categories":null,"content":"  Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean. We will consider the estimation formulas in part of this to gain a sense of how these can be computed.\nNew Example Data The new data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\n  Bivariate Figure Note, below I do a bit of post-processing to combine data from different POC values within a single CBSA.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality %\u0026gt;% left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) %\u0026gt;% drop_na() ## Warning in left_join(., wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. head(airquality) ## # A tibble: 6 × 13 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 5 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt; dim(airquality) ## [1] 4821 13 gf_point(daily_aqi ~ avg_wind, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; cor(daily_aqi ~ avg_wind, data = airquality) |\u0026gt; round(3) ## [1] -0.292 air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) |\u0026gt; round(3) ## (Intercept) avg_wind ## 48.223 -2.212 summary(air_lm)$r.square |\u0026gt; round(3) ## [1] 0.085 summary(air_lm)$sigma |\u0026gt; round(3) ## [1] 18.055 Interpreting these estimates What do these parameter estimates mean in this context?\nIntercept: This is the model implied ______ when the ________ equals 0.\nSlope: For each 1 unit change in ____ there is a -2.2 unit decrease in .\nR-Square: The __ in ____ is explained by ____.\nSigma: The _____ distance each point is from ____.\n  Centering predictors There are times when centering of predictors can be helpful for interpretation of the model parameters. This can be helpful when 0 is not a practically useful characteristic of the attribute or for more specific tests of certain elements of the X attribute.\nMean Centering Mean centering is where the mean of the attribute is subtracted from each value. This is a linear transformation where each data point is subtracted by a constant, the mean. This means that the distance between points do not change.\nairquality \u0026lt;- airquality %\u0026gt;% mutate(avg_wind_mc = avg_wind - mean(avg_wind), avg_wind_maxc = avg_wind - max(avg_wind), avg_wind_10 = avg_wind - 10) gf_point(daily_aqi ~ avg_wind_mc, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; air_lm_mc \u0026lt;- lm(daily_aqi ~ avg_wind_mc, data = airquality) coef(air_lm_mc) ## (Intercept) avg_wind_mc ## 38.788011 -2.211798 summary(air_lm_mc)$r.square ## [1] 0.08528019 summary(air_lm_mc)$sigma ## [1] 18.05479 air_lm_maxc \u0026lt;- lm(daily_aqi ~ avg_wind_maxc, data = airquality) coef(air_lm_maxc) ## (Intercept) avg_wind_maxc ## 5.968391 -2.211798 summary(air_lm_maxc)$r.square ## [1] 0.08528019 summary(air_lm_maxc)$sigma ## [1] 18.05479 air_lm_10 \u0026lt;- lm(daily_aqi ~ avg_wind_10, data = airquality) coef(air_lm_10) ## (Intercept) avg_wind_10 ## 26.104968 -2.211798 summary(air_lm_10)$r.square ## [1] 0.08528019 summary(air_lm_10)$sigma ## [1] 18.05479   Standardized Regression Another type of regression that can be done is one in which the attributes are standardized prior to estimating the linear regression. What is meant by standardizing? This is converting the attributes into z-scores:\n\\[ Z_{api} = \\frac{(aqi - \\bar{aqi})}{s_{aqi}} \\]\nairquality \u0026lt;- airquality %\u0026gt;% mutate(z_aqi = scale(daily_aqi), z_aqi2 = (daily_aqi - mean(daily_aqi)) / sd(daily_aqi), z_wind = scale(avg_wind)) head(airquality) ## # A tibble: 6 × 19 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 11 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, avg_wind_mc \u0026lt;dbl\u0026gt;, ## # avg_wind_maxc \u0026lt;dbl\u0026gt;, avg_wind_10 \u0026lt;dbl\u0026gt;, z_aqi \u0026lt;dbl[,1]\u0026gt;, z_aqi2 \u0026lt;dbl\u0026gt;, ## # z_wind \u0026lt;dbl[,1]\u0026gt; air_lm_s \u0026lt;- lm(z_aqi ~ z_wind, data = airquality) coef(air_lm_s) ## (Intercept) z_wind ## -2.113006e-15 -2.920277e-01 summary(air_lm_s)$r.square ## [1] 0.08528019 summary(air_lm_s)$sigma ## [1] 0.9565091 We can also use this formula to convert any unstandardized regression coefficients into a standardized metric.\n\\[ b^{\u0026#39;}_{k} = b_{k} * \\frac{s_{x_{k}}}{s_{y}} \\]\n-2.211 * sd(airquality$avg_wind) / sd(airquality$daily_aqi) ## [1] -0.2919224 cor(daily_aqi ~ avg_wind, data = airquality) ## [1] -0.2920277  Parameter Estimation Now that we looked how the parameters are impacted by some changes in the model specification, how are these parameters actually estimated? I will show two ways, one is general, the other is specific to this simple case with a single predictor/covariate attribute. In general, linear regression (or more generally the general linear model) uses least square estimation. This means that the the parameters in the model minimize the squared distance between the observed and predicted values. That is, least squares estimates minimize this criterion:\n\\[ \\sum (Y - \\hat{Y})^2 \\]\nSpecific example Calculus can be used to show that these two equations can be solved simultanuously to get estimates for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimize the criterion above. These formulas are:\n\\[ b_{1} = \\frac{\\sum(X - \\bar{X})(Y - \\bar{Y})}{\\sum(X - \\bar{X})^2} \\] \\[ b_{0} = \\bar{Y} - b_{1}\\bar{X} \\]\nLet’s use R to get these quantities.\nb1 \u0026lt;- with(airquality, sum((avg_wind - mean(avg_wind)) * (daily_aqi - mean(daily_aqi))) / sum((avg_wind - mean(avg_wind))^2) ) b0 \u0026lt;- with(airquality, mean(daily_aqi) - b1 * mean(avg_wind) ) b0 ## [1] 48.22295 b1 ## [1] -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798  General Approach When there are more than one predictor, the number of equations gets a bit unyieldy, therefore, there is a general analytic approach that works for any set of predictor attributes. The general approach uses matrix algebra (anyone take linear algebra?), to achieve their estimates. This general form is:\n\\[ \\mathbf{b} = \\left( \\mathbf{X}^{`}\\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{`} \\mathbf{Y} \\right). \\] Where \\(\\mathbf{b}\\) is a vector of estimated regression coefficients, \\(\\mathbf{X}\\) is a matrix of covariate/predictor attributes (called the design matrix), and \\(\\mathbf{Y}\\) is a vector of the outcome attribute.\nBelow, I show what these would look like for the air quality example that has been used and solve for the regression coefficients.\nX \u0026lt;- model.matrix(air_lm) head(X) ## (Intercept) avg_wind ## 1 1 2.941667 ## 2 1 2.445833 ## 3 1 1.995833 ## 4 1 3.445833 ## 5 1 1.116667 ## 6 1 6.091667 Y \u0026lt;- as.matrix(airquality$daily_aqi) head(Y) ## [,1] ## [1,] 57 ## [2,] 54 ## [3,] 69 ## [4,] 56 ## [5,] 54 ## [6,] 22 X_X \u0026lt;- solve(t(X) %*% X) X_X ## (Intercept) avg_wind ## (Intercept) 0.0008152474 -1.424894e-04 ## avg_wind -0.0001424894 3.340328e-05 X_Y \u0026lt;- t(X) %*% Y X_Y ## [,1] ## (Intercept) 186997 ## avg_wind 731464 X_X %*% X_Y ## [,1] ## (Intercept) 48.222946 ## avg_wind -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798    ","date":1693267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693267200,"objectID":"4f3ecf68d62b085e025f260de84450f2","permalink":"https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/","publishdate":"2023-08-29T00:00:00Z","relpermalink":"/lectures/03-regression-estimates/","section":"lectures","summary":"Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean.","tags":null,"title":"Regression Estimation","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.\nObjectives  Define a residual or error from a regression model. Explore benefits of centering predictor attributes. Engage with linear regression parameter estimation.  Activities  Read Chapter 1 of Applied Regression: An Introduction Engage with course notes. Optional, Read Chapter 7 of Statistical Reasoning through Computation and R Optional, Read 7.2 of Introduction to Modern Statistics  Assignments  Quiz 1 to come soon Activity 1 - Due Around September 18th Activity 2 - Due Around October 2nd  ","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662508800,"objectID":"a45a23bf966f2b7c885958051528505d","permalink":"https://psqf6243.brandonlebeau.org/content/03-week3/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/content/03-week3/","section":"content","summary":"TBD","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Introduction Digging deeper into linear regression. Time will be spent to show/prove via simulation the least square criterion that is minimized by the regression estimates. The foundation for inference using classical statistical methods will be explored.\nObjectives  Understand the least squares minimization for regression. Explore inference for regression parameters. Define what a standard error is. Define null hypothesis significance testing (NHST)  Activities  Read Chapter 2 of Applied Regression: An Introduction Read Chapter 3, section 1 of Introduction to Statistical Learning Engage with course notes. Optional, Read 24.4 of Introduction to Modern Statistics  Assignments  Quiz 2 \u0026ndash; To come \u0026hellip; Activity 1 - Due Around September 18th Activity 2 - Due Around October 2nd  ","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662508800,"objectID":"e315f03cac4ec7010fcc961beef42232","permalink":"https://psqf6243.brandonlebeau.org/content/04-week4/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/content/04-week4/","section":"content","summary":"TBD","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"Introduction Digging deeper into linear regression inference using classical procedures. Further definitions of confidence intervals, sampling distribution, and interpretations of p-values. Regression conditions for valid inferences. Topics will explore evaluation of residuals for checking of conditions.\nObjectives  Construct a confidence interval Define the sampling distribution Perform significance testing Interpret p-value as a continuous probability Define what a residual is Define common statistical conditions for linear regression Interpret residual plots  Activities  Read Chapter 2 of Applied Regression: An Introduction Read Chapter 3, section 1 of Introduction to Statistical Learning Engage with course notes. Optional, Read 24.4 of Introduction to Modern Statistics Section 11.1 of Regression and Other Stories Regression and Other Stories. Optional, Read ASA statement on p-values  Assignments  Activity 1 \u0026ndash; Due around September 18th Activity 2 \u0026ndash; Due around October 2nd Quiz 2 \u0026ndash; Due September 25th  ","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695254400,"objectID":"7b26937bb8dacd1f2a59f61a2d255d6b","permalink":"https://psqf6243.brandonlebeau.org/content/05-week5/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/content/05-week5/","section":"content","summary":"TBD","tags":null,"title":"Week 5","type":"book"},{"authors":null,"categories":null,"content":"  Example to show least squares minimization This little example is meant as a way to show the least square really minimizes the criterion, $ ( Y - )^2 $.\nIn this example, we will generate some data so that we know what the truth is. Then, upon data generation, we will compute a bunch of different values for the linear slope and y-intercept. For each combination of the y-intercept and slope, I will compute the sum of squares error depicted above.\nSimulate some data The following example simulates data based on the following linear regression formula:\n\\[ Y = 5 + 0.5 X + \\epsilon \\]\nMore explicitly, the simulation allows us to specify what the intercept and slope is in the population. These are specified below in the reg_weights simulation argument.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(simglm) theme_set(theme_bw(base_size = 18)) set.seed(2023) sim_arguments \u0026lt;- list( formula = y ~ x, fixed = list(x = list(var_type = \u0026#39;continuous\u0026#39;, mean = 100, sd = 20)), error = list(variance = 100), sample_size = 1000, reg_weights = c(5, .5) ) sim_data \u0026lt;- simulate_fixed(data = NULL, sim_arguments) |\u0026gt; simulate_error(sim_arguments) |\u0026gt; generate_response(sim_arguments) head(sim_data) ## X.Intercept. x level1_id error fixed_outcome random_effects ## 1 1 98.32431 1 -2.6862147 54.16216 0 ## 2 1 80.34113 2 -1.1886232 45.17056 0 ## 3 1 62.49865 3 0.6545456 36.24933 0 ## 4 1 96.27711 4 12.9091527 53.13855 0 ## 5 1 87.33029 5 -4.5144816 48.66514 0 ## 6 1 121.81595 6 19.5705749 65.90797 0 ## y ## 1 51.47594 ## 2 43.98194 ## 3 36.90387 ## 4 66.04771 ## 5 44.15066 ## 6 85.47855  Visualize the Simulated Data The following code visualizes the simulated data from above. What would you estimate the correlation to be?\nlibrary(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) gf_point(y ~ x, data = sim_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;)  Estimate Regression Coefficients Even though we know what truth is, there is error involved in the simulation process, therefore, the population values specified above will not equal the exact regression coefficients estimated. Below, we estimate what those regression coefficients are.\nsim_lm \u0026lt;- lm (y ~ x, data = sim_data) coef(sim_lm) ## (Intercept) x ## 5.1260093 0.5002076  Create different combinations of intercept and slope coefficients The following code generates a sequence of intercept and corresponding slope conditions. We will use these different values to estimate the sum of squares error shown at the top of the notes for each of these intercept and slope values to show that the regression estimates are optimal to minimize the sum of square error.\ny_intercept \u0026lt;- seq(0, 15, by = .25) slope \u0026lt;- seq(0, 1.5, by = .01) conditions \u0026lt;- rbind(expand.grid(y_intercept = y_intercept, slope = slope), coef(sim_lm)) tail(conditions) ## y_intercept slope ## 9207 14.000000 1.5000000 ## 9208 14.250000 1.5000000 ## 9209 14.500000 1.5000000 ## 9210 14.750000 1.5000000 ## 9211 15.000000 1.5000000 ## 9212 5.126009 0.5002076 dim(conditions) ## [1] 9212 2  Showing Two Combinations Here we visualize two possible slope conditions. Which one seems better for the data?\ngf_point(y ~ x, data = sim_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;) |\u0026gt; gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 1), linetype = 2, size = 2) |\u0026gt; gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 855), linetype = 2, color = \u0026#39;lightgreen\u0026#39;, size = 2) |\u0026gt; gf_refine(coord_cartesian(xlim = c(0, 160), ylim = c(0, 120))) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated.  Compute Sum of Squares Error The following code creates a new function that computes the sum of square error. The function takes two arguments, the combination of intercept and slope values and the simulated data. The output is the sigma or average error from the regression line. The first code chunk below performs the computation for a single condition. The second code chunk does it for all of the conditions.\nsum_square_error \u0026lt;- function(conditions, sim_data) { fitted \u0026lt;- conditions[[\u0026#39;y_intercept\u0026#39;]] + conditions[[\u0026#39;slope\u0026#39;]] * sim_data[[\u0026#39;x\u0026#39;]] deviation \u0026lt;- sim_data[[\u0026#39;y\u0026#39;]] - fitted sqrt((sum(deviation^2) / (nrow(sim_data) - 2))) } sum_square_error(conditions[1892, ], sim_data) ## [1] 26.25721 summary(sim_lm)$sigma ## [1] 9.738423 library(future) plan(multicore) ## Warning in supportsMulticoreAndRStudio(...): [ONE-TIME WARNING] Forked ## processing (\u0026#39;multicore\u0026#39;) is not supported when running R from RStudio because ## it is considered unstable. For more details, how to control forked processing ## or not, and how to silence this warning in future R sessions, see ## ?parallelly::supportsMulticore conditions$sse \u0026lt;- unlist(lapply(1:nrow(conditions), function(xx) sum_square_error(conditions[xx, ], sim_data))) head(conditions) ## y_intercept slope sse ## 1 0.00 0 56.72588 ## 2 0.25 0 56.48344 ## 3 0.50 0 56.24108 ## 4 0.75 0 55.99878 ## 5 1.00 0 55.75655 ## 6 1.25 0 55.51440   ","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"5207d81b053ace685d82a5546345fe22","permalink":"https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/lectures/04-least-squares-simulation/","section":"lectures","summary":"Example to show least squares minimization This little example is meant as a way to show the least square really minimizes the criterion, $ ( Y - )^2 $.","tags":null,"title":"Least Squares Minimization","type":"book"},{"authors":null,"categories":null,"content":"Introduction  Exploration of utilizing a bootstrap for a robust alternative to inference for regression parameters.  Objectives  Define the bootstrap procedure Define and explain the steps to conduct a bootstrap Interpret bootstrap results  Activities  Read Chapter 2 of Applied Regression: An Introduction Read Chapter 3, section 1 of Introduction to Statistical Learning Section 11.1 of Regression and Other Stories Regression and Other Stories. Engage with course notes.  Assignments  Activity 2 \u0026ndash; Due around October 2nd Quiz 3 - Due October 2nd Assignment 1 - Due around October 9th Assignment 2 - Due around October 23rd  ","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"a1ba50a25029964cc75a26ac15871b71","permalink":"https://psqf6243.brandonlebeau.org/content/06-week6/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/content/06-week6/","section":"content","summary":"TBD","tags":null,"title":"Week 6","type":"book"},{"authors":null,"categories":null,"content":"Introduction  Wrapup of multiple linear regression. Regression with Categorical Predictors  Objectives  Define multiple linear regression Interpret multiple linear regression coefficients Explain variance decomposition Interpret coefficients for categorical predictors  Activities  Read Chapter 3.2 of An Introduction to Statistical Learning Read Chapter 8 of Introduction to Modern Statistics Engage with course notes.  Assignments  Activity 2 \u0026ndash; Due around October 2nd Activity 3 \u0026ndash; Due around October 16th Quiz 4 - Due around October 16th. Assignment 1 - Due around October 9th Assignment 2 - Due around October 23rd  ","date":1696377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696377600,"objectID":"edf57c5339859dc84c8243e38d10813b","permalink":"https://psqf6243.brandonlebeau.org/content/07-week7/","publishdate":"2023-10-04T00:00:00Z","relpermalink":"/content/07-week7/","section":"content","summary":"TBD","tags":null,"title":"Week 7","type":"book"},{"authors":null,"categories":null,"content":"  Inference for regression parameters It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population. Therefore, a subset of the population (the sample) is used to estimate the population parameters that are of most interest. As such, our estimates come with error and this uncertainty we can quantify when making inference about the parameter estimates.\nIn this course, I plan to show you two ways to perform this inference. One of those frameworks will be the classical approach which uses classical statistical theory to estimate the amount of uncertainty in the parameter estimates. The second approach will use the bootstrap as a way to computationally estimate the uncertainty. The benefit of the bootstrap is that it comes with fewer assumptions than the classical approach. We will build up to these arguments.\nClassical Inferential Framework The classical inferential framework, sometimes referred to as the null hypothesis significance test (NHST) has been around for more than 100 years. This framework builds off of the idea of a null hypothesis.\nA null hypothesis is typically thought as a hypothesis that assumes there is no relationship or a null effect. Framing this in the regression concept that we have been working with, we could define the following null hypotheses.\n\\[ H_{0}: \\beta_{0} = 0.\\ The\\ population\\ yintercept\\ equals\\ 0. \\]\nor\n\\[ H_{0}: \\beta_{1} = 0.\\ The\\ population\\ slope\\ equals\\ 0. \\]\nIn the following two null hypotheses, represented with \\(H_{0}\\), the population parameters are being assumed to be 0 in the population. This is one definition of a null effect, but is not the only null effect we can define (more on this later). The value defined as a null effect is important as it centers the sampling distribution used for evaluating where the sample estimate falls in that distribution.\nAnother hypothesis is typically defined with the null hypothesis, called the alternative hypothesis. This hypothesis states that there is an effect. Within the linear regression framework, we could write the alternative hypotheses as:\n\\[ H_{A}: \\beta_{0} \\neq 0.\\ The\\ population\\ yintercept\\ is\\ not\\ equal\\ to\\ 0. \\]\nor\n\\[ H_{A}: \\beta_{1} \\neq 0.\\ The\\ population\\ slope\\ is\\ not\\ equal\\ to\\ 0. \\]\nIn the following two alternative hypotheses, represented with \\(H_{A}\\), the population parameters are assumed to be not equal to 0. These can also be one-sided, more on this with an example later.\nEstimating uncertainty in parameter estimates The standard error is used to estimate uncertainty or error in the parameter estimates due to having a sample from the population. More specifically, this means that the entire population is not used to estimate the parameter, therefore the estimate we have is very likely not equal exactly to the parameter. Instead, there is some sort of sampling error involved that we want to quantify. If the sample was collected well, ideally randomly, then the estimate should be unbiased. Unbiased here doesn’t mean that the estimate equals the population parameter, rather, that through repeated sampling, the average of our sample estimates would equal the population parameter.\nAs mentioned, standard errors are used to quantify this uncertainty. In the linear regression case we have explored so far, there are mathematical formula for the standard errors. These are shown below.\n\\[ SE\\left( \\hat{\\beta}_{0} \\right) = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum \\left( X - \\bar{X} \\right)^2} \\right)} \\]\nand\n\\[ SE\\left( \\hat{\\beta}_{1} \\right) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum \\left( X - \\bar{X} \\right)^2}} \\]\nIn the equation above, \\(\\hat{\\sigma}^2\\), is equal to \\(\\sqrt{\\frac{SS_{error}}{n - 2}}\\), and \\(n\\) is the sample size (ie, number of rows of data in the model).\nMatrix Representation It is also possible, and more easily extendable, to write the standard error computations or the variance of the estimated parameters in matrix representation. This framework extends beyond the single predictor case (ie. one \\(X\\)), therefore is more readily used in practice.\n\\[ \\hat{var}\\left({\\hat{\\beta}}\\right) = \\hat{\\sigma}^2 \\left( \\mathbf{X}^{`}\\mathbf{X} \\right)^{-1} \\]\nIn the equation above, \\(\\hat{\\sigma}^2\\), is equal to \\(\\sqrt{\\frac{SS_{error}}{n - 2}}\\), and \\(\\mathbf{X}\\) is the design matrix from the regression analysis. Finally, to get the standard errors back, you take the square root of the diagonal elements.\n$$ SE( ) =   Data The data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality %\u0026gt;% left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) %\u0026gt;% drop_na() ## Warning in left_join(., wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 summary(air_lm)$r.square ## [1] 0.08528019 summary(air_lm)$sigma ## [1] 18.05479 sum_x \u0026lt;- airquality %\u0026gt;% summarise(mean_wind = mean(avg_wind), sum_dev_x_sq = sum( (avg_wind - mean_wind) ^ 2)) sum_x ## # A tibble: 1 × 2 ## mean_wind sum_dev_x_sq ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4.27 29937. se_b0 \u0026lt;- sqrt(summary(air_lm)$sigma^2 * ((1 / nrow(airquality)) + ( sum_x[[\u0026#39;mean_wind\u0026#39;]]^2 / sum_x[[\u0026#39;sum_dev_x_sq\u0026#39;]]) )) se_b1 \u0026lt;- sqrt(summary(air_lm)$sigma^2 / sum_x[[\u0026#39;sum_dev_x_sq\u0026#39;]]) se_b0 ## [1] 0.51551 se_b1 ## [1] 0.1043487 X \u0026lt;- model.matrix(air_lm) var_b \u0026lt;- summary(air_lm)$sigma^2 * solve(t(X) %*% X) var_b  ## (Intercept) avg_wind ## (Intercept) 0.26575054 -0.04644803 ## avg_wind -0.04644803 0.01088865 sqrt(diag(var_b)) ## (Intercept) avg_wind ## 0.5155100 0.1043487 summary(air_lm)$coefficients[,2] ## (Intercept) avg_wind ## 0.5155100 0.1043487   Moving toward inference Now that there the parameters are estimated and the amount of uncertainty is quantified, inference is possible. There are two related pieces that can be computed now, a confidence interval and/or the test-statistic and p-value. Let’s go through both.\nFirst, a confidence interval can be computed. Confidence intervals take the following general form:\n\\[ \\hat{\\beta} \\pm C * SE \\]\nWhere, \\(\\hat{\\beta}\\) is the parameter estimate, \\(C\\) is the confidence level, and \\(SE\\) is the standard error. The parameter estimates and standard errors are what we have already established, the \\(C\\) is the confidence level. This indicates the percentage of times, over the long run/repeated sampling, that the interval will capture the population parameter. Historically, this value is often specified as 95%, but any value is theoretically possible.\nThe \\(C\\) value represents a quantile from a mathematical distribution that separates the middle percetage desired (ie, 95%) from the rest of the distribution. The mathematical distribution is most often the t-distribution, but the difference between a t-distribution and normal distribution are modest once the sample size is greater than 30 or so.\nThe figure below tries to highlight the \\(C\\) value.\nt_30 \u0026lt;- data.frame(value = seq(-5, 5, .01), density = dt(seq(-5, 5, .01), df = 30)) gf_line(density ~ value, data = t_30) %\u0026gt;% gf_vline(xintercept = ~ qt(.025, df = 30), linetype = 2) %\u0026gt;% gf_vline(xintercept = ~ qt(.975, df = 30), linetype = 2) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 summary(air_lm)$coefficients[,2] ## (Intercept) avg_wind ## 0.5155100 0.1043487 abs(qt(.025, df = nrow(airquality) -2)) ## [1] 1.960456 48.2 + c(-1, 1) * 1.96 * .5155 ## [1] 47.18962 49.21038 -2.211 + c(-1, 1) * 1.96 * .1043 ## [1] -2.415428 -2.006572  Inference with test statistics It is also possible to do inference with a test statistic and computation of a p-value. Inference in this framework can be summarized into the following steps:\nGenerate hypotheses, ie null and alternative hypotheses. Establish an \\(\\alpha\\) value Estimate parameters Compute test statistic Generate p-value  An \\(\\alpha\\) value is the level of significance and represents the probability of obtaining the results due to chance. This is a value that the researcher can select. For a 5% \\(\\alpha\\) value, this is what was used above to compute the confidence intervals.\nThe test statistic is computed as follows:\n\\[ test\\ stat = \\frac{\\hat{\\beta} - hypothesized\\ value}{SE(\\hat{\\beta})} \\]\nwhere \\(\\hat{\\beta}\\) is the estimated parameter, \\(SE(\\hat{\\beta})\\) is the standard error of the parameter estimate, and the \\(hypothesized\\ value\\) is the hypothesized value from the null hypothesis. This is often 0, but does not need to be zero.\nLet’s assume the following null/alternative hypotheses:\n\\[ H_{0}: \\beta_{1} = 0 \\\\ H_{A}: \\beta_{1} \\neq 0 \\]\nLet’s use R to compute this test statistic.\nt = (-2.211 - 0) / .1043 t |\u0026gt; round(3) ## [1] -21.198 pt(-21.198, df = nrow(airquality) -2, lower.tail = TRUE) |\u0026gt; round(3) ## [1] 0 t_dist \u0026lt;- data.frame(value = seq(-25, 25, .05), density = dt(seq(-25, 25, .05), df = (nrow(airquality) - 2))) gf_line(density ~ value, data = t_dist) %\u0026gt;% gf_vline(xintercept = ~ qt(.025, df = (nrow(airquality) - 2)), linetype = 2) %\u0026gt;% gf_vline(xintercept = ~ qt(.975, df = (nrow(airquality) - 2)), linetype = 2) %\u0026gt;% gf_vline(xintercept = ~ -21.198, color = \u0026#39;red\u0026#39;) %\u0026gt;% gf_vline(xintercept = ~ 21.198, color = \u0026#39;red\u0026#39;) summary(air_lm) ## ## Call: ## lm(formula = daily_aqi ~ avg_wind, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.71 -14.38 -0.73 12.43 86.84 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 48.2229 0.5155 93.54 \u0026lt;2e-16 *** ## avg_wind -2.2118 0.1043 -21.20 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.05 on 4819 degrees of freedom ## Multiple R-squared: 0.08528,\tAdjusted R-squared: 0.08509 ## F-statistic: 449.3 on 1 and 4819 DF, p-value: \u0026lt; 2.2e-16 confint(air_lm) ## 2.5 % 97.5 % ## (Intercept) 47.212311 49.233581 ## avg_wind -2.416369 -2.007227 confint(air_lm, level = .5) ## 25 % 75 % ## (Intercept) 47.875214 48.57068 ## avg_wind -2.282185 -2.14141    ","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"80c54d5cbf0059b702a774d2ae849d3c","permalink":"https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/lectures/05-regression-inference/","section":"lectures","summary":"Inference for regression parameters It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population.","tags":null,"title":"Inference for regression parameters","type":"book"},{"authors":null,"categories":null,"content":"Introduction Generalize the regression model to using categorical predictors with more than 2 groups. In this section, exploration of dummy/indicator coding for coding of categorical predictors will be the focus. Interpretation of model parameters will be the focus\nObjectives  Identify a categorical attribute Define dummy/indicator coding for categorical attributes with more than 2 categories Interpret regression results from dummy coded categorical attribute  Activities  Read Chapter 3.6.6 of An Introduction to Statistical Learning Read Chapter 7.2.6 of Introduction to Modern Statistics Engage with course notes.  Assignments  Activity 2 \u0026ndash; Due around October 3rd Activity 3 \u0026ndash; Due around October 17th Quiz 5 \u0026ndash; coming soon. Assignment 1 - Due around October 10th Assignment 2 - Due around October 24th  ","date":1665014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665014400,"objectID":"61b59fb25c9c9655e1869c626f92dc17","permalink":"https://psqf6243.brandonlebeau.org/content/08-week8/","publishdate":"2022-10-06T00:00:00Z","relpermalink":"/content/08-week8/","section":"content","summary":"TBD","tags":null,"title":"Week 8","type":"book"},{"authors":null,"categories":null,"content":"  In class activity - September 13, 2023 This activity is intended to give you some practice interpreting regression coefficients and also conducting inference.\nThe data used for this activity comes from medical insurance claims.\n  Attribute Type Description    age integer Age of individual  sex character Sex of individual at birth  bmi integer Body mass index  children integer Number of children covered on insurance  smoker character Whether individuals smokes or not  region character Region of the United States  charges integer Medical charges    The following code chunk reads in the data and shows the first 6 rows.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum charges \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\u0026quot;) ## Rows: 1338 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (3): sex, smoker, region ## dbl (4): age, bmi, children, charges ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(charges) ## # A tibble: 6 × 7 ## age sex bmi children smoker region charges ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 19 female 27.9 0 yes southwest 16885. ## 2 18 male 33.8 1 no southeast 1726. ## 3 28 male 33 3 no southeast 4449. ## 4 33 male 22.7 0 no northwest 21984. ## 5 32 male 28.9 0 no northwest 3867. ## 6 31 female 25.7 0 no southeast 3757. Descriptive analysis Pick 2 integer attributes and explore those two attributes descriptively. Think about key characteristics about these two attributes.\n Fitting the linear model Fit the linear model to estimate the regression parameters. Then do the following:\nExtract the linear model estimates.   Example text  Interpret the slope and intercept. Interpret the standard errors Interpret overall model fit.   Conduct Inference Suppose we wished to use this data to make broader claims to a population of interest. Let us perform classical inference using the NHST framework. Do the following:\nEstablish the statistical hypotheses. Extract and interpret the test-statistics and p-values What do the test-statistics and p-values tell us about the statistical hypotheses? What do the test-statistics and p-values tell us about the magnitude (i.e., size) of effect? What do the test-statistics and p-values tell us about the practical implications of the effect?   Compute confidence interval Compute a confidence interval in the following steps.\nEstablish the confidence level Compute the confidence interval. Interpret the confidence interval. What does the confidence interval tell us about magnitude or practical implications of the effect?  theme_set(theme_bw(base_size = 16)) gf_density(~ charges, data = charges) df_stats(~ charges, data = charges, mean, median, sd, IQR) ## response mean median sd IQR ## 1 charges 13270.42 9382.033 12110.01 11899.63 gf_point(charges ~ age, data = charges, size = 5, alpha = .25) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 3) gf_point(charges ~ age, data = charges, color = ~ sex, size = 5, alpha = .25) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 3) gf_point(charges ~ age, data = charges, color = ~ smoker, size = 5, alpha = .25) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 3) gf_point(charges ~ age, data = charges, color = ~ region, size = 5, alpha = .25) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 3) gf_point(charges ~ age, data = charges, color = ~ smoker, size = 5, alpha = .25) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 3) |\u0026gt; gf_facet_wrap(~ sex) df_stats(charges ~ age, data = charges, mean, median, sd, min, max) ## response age mean median sd min max ## 1 charges 18 7086.218 2198.190 10198.460 1121.874 38792.69 ## 2 charges 19 9747.909 2135.892 12469.537 1241.565 39722.75 ## 3 charges 20 10159.698 2459.720 12049.625 1391.529 38344.57 ## 4 charges 21 4730.464 2254.424 6168.059 1515.345 26018.95 ## 5 charges 22 10012.933 2641.156 14653.364 1665.000 44501.40 ## 6 charges 23 12419.820 3594.538 13421.332 1815.876 40904.20 ## 7 charges 24 10648.016 3045.138 12203.651 1969.614 38126.25 ## 8 charges 25 9838.365 3750.149 11551.289 2137.654 42112.24 ## 9 charges 26 6133.825 3388.882 7765.729 2302.300 36085.22 ## 10 charges 27 12184.702 4544.324 11941.822 2483.736 39611.76 ## 11 charges 28 9069.188 4344.951 11428.968 2689.495 51194.56 ## 12 charges 29 10430.159 4906.410 10680.198 2866.091 44585.46 ## 13 charges 30 12719.110 4837.582 12515.212 3554.203 40932.43 ## 14 charges 31 10196.981 4738.268 13646.443 3260.199 58571.07 ## 15 charges 32 9220.300 4672.016 9346.679 3866.855 37607.53 ## 16 charges 33 12351.533 6210.083 12877.616 3704.354 55135.40 ## 17 charges 34 11613.528 5490.092 11735.073 3935.180 43943.88 ## 18 charges 35 11307.182 5836.520 10699.982 4746.344 39983.43 ## 19 charges 36 12204.476 5478.037 12884.342 4399.731 43753.34 ## 20 charges 37 18019.912 6985.507 15125.287 4646.759 46113.51 ## 21 charges 38 8102.734 6455.863 7328.907 5383.536 41949.24 ## 22 charges 39 11778.243 7512.267 8633.210 5649.715 40103.89 ## 23 charges 40 11772.251 7077.189 9908.913 5415.661 40003.33 ## 24 charges 41 9653.746 6875.961 8859.592 5699.837 40273.65 ## 25 charges 42 13061.039 7443.643 10707.617 5966.887 43896.38 ## 26 charges 43 19267.279 18767.738 13529.394 6250.435 45863.21 ## 27 charges 44 15859.397 8023.135 14394.949 6948.701 48885.14 ## 28 charges 45 14830.200 8603.823 13722.542 7222.786 62592.87 ## 29 charges 46 14342.591 8825.086 12159.483 7147.105 46151.12 ## 30 charges 47 17654.000 9715.841 12208.024 8062.764 44202.65 ## 31 charges 48 14632.500 9447.382 10163.267 7789.635 45702.02 ## 32 charges 49 12696.006 9681.120 7531.013 8116.680 39727.61 ## 33 charges 50 15663.003 10107.221 10939.124 8442.667 42856.84 ## 34 charges 51 15682.256 9875.680 12707.113 8782.469 47462.89 ## 35 charges 52 18256.270 11396.900 12408.353 9140.951 60021.40 ## 36 charges 53 16020.931 11157.174 10096.072 9487.644 46661.44 ## 37 charges 54 18758.546 11816.449 14216.087 9850.432 63770.43 ## 38 charges 55 16164.545 11880.231 10107.690 10214.636 44423.80 ## 39 charges 56 15025.516 11658.247 9204.315 10577.087 43921.18 ## 40 charges 57 16447.185 11893.878 9987.941 10959.330 48675.52 ## 41 charges 58 13878.928 11931.125 7437.417 11345.519 47496.49 ## 42 charges 59 18895.870 12928.791 11170.560 11743.299 48970.25 ## 43 charges 60 21979.419 13204.286 14923.698 12142.579 52590.83 ## 44 charges 61 22024.458 13635.638 12525.159 12557.605 48517.56 ## 45 charges 62 19163.857 13844.797 10320.270 12957.118 46718.16 ## 46 charges 63 19884.998 14349.854 11850.935 13390.559 48824.45 ## 47 charges 64 23275.531 15528.758 13029.842 13822.803 49577.66 cor(charges ~ age, data = charges) |\u0026gt; round(3) ## [1] 0.299 Type text bold italic\ncharge_mod \u0026lt;- lm(charges ~ age, data = charges) charge_mod_c \u0026lt;- lm(charges ~ I(age - min(age)), data = charges) summary(charge_mod) ## ## Call: ## lm(formula = charges ~ age, data = charges) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8059 -6671 -5939 5440 47829 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3165.9 937.1 3.378 0.000751 *** ## age 257.7 22.5 11.453 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 11560 on 1336 degrees of freedom ## Multiple R-squared: 0.08941,\tAdjusted R-squared: 0.08872 ## F-statistic: 131.2 on 1 and 1336 DF, p-value: \u0026lt; 2.2e-16 broom::tidy(charge_mod) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 3166. 937. 3.38 7.51e- 4 ## 2 age 258. 22.5 11.5 4.89e-29 broom::tidy(charge_mod_c) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 7805. 572. 13.6 9.77e-40 ## 2 I(age - min(age)) 258. 22.5 11.5 4.89e-29 broom::glance(charge_mod) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0894 0.0887 11560. 131. 4.89e-29 1 -14415. 28836. 28852. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; confint(charge_mod, level = 0.99) ## 0.5 % 99.5 % ## (Intercept) 748.4946 5583.2755 ## age 199.6774 315.7679   ","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695254400,"objectID":"a8451f1f883487fda32b3c245c79a9b6","permalink":"https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/lectures/05a-regression-in-class/","section":"lectures","summary":"In class activity - September 13, 2023 This activity is intended to give you some practice interpreting regression coefficients and also conducting inference.\nThe data used for this activity comes from medical insurance claims.","tags":null,"title":"Regression in class","type":"book"},{"authors":null,"categories":null,"content":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed assignment per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nIncluding Statistical Evidence This assignment is worth a total of 10 points spread over 9 questions. Please make sure to include any statistical evidence to support your statements, this could include graphics or statistics. This assignment also gives you a choice about what question you are interested in exploring within these data, therefore, including the statistical evidence is extremely important. Failure to include statistical evidence to support claims will result in a 1pt deduction.\nDue Date Due around October 9th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this assignment is tuition and salary data for higher education institutions. The data originally come from Tidy Tuesday. I have done some processing to join two tables together with fuzzy string matching for you.\nData to use: The data can be obtained in csv format from GitHub. The data are also posted to the data folder within the IDAS. A short description for each attribute is as follows.\n   variable class description     name character Name of school   state_name character state name   early_career_pay double Estimated early career pay in USD   mid_career_pay double Estimated mid career pay in USD   make_world_better_percent double Percent of alumni who think they are making the world a better place   stem_percent double Percent of student body in STEM   type character Type: Public, private, for-profit   degree_length character 4 year or 2 year degree   room_and_board double Room and board in USD   in_state_tuition double Tuition for in-state residents in USD   in_state_total double Total cost for in-state residents in USD (sum of room \u0026amp; board + in state tuition)   out_of_state_tuition double Tuition for out-of-state residents in USD   out_of_state_total double Total cost for in-state residents in USD (sum of room \u0026amp; board + out of state tuition)    Questions Note: Each question is worth 1 pt unless otherwise specified.\n  Identify a research question of interest that can use linear regression to answer the question.\n  Explore the research question you identified in #1 descriptively. In a few sentences, summarize any potential relationship, including statistical evidence (i.e., figures or statistics) to support your statements.\n  Fit the linear regression to answer your question from #1. Interpret the regression coefficients in the context of the problem at hand. That is, what do the coefficients mean for the attributes included in the model.\n  Is the model intercept as interpretable as it could be? What could be done to enhance the interpretation of the intercept? Summarize in a few sentences any theory or data elements that may help to decide how to improve the interpretation of the intercept.\n  Interpret the model estimates that show how well the model is performing. That is, what model statistics help to understand how well the model representing the outcome attribute? What do these statistics mean in the context of the problem?\n  Explore and evaluate the model assumptions/data conditions regarding the residual/error term. Summarize how well the model is meeting the assumptions, citing specific statistics or visualizations to justify your conclusions. 2 pts\n  Write out the null and alternative hypotheses based on your research question from #1.\n  Summarize the statistical evidence surrounding the null or alternative hypotheses from question 7. In a few sentences, describe if the evidence provides support for or against the null hypothesis. Provide specific statistical evidence to support your justification.\n  Based on the justification in #8, what practical implications does this result have? That is, are the statistical results that you have been describing/summarizing throughout this assignment practically relevant? Be as specific as you can in your discussion about why you believe the results are practically useful or not.\n  ","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"c6a2a640211fdb5baf80c385f5efa650","permalink":"https://psqf6243.brandonlebeau.org/assignments/assignment/assignment1/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/assignments/assignment/assignment1/","section":"assignments","summary":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Assignment 1","type":"book"},{"authors":null,"categories":null,"content":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed assignment per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nIncluding Statistical Evidence This assignment is worth a total of 10 points spread over 7 questions. Please make sure to include any statistical evidence to support your statements, this could include graphics or statistics. This assignment also gives you a choice about what question you are interested in exploring within these data, therefore, including the statistical evidence is extremely important. Failure to include statistical evidence to support claims will result in a 1pt deduction.\nDue Date Due around October 23rd, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this assignment is tuition and salary data for higher education institutions. The data originally come from Tidy Tuesday. I have done some processing to join two tables together with fuzzy string matching for you.\nData to use: The data can be obtained in csv format from GitHub. The data are also posted to the data folder within the IDAS. A short description for each attribute is as follows.\n   variable class description     name character Name of school   state_name character state name   early_career_pay double Estimated early career pay in USD   mid_career_pay double Estimated mid career pay in USD   make_world_better_percent double Percent of alumni who think they are making the world a better place   stem_percent double Percent of student body in STEM   type character Type: Public, private, for-profit   degree_length character 4 year or 2 year degree   room_and_board double Room and board in USD   in_state_tuition double Tuition for in-state residents in USD   in_state_total double Total cost for in-state residents in USD (sum of room \u0026amp; board + in state tuition)   out_of_state_tuition double Tuition for out-of-state residents in USD   out_of_state_total double Total cost for in-state residents in USD (sum of room \u0026amp; board + out of state tuition)    Note This assignment builds off of assignment 1, therefore, please complete assignment 1 first. Review assignment 1 briefly prior to starting this assignment.\nQuestions Note: Each question is worth 1 pt unless otherwise specified.\n  Based on your first assignment, are there attributes that could have been missing from the linear regression fitted in assignment 1? That is, is the assumption of all attributes being included in the model appropriate? Why or why not? If not, what other attributes may be of interest?\n  Add one or more of the attributes identified in question 1 to create a multiple regression model. That is, add one or more of the attributes from question 1 while keeping the attribute from assignment 1 into the regression model. Summarize the interpretation of the regression coefficient estimates for this multiple regression model.\n  Estimate model fit indices to compare the model from assignment 1 to the multiple regression fitted in question 2. Does the model improve model fit based on that from assignment 1? Why or why not?\n  Explore and evaluate the model assumptions regarding the residual/error term. Summarize how well the model is meeting the assumptions, citing specific statistics or visualizations to justify your conclusions. In addition, do the model assumptions seem better met compared to those from the regression in assignment 1? 2 pts\n  Summarize the statistical evidence surrounding the null or alternative hypotheses that are being explored for the coefficients entered into the model. Note, I did not explicitly ask you for null/alternative hypotheses, but you may want to write those down for your reference. In a few sentences, describe if the evidence provides support for or against the null hypothesis. Provide specific statistical evidence to support your justification. 2 pts\n  Create confidence intervals for the coefficients in the multiple regression model. Justify your confidence level and interpret the confidence intervals in the context of the data. What do these confidence intervals suggest about the magnitude of effect? 2 pts\n  Based on the justification in #5 and #6, what practical implications does this result have? That is, are the statistical results that you have been describing/summarizing throughout this assignment practically relevant? Be as specific as you can in your discussion about why you believe the results are practically useful or not.\n  ","date":1695081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695081600,"objectID":"345381affcf9d5722c59bb3867d9c155","permalink":"https://psqf6243.brandonlebeau.org/assignments/assignment/assignment2/","publishdate":"2023-09-19T00:00:00Z","relpermalink":"/assignments/assignment/assignment2/","section":"assignments","summary":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Assignment 2","type":"book"},{"authors":null,"categories":null,"content":"  Conditions for Linear Regression The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:\n\\[ Y - \\hat{Y} \\]\nThese are the deviations in the observed scores from the predicted scores from the linear regression. Recall, through least square estimation that these residauls will sum to 0, therefore, their mean would also be equal to 0. However, there are certain conditions about these residuals that are made for the linear regression model to have the inferences be appropriate. We’ll talk more about what the implications for violating these conditions will have on the linear regression model, but first, the conditions.\nApproximately Normally distributed residuals Homogeneity of variance Uncorrelated residuals Error term is uncorrelated with the predictor attribute Linearity and additivity of the model.  Each of these will be discussed in turn.\nData conditions that are not directly testable:\nValidity Representativeness  Approximately Normally distributed residuals The first assumption is that the residuals are at least approximately Normally distributed. This assumption is really only much of a concern when the sample size is small. If the sample size is larger, the Central Limit Thereom (CLT) states that the distribution of the statstics will be approximately normally distributed. The threshold for the CLT to be properly invoked is about 30. Larger then this, the residuals do not need to be approximately normally distributed. Even still, exploring the distribution of the residuals can still be helpful and can also be helpful to identify potential extreme values.\nThis example will make use of the air quality data one more time.\nData The data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality %\u0026gt;% left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) %\u0026gt;% drop_na() ## Warning in left_join(., wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 The residuals can be saved with the resid() function. These can also be added to the original data, which are particularly helpful.\nhead(resid(air_lm)) ## 1 2 3 4 5 6 ## 15.283427 11.186742 25.191433 15.398540 8.246896 -12.749410 airquality$residuals \u0026lt;- resid(air_lm) head(airquality) ## # A tibble: 6 × 14 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 6 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, residuals \u0026lt;dbl\u0026gt; gf_density(~ residuals, data = airquality) %\u0026gt;% gf_labs(x = \u0026quot;Residuals\u0026quot;) ggplot(airquality, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(size = 2) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated.   Standardized Residuals Standardized residuals can be another way to explore the residuals. These will now be standardized to have a variance of 1, similar to that of a Z-score. These can be computed as:\n\\[ standardized\\ residuals = \\frac{\\epsilon_{i}}{SD_{\\epsilon}} \\]\nWithin R, these can be computed using the function rstandard(). Furthermore, these can be computed from another package called broom with the augment() function.\nhead(rstandard(air_lm)) ## 1 2 3 4 5 6 ## 0.8466153 0.6196983 1.3955421 0.8529766 0.4568937 -0.7062638 library(broom) resid_diagnostics \u0026lt;- augment(air_lm) head(resid_diagnostics) ## # A tibble: 6 × 8 ## daily_aqi avg_wind .fitted .resid .hat .sigma .cooksd .std.resid ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 57 2.94 41.7 15.3 0.000266 18.1 0.0000953 0.847 ## 2 54 2.45 42.8 11.2 0.000318 18.1 0.0000611 0.620 ## 3 69 2.00 43.8 25.2 0.000380 18.1 0.000370 1.40 ## 4 56 3.45 40.6 15.4 0.000230 18.1 0.0000836 0.853 ## 5 54 1.12 45.8 8.25 0.000539 18.1 0.0000563 0.457 ## 6 22 6.09 34.7 -12.7 0.000319 18.1 0.0000795 -0.706 gf_density(~ .std.resid, data = resid_diagnostics) %\u0026gt;% gf_labs(x = \u0026quot;Standardized Residuals\u0026quot;) ggplot(resid_diagnostics, aes(sample = .std.resid)) + stat_qq(size = 5) + stat_qq_line(size = 2)   Homogeneity of variance Homoegeneity of variance is an assumption that is of larger concern compared to normality of the residuals. Homoogeneity of variance is an assumption that states that the variance of the residuals are similar across the predicted or fitted values from the regression line. This assumption can be explored by looking at the residuals (standardized or raw residuals), by the fitted or predicted values. Within this plot, the range of residuals should be similar across the range of fitted or predicted values.\ngf_point(.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %\u0026gt;% gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) %\u0026gt;% gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Residuals\u0026#39;) gf_point(.std.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %\u0026gt;% gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) %\u0026gt;% gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Standardized Residuals\u0026#39;) Another figure that can also be helpful for the homogeneity of variance assumption is one that rescales the residuals on the y-axis. The rescaling makes all the standardized residuals positive (takes the absolute value) and then takes the square root of this.\nresid_diagnostics %\u0026gt;% mutate(sqrt_abs_sresid = sqrt(abs(.std.resid))) %\u0026gt;% gf_point(sqrt_abs_sresid ~ .fitted, size = 5, alpha = .15) %\u0026gt;% gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) %\u0026gt;% gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Sqrt Abs Standardized Residuals\u0026#39;)  Data with high leverage Data with high leverage are extreme values that may significantly impact the regression estimates. These statistics include extreme values for the outcome or predictor attributes. Cook’s distance is one statistic that can help to identify points with high impact/leverage for the regression estimates. Cook’s distance is a statistic that represents how much change there would be in the fitted values if the point was removed when estimating the regression coefficients. There is some disagreement between what type of thresholds to use for Cook’s distance, but one rule of thumb is Cook’s distance greater than 1. There has also been some research showing that Cook’s distance follows an F-distribution, so a more specific value could be computed. The rule of thumb for greater than 1 comes from the F distribution for large samples.\nresid_diagnostics %\u0026gt;% mutate(obs_num = 1:n()) %\u0026gt;% gf_col(.cooksd ~ obs_num, fill = \u0026#39;black\u0026#39;, color = \u0026#39;black\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Cook\u0026#39;s Distance\u0026quot;) Another leave one out statistic is the studentized deleted residuals. These are computed by removing a data point, refitting the regression model, then generate a predicted value for the X value for the data point removed. Then the residual is computed the same as before and is standardized like the standardized residuals above. The function in R to compute these is rstudent().\nhead(rstudent(air_lm)) ## 1 2 3 4 5 6 ## 0.8465904 0.6196587 1.3956793 0.8529524 0.4568561 -0.7062271 airquality$student_residuals \u0026lt;- rstudent(air_lm) head(airquality) ## # A tibble: 6 × 15 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 7 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, residuals \u0026lt;dbl\u0026gt;, ## # student_residuals \u0026lt;dbl\u0026gt; airquality %\u0026gt;% mutate(obs_num = 1:n()) %\u0026gt;% gf_point(student_residuals ~ obs_num, size = 5, alpha = .15) %\u0026gt;% gf_hline(yintercept = ~ 3, color = \u0026#39;blue\u0026#39;, size = 2) %\u0026gt;% gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Studentized Residuals\u0026quot;) Leverage can be another measure to help detect outliers in X values. The hat values that were computed from the augment() function above and can be interpreted as the distance the X scores are from the center of all X predictors. In the case of a single predictor, the hat values are the distance the X score is from the mean of X. The hat values will also sum up to the number of predictors and will always range between 0 and 1.\nresid_diagnostics %\u0026gt;% mutate(obs_num = 1:n()) %\u0026gt;% gf_col(.hat ~ obs_num, fill = \u0026#39;black\u0026#39;, color = \u0026#39;black\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Hat Values (leverage)\u0026quot;) plot(air_lm, which = 1:5)  Solutions to overcome data conditions not being met There are some strategies that can be done when a variety of data conditions for linear regression have not been met. In general, the strategies stem around generalizing the model and adding some complexity.\nThe following are options that would be possible:\nAdd interactions or other non-linear terms Use more complicated model  weighted least squares for homogeneity of variance concerns models that include measurement error mixed models for correlated data  Transform the data.    ","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"c603bd50d5b7676892b81eba2ae9ddf7","permalink":"https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/lectures/06-regression-conditions/","section":"lectures","summary":"Conditions for Linear Regression The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:\n\\[ Y - \\hat{Y} \\]\nThese are the deviations in the observed scores from the predicted scores from the linear regression.","tags":null,"title":"Conditions for Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed assignment per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nIncluding Statistical Evidence This assignment is worth a total of 10 points spread over 10 questions. Please make sure to include any statistical evidence to support your statements, this could include graphics or statistics. This assignment also gives you a choice about what question you are interested in exploring within these data, therefore, including the statistical evidence is extremely important. Failure to include statistical evidence to support claims will result in a 1pt deduction.\nDue Date Due around November 6th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nThe data for this assignment is fertility data for countries. The data originally were compiled from a variety of sources such as the World Bank and comes from a colleague of mine who uses the data in their class.\nData to use: The data can be obtained in csv format from GitHub. The data are also posted to the data folder within the IDAS. A short description for each attribute is as follows.\n   variable class description     country character Country Name   region character Region of the world (7 total)   fertility_rate double Average number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates.   educ_female double Average number of years of formal education (schooling) for females   infant_mortality double Number of infants dying before reaching one year of age, per 1,000 live births in a given year.   contraceptive double Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.   gni_class character Categorization based on country’s gross national income per capita (calculated using the World Bank Atlas method); Low = Low-income economies; GNI per capita of $1,025 or less, Low/Middle = Lower-middle-income economies; GNI per capita between $1,026 and $3,995; Upper/Middle = Upper middle-income economies; GNI per capita between $3,996 and $12,375; Upper = High-income economies; GNI per capita of $12,376 or more.   high_gni double Dummy variable indicating if the country is has an upper-middle or high income economy (low- or low/middle-income = 0; upper/middle or upper income = 1)   region_simple character Region of the world, simplified. North/South America are combined and South Asia is combined with East Asia and Pacific (5 total regions)    Note This is a new assignment that does not build off of the first two assignments in the course.\nQuestions Note: Each question is worth 1 pt unless otherwise specified.\n  Using the data descriptions above, create a research question to explore for this assignment that includes a categorical predictor. Explore the research question from #1 descriptively. Summarize key similarities/differences from the descriptive analysis. Note, for this first part, include just a single categorical predictor in your research questions.\n  Fit a linear regression model to explore the research question from #1. Interpret each regression coefficient estimate from the linear regression model. That is, what do the specific linear regression coefficients mean in the context of the data problem at hand?\n  Evaluate the linear regression data conditions/assumptions for the model fitted in #3. Summarize the degree to which the model meets the statistical assumptions of linear regression.\n  Summarize the statistical evidence to answer the research question from #1. Be as specific as possible about which tests you are exploring and whether the statistical evidence supports or does not support the null hypothesis.\n  Add any predictors you think may help increase the utility of the statistical model fitted as main effects/additive effects. Note, you are welcome to include more than one attribute in this step, but include at least one. For now, let\u0026rsquo;s not consider any interactions, make sure the terms are additive.\n Perform model comparisons to identify the extent to which this second model performs better than the first model fitted in #3. Summarize the evidence for which model is fitting the data the best. Do the primary conclusions from the model fitted in #3 differ for the first categorical attribute considered?    Attempt to create a figure that captures/summarizes the key takeaway from the statistical results/conclusions from #5. Why did you choose this figure to highlight the statistical results/conclusions?\n  Part 2  Explore an interaction between two categorical attributes. Ensure both categorical attributes are included as main effects too.\n Interpret the interaction estimate. That is, what do the parameter estimates mean in the context of the data? Note: I recommend including at most a two-way interaction effect (i.e., an interaction between two attributes) to promote interpretability here. Also, as part of the interpretation, what does the statistical evidence suggest about whether the interaction effect differs from 0?    Create a figure that summarizes the key statistical results from the model from #7. In a few sentences, describe why this figure is appropriate to summarize the statistical results.\n  Build a new model that adds a new interaction effect between a categorical and a continuous attribute (ensure both terms are included as main effects as well). Note, this model could be the one that you built in part 1 of this assignment, for example with #5. I recommend to aid in interpretability for this assignment, just include one categorical, one continuous, and the interaction effect.\n Interpret the interaction estimate, that is, what does the interaction parameter estimate mean in the context of the data? As part of the intepretation, what does the statistical evidence suggest about whether the interaction effect differs from 0?    Create a figure that summarizes the key statistical results from the model from #9. In a few sentences, describe why this figure is appropriate to summarize the statistical results.\n  ","date":1667520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667520000,"objectID":"6e81801222a7818e1b83b93e282b1f92","permalink":"https://psqf6243.brandonlebeau.org/assignments/assignment/assignment3/","publishdate":"2022-11-04T00:00:00Z","relpermalink":"/assignments/assignment/assignment3/","section":"assignments","summary":"The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Assignment 3","type":"book"},{"authors":null,"categories":null,"content":"  Bootstrap The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework. The only real assumption when doing a bootstrap approach is that the sample is obtained randomly from the population, an assumption already made in the NHST framework. The primary drawback of the bootstrap approach is that it is computationally expensive, therefore, it can take time to peform the procedure.\nBootstrapping Steps The following are the steps when performing a bootstrap.\nTreat the sample data as the population. Resample, with replacement, from the sample data, ensuring the new sample is the same size as the original. Estimate the model using the resampled data from step 2. Repeat steps 2 and 3 many many times (eg, 10,000 or more). Visualize distribution of effect of interest   Resampling with replacement What is meant by sampling with replacement? Let’s do an example.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors fruit \u0026lt;- data.frame(name = c(\u0026#39;watermelon\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;kumquat\u0026#39;, \u0026#39;grapes\u0026#39;, \u0026#39;canteloupe\u0026#39;, \u0026#39;kiwi\u0026#39;, \u0026#39;banana\u0026#39;)) |\u0026gt; mutate(obs_num = 1:n()) fruit ## name obs_num ## 1 watermelon 1 ## 2 apple 2 ## 3 orange 3 ## 4 kumquat 4 ## 5 grapes 5 ## 6 canteloupe 6 ## 7 kiwi 7 ## 8 banana 8 slice_sample(fruit, n = nrow(fruit), replace = FALSE) ## name obs_num ## 1 orange 3 ## 2 kumquat 4 ## 3 grapes 5 ## 4 watermelon 1 ## 5 banana 8 ## 6 kiwi 7 ## 7 canteloupe 6 ## 8 apple 2 slice_sample(fruit, n = nrow(fruit), replace = TRUE) ## name obs_num ## 1 canteloupe 6 ## 2 watermelon 1 ## 3 apple 2 ## 4 grapes 5 ## 5 canteloupe 6 ## 6 orange 3 ## 7 kiwi 7 ## 8 canteloupe 6  More practical example Let’s load some data to do a more practical example. The following data come from a TIMSS on science achievement. The data provided is a subset of the available data and is not intended to be representative. Below is a short description of the data. Please don’t hesitate to send any data related questions, happy to provide additional help on interpreting the data appropriately.\n IDSTUD: A unique student ID ITBIRTHM: The birth month of the student ITBIRTHY: The birth year of the student ITSEX: The birth sex of the student ASDAGE: The age of the student (at time of test) ASSSCI01: Overall science scale score ASSEAR01: Earth science scale score ASSLIF01: Life science scale score ASSPHY01: Physics scale score ASSKNO01: Science knowing scale score ASSAPP01: Science applying scale score ASSREA01: Science reasoning scale score  library(tidyverse) timss \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/timss_grade4_science.csv\u0026#39;) |\u0026gt; filter(ASDAGE \u0026lt; 15) ## Rows: 10029 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## dbl (12): IDSTUD, ITBIRTHM, ITBIRTHY, ITSEX, ASDAGE, ASSSCI01, ASSEAR01, ASS... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(timss) ## # A tibble: 6 × 12 ## IDSTUD ITBIRTHM ITBIRTHY ITSEX ASDAGE ASSSCI01 ASSEAR01 ASSLIF01 ASSPHY01 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 10101 6 2005 2 9.92 505. 445. 408. 458. ## 2 10102 10 2005 2 9.58 401. 291. 323. 297. ## 3 10103 1 2005 1 10.3 542. 440. 529. 529. ## 4 10104 6 2005 2 9.92 544. 534. 552. 557. ## 5 10105 6 2005 2 9.92 588. 555. 500. 559. ## 6 10106 12 2005 1 9.42 583. 556. 585. 562. ## # ℹ 3 more variables: ASSKNO01 \u0026lt;dbl\u0026gt;, ASSAPP01 \u0026lt;dbl\u0026gt;, ASSREA01 \u0026lt;dbl\u0026gt;  Two Guiding Questions Is the age of a student related to their overall science scale score? Is the life science scale score related to the overall science scale score?   Bivariate exploration It is important to explore relationships bivariately before going to the model phase. To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”. You may also want to fill in an appropriate axis labels in place of “@@” below.\n Summarize the bivariate association  library(ggformula) theme_set(theme_bw(base_size = 18)) gf_point(%% ~ ^^, data = timss, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5, color = \u0026#39;green\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;@@\u0026quot;, y = \u0026quot;@@\u0026quot;)  Estimate Parameters To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”.\n Interpret the following parameter estimates in the context of the current problem. (ie., what do these parameter estimates mean?)  timss_model \u0026lt;- lm(%% ~ ^^, data = timss) summary(timss_model) confint(timss_model)  Resample these data For the astronauts data, to resample, a similar idea can be made. Essentially, we are treating these data as a random sample of some population of space missions. We again, would resample, with replacement, which means that multiple missions would likely show up in the resampling procedure.\nslice_sample(timss, n = nrow(timss), replace = TRUE) |\u0026gt; count(IDSTUD) resamp_lm \u0026lt;- slice_sample(timss, n = nrow(timss), replace = TRUE) |\u0026gt; lm(%% ~ ^^, data = .) summary(resamp_lm) confint(resamp_lm) Questions to consider Do the parameter estimates differ from before? Why or why not? Would you come to substantially different conclusions from the original analysis? Why or why not?  Let’s now repeat this a bunch of times.\nset.seed(2022) resample_timss \u0026lt;- function(data, model_equation) { timss_resample \u0026lt;- slice_sample(data, n = nrow(data), replace = TRUE) lm(model_equation, data = timss_resample) |\u0026gt; coef() } Run this function a bunch of times manually, what happens to the estimates? Why is this happening?\nresample_timss(data = timss, model_equation = %% ~ ^^)   Make sure future.apply is installed The following code chunk makes sure the future.apply function is installed for parallel processing in R. If you get an error, you can uncomment (delete the # symbol) in the first line of code to (hopefully) install the package yourself.\n# install.packages(\u0026quot;future.apply\u0026quot;) library(future.apply) plan(multisession)  Replicate The following code replicates the analysis many times. Pick an initial value for N and fill in the model equation to match your code above. I will ask you to change this N value later.\ntimss_coef \u0026lt;- future.apply::future_replicate(N, resample_timss(data = timss, model_equation = %% ~ ^^), simplify = FALSE) |\u0026gt; dplyr::bind_rows() |\u0026gt; tidyr::pivot_longer(cols = everything()) head(timss_coef)  Visualize results The following code visualizes the results of the analysis above. Explore the following questions.\nWhat does this distribution show/represent? What are key features of this distribution? How do these values compare to the original linear regression results?  Are there comparable statistics here compared to those?   gf_density(~ value, data = timss_coef) |\u0026gt; gf_facet_wrap(~ name, scales = \u0026#39;free\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;Regression Estimates\u0026quot;)  Change the N value Now, change the N value for the replicate step (you can either add a new cell to copy/paste the code or just change it in the code above).\n What happens to the resulting figure when the N increases? What value for N seems to be reasonable? Why?    ","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695254400,"objectID":"abb435a371c9f3fa75a247571205d00d","permalink":"https://psqf6243.brandonlebeau.org/lectures/07-bootstrap/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/lectures/07-bootstrap/","section":"lectures","summary":"Bootstrap The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework.","tags":null,"title":"Bootstrap","type":"book"},{"authors":null,"categories":null,"content":"  Multiple Regression So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single \\(X\\) attribute.\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nHowever, there are many situations where we would want more than one predictor attribute in the data. We can simply add another predictor attribute and this would look like the following regression equation.\n\\[ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon \\]\nwhere \\(X_{1}\\) and \\(X_{2}\\) are two different predictors attributes. Let’s dive right into some data to explore this a bit more.\n\\[ Y = \\beta_{0} + \\beta_{1} X_{1}^{X_{2}} + \\epsilon \\]\nData For this portion, using data from the college scorecard representing information about higher education institutions.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) college \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv\u0026quot;) ## Rows: 7058 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (6): instnm, city, stabbr, preddeg, region, locale ## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(college) ## # A tibble: 6 × 16 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Alabama A… Norm… AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universit… Birm… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Amridge U… Mont… AL Bachel… South… City:… NA NA 322 15080 ## 4 Universit… Hunt… AL Bachel… South… City:… 0.812 28 6917 22108 ## 5 Alabama S… Mont… AL Bachel… South… City:… 0.979 18 4189 19413 ## 6 The Unive… Tusc… AL Bachel… South… City:… 0.533 28 32387 28836 ## # ℹ 6 more variables: costt4_p \u0026lt;dbl\u0026gt;, tuitionfee_in \u0026lt;dbl\u0026gt;, ## # tuitionfee_out \u0026lt;dbl\u0026gt;, debt_mdn \u0026lt;dbl\u0026gt;, grad_debt_mdn \u0026lt;dbl\u0026gt;, female \u0026lt;dbl\u0026gt;  Question Suppose we were interested in exploring admission rates and which attributes helped to explain variation in admission rates.\ngf_density(~ adm_rate, data = college) %\u0026gt;% gf_labs(x = \u0026quot;Admission Rate\u0026quot;) ## Warning: Removed 5039 rows containing non-finite values (`stat_density()`). gf_point(adm_rate ~ ugds, data = college, size = 4, alpha = .4) %\u0026gt;% gf_smooth(method = \u0026#39;loess\u0026#39;, linewidth = 3) %\u0026gt;% gf_labs(x = \u0026quot;Enrollment Size\u0026quot;, y = \u0026quot;Admission Rate\u0026quot;) ## Warning: Removed 5039 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 5039 rows containing missing values (`geom_point()`). filter(college, ugds \u0026gt; 50000) ## # A tibble: 9 × 16 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Universit… Orla… FL Bachel… South… Subur… 0.499 27 56366 20950 ## 2 Miami Dad… Miami FL Associ… South… City:… NA NA 51015 12476 ## 3 Ivy Tech … Indi… IN Certif… Great… City:… NA NA 61700 12438 ## 4 Southern … Manc… NH Bachel… New E… Subur… 0.780 NA 68214 49005 ## 5 Houston C… Hous… TX Associ… South… City:… NA NA 56976 9956 ## 6 Lone Star… The … TX Associ… South… City:… NA NA 59107 11411 ## 7 Texas A \u0026amp;… Coll… TX Bachel… South… City:… 0.705 28 52568 28143 ## 8 Western G… Salt… UT Bachel… Rocky… Subur… NA NA 72385 NA ## 9 Universit… Tempe AZ Bachel… South… City:… NA NA 77269 20083 ## # ℹ 6 more variables: costt4_p \u0026lt;dbl\u0026gt;, tuitionfee_in \u0026lt;dbl\u0026gt;, ## # tuitionfee_out \u0026lt;dbl\u0026gt;, debt_mdn \u0026lt;dbl\u0026gt;, grad_debt_mdn \u0026lt;dbl\u0026gt;, female \u0026lt;dbl\u0026gt; # install.packages(\u0026quot;GGally\u0026quot;) library(GGally) ## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: ## method from ## +.gg ggplot2 ggpairs(college[c(\u0026#39;adm_rate\u0026#39;, \u0026#39;actcmmid\u0026#39;, \u0026#39;costt4_a\u0026#39;, \u0026#39;tuitionfee_in\u0026#39;)]) ## Warning: Removed 5039 rows containing non-finite values (`stat_density()`). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5769 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5236 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5199 rows containing missing values ## Warning: Removed 5769 rows containing missing values (`geom_point()`). ## Warning: Removed 5769 rows containing non-finite values (`stat_density()`). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5777 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5769 rows containing missing values ## Warning: Removed 5236 rows containing missing values (`geom_point()`). ## Warning: Removed 5777 rows containing missing values (`geom_point()`). ## Warning: Removed 3486 rows containing non-finite values (`stat_density()`). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 3486 rows containing missing values ## Warning: Removed 5199 rows containing missing values (`geom_point()`). ## Warning: Removed 5769 rows containing missing values (`geom_point()`). ## Warning: Removed 3486 rows containing missing values (`geom_point()`). ## Warning: Removed 3049 rows containing non-finite values (`stat_density()`). Let’s now fit a multiple regression model.\ncollege \u0026lt;- mutate(college, cost_a_1000 = costt4_a / 1000, act_mean = actcmmid - mean(actcmmid, na.rm = TRUE)) select(college, actcmmid, act_mean) |\u0026gt; head() ## # A tibble: 6 × 2 ## actcmmid act_mean ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 18 -5.43 ## 2 25 1.57 ## 3 NA NA ## 4 28 4.57 ## 5 18 -5.43 ## 6 28 4.57 adm_mult_reg \u0026lt;- lm(adm_rate ~ I(actcmmid - mean(actcmmid, na.rm = TRUE)) + I(cost_a_1000 - mean(cost_a_1000, na.rm = TRUE)), data = college) broom::tidy(adm_mult_reg) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.683 0.00635 108. 0 ## 2 I(actcmmid - mean(actcmmid, na.rm = TRU… -0.0167 0.00165 -10.1 3.46e-23 ## 3 I(cost_a_1000 - mean(cost_a_1000, na.rm… -0.00230 0.000405 -5.69 1.55e- 8 This model is now the following form:\n\\[ Admission\\ Rate = 1.1 + -0.017 ACT + -0.000002 cost + \\epsilon \\]\nWhy are these parameter estimates so small? How well is the overall model doing? Are both terms important in understanding variation in admission rates? How can you tell?   Variance decomposition Ultimately, multiple regression is a decomposition of variance. Recall,\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} - \\bar{Y})^2 + \\sum (Y - \\hat{Y})^2 \\\\[10pt] SS_{Total} = SS_{Regression} + SS_{Error} \\]\nMultiple regression still does this, but now, there are two attributes going into helping explain variation in the regression portion of the variance decomposition. How is this variance decomposed by default? For linear regression, the variance decomposition is commonly done using type I sum of square decomposition. What does this mean? Essentially, this means that additional terms are added to determine if they help explain variation over and above the other terms in the model. This is a conditional variance added. For example, given the model above, the variance decomposition could be broken down into the following.\n\\[ SS_{Total} = SS_{Regression} + SS_{Error} \\\\[10pt] SS_{Total} = SS_{X_{1}} + SS_{X_{2} | X_{1}} + SS_{Error} \\]\nact_lm \u0026lt;- lm(adm_rate ~ actcmmid, data = college) summary(act_lm) ## ## Call: ## lm(formula = adm_rate ~ actcmmid, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71615 -0.12521 0.02024 0.14490 0.37314 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.180914 0.032988 35.80 \u0026lt;2e-16 *** ## actcmmid -0.022162 0.001391 -15.93 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1844 on 1287 degrees of freedom ## (5769 observations deleted due to missingness) ## Multiple R-squared: 0.1647,\tAdjusted R-squared: 0.1641 ## F-statistic: 253.8 on 1 and 1287 DF, p-value: \u0026lt; 2.2e-16 act_lm \u0026lt;- lm(adm_rate ~ actcmmid, data = college) summary(act_lm) ## ## Call: ## lm(formula = adm_rate ~ actcmmid, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71615 -0.12521 0.02024 0.14490 0.37314 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.180914 0.032988 35.80 \u0026lt;2e-16 *** ## actcmmid -0.022162 0.001391 -15.93 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1844 on 1287 degrees of freedom ## (5769 observations deleted due to missingness) ## Multiple R-squared: 0.1647,\tAdjusted R-squared: 0.1641 ## F-statistic: 253.8 on 1 and 1287 DF, p-value: \u0026lt; 2.2e-16 sqrt(.034017) ## [1] 0.184437 anova(act_lm) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## actcmmid 1 8.635 8.6351 253.84 \u0026lt; 2.2e-16 *** ## Residuals 1287 43.781 0.0340 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 anova(adm_mult_reg) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value ## I(actcmmid - mean(actcmmid, na.rm = TRUE)) 1 8.386 8.3861 255.092 ## I(cost_a_1000 - mean(cost_a_1000, na.rm = TRUE)) 1 1.065 1.0655 32.411 ## Residuals 1278 42.014 0.0329 ## Pr(\u0026gt;F) ## I(actcmmid - mean(actcmmid, na.rm = TRUE)) \u0026lt; 2.2e-16 *** ## I(cost_a_1000 - mean(cost_a_1000, na.rm = TRUE)) 1.546e-08 *** ## Residuals ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 cost_lm \u0026lt;- lm(adm_rate ~ costt4_a, data = drop_na(college, costt4_a, actcmmid)) summary(cost_lm) ## ## Call: ## lm(formula = adm_rate ~ costt4_a, data = drop_na(college, costt4_a, ## actcmmid)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.56548 -0.12036 0.02441 0.13285 0.36629 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 8.257e-01 1.357e-02 60.83 \u0026lt;2e-16 *** ## costt4_a -4.579e-06 3.496e-07 -13.10 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1884 on 1279 degrees of freedom ## Multiple R-squared: 0.1183,\tAdjusted R-squared: 0.1176 ## F-statistic: 171.6 on 1 and 1279 DF, p-value: \u0026lt; 2.2e-16 anova(cost_lm) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## costt4_a 1 6.088 6.0885 171.61 \u0026lt; 2.2e-16 *** ## Residuals 1279 45.377 0.0355 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 adm_mult_cost \u0026lt;- lm(adm_rate ~ costt4_a + actcmmid, data = college) anova(adm_mult_cost) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## costt4_a 1 6.088 6.0885 185.2 \u0026lt; 2.2e-16 *** ## actcmmid 1 3.363 3.3631 102.3 \u0026lt; 2.2e-16 *** ## Residuals 1278 42.014 0.0329 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 library(car) ## Loading required package: carData ## ## Attaching package: \u0026#39;car\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## recode ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## some Anova(adm_mult_cost, type = 3) ## Anova Table (Type III tests) ## ## Response: adm_rate ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 38.294 1 1164.836 \u0026lt; 2.2e-16 *** ## costt4_a 1.065 1 32.411 1.546e-08 *** ## actcmmid 3.363 1 102.301 \u0026lt; 2.2e-16 *** ## Residuals 42.014 1278 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 more_than_2 \u0026lt;- lm(adm_rate ~ costt4_a + actcmmid + tuitionfee_in + ugds + debt_mdn, data = college) broom::tidy(more_than_2) ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1.01 0.0406 25.0 1.98e-112 ## 2 costt4_a -0.00000616 0.00000200 -3.08 2.15e- 3 ## 3 actcmmid -0.0160 0.00187 -8.55 3.52e- 17 ## 4 tuitionfee_in 0.00000210 0.00000212 0.989 3.23e- 1 ## 5 ugds -0.000000197 0.000000823 -0.239 8.11e- 1 ## 6 debt_mdn 0.0000117 0.00000129 9.08 4.21e- 19 anova(more_than_2) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## costt4_a 1 6.175 6.1745 201.9919 \u0026lt;2e-16 *** ## actcmmid 1 3.281 3.2812 107.3402 \u0026lt;2e-16 *** ## tuitionfee_in 1 0.073 0.0727 2.3795 0.1232 ## ugds 1 0.000 0.0003 0.0086 0.9261 ## debt_mdn 1 2.518 2.5178 82.3654 \u0026lt;2e-16 *** ## Residuals 1268 38.761 0.0306 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 broom::glance(more_than_2) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.237 0.234 0.175 78.8 4.26e-72 5 417. -820. -784. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; names(college) ## [1] \u0026quot;instnm\u0026quot; \u0026quot;city\u0026quot; \u0026quot;stabbr\u0026quot; \u0026quot;preddeg\u0026quot; ## [5] \u0026quot;region\u0026quot; \u0026quot;locale\u0026quot; \u0026quot;adm_rate\u0026quot; \u0026quot;actcmmid\u0026quot; ## [9] \u0026quot;ugds\u0026quot; \u0026quot;costt4_a\u0026quot; \u0026quot;costt4_p\u0026quot; \u0026quot;tuitionfee_in\u0026quot; ## [13] \u0026quot;tuitionfee_out\u0026quot; \u0026quot;debt_mdn\u0026quot; \u0026quot;grad_debt_mdn\u0026quot; \u0026quot;female\u0026quot; ## [17] \u0026quot;cost_a_1000\u0026quot; \u0026quot;act_mean\u0026quot;   ","date":1696464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696464000,"objectID":"ab99afac41efd5820604a788a33e7914","permalink":"https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression-class/","publishdate":"2023-10-05T00:00:00Z","relpermalink":"/lectures/08-multiple-regression-class/","section":"lectures","summary":"Multiple Regression So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single \\(X\\) attribute.\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]","tags":null,"title":"Multiple Regression - In Class Notes","type":"book"},{"authors":null,"categories":null,"content":"  Multiple Regression So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single \\(X\\) attribute.\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nHowever, there are many situations where we would want more than one predictor attribute in the data. We can simply add another predictor attribute and this would look like the following regression equation.\n\\[ Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon \\]\nwhere \\(X_{1}\\) and \\(X_{2}\\) are two different predictors attributes. Let’s dive right into some data to explore this a bit more.\nData For this portion, using data from the college scorecard representing information about higher education institutions.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) college \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv\u0026quot;) ## Rows: 7058 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (6): instnm, city, stabbr, preddeg, region, locale ## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(college) ## # A tibble: 6 × 16 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Alabama A… Norm… AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universit… Birm… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Amridge U… Mont… AL Bachel… South… City:… NA NA 322 15080 ## 4 Universit… Hunt… AL Bachel… South… City:… 0.812 28 6917 22108 ## 5 Alabama S… Mont… AL Bachel… South… City:… 0.979 18 4189 19413 ## 6 The Unive… Tusc… AL Bachel… South… City:… 0.533 28 32387 28836 ## # ℹ 6 more variables: costt4_p \u0026lt;dbl\u0026gt;, tuitionfee_in \u0026lt;dbl\u0026gt;, ## # tuitionfee_out \u0026lt;dbl\u0026gt;, debt_mdn \u0026lt;dbl\u0026gt;, grad_debt_mdn \u0026lt;dbl\u0026gt;, female \u0026lt;dbl\u0026gt;  Question Suppose we were interested in exploring admission rates and which attributes helped to explain variation in admission rates.\ngf_density(~ adm_rate, data = college) %\u0026gt;% gf_labs(x = \u0026quot;Admission Rate\u0026quot;) ## Warning: Removed 5039 rows containing non-finite values (`stat_density()`). gf_point(adm_rate ~ actcmmid, data = college) %\u0026gt;% gf_smooth(method = \u0026#39;loess\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Median ACT Score\u0026quot;, y = \u0026quot;Admission Rate\u0026quot;) ## Warning: Removed 5769 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 5769 rows containing missing values (`geom_point()`). library(GGally) ## Registered S3 method overwritten by \u0026#39;GGally\u0026#39;: ## method from ## +.gg ggplot2 ggpairs(college[c(\u0026#39;adm_rate\u0026#39;, \u0026#39;actcmmid\u0026#39;, \u0026#39;costt4_a\u0026#39;)]) ## Warning: Removed 5039 rows containing non-finite values (`stat_density()`). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5769 rows containing missing values ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5236 rows containing missing values ## Warning: Removed 5769 rows containing missing values (`geom_point()`). ## Warning: Removed 5769 rows containing non-finite values (`stat_density()`). ## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : ## Removed 5777 rows containing missing values ## Warning: Removed 5236 rows containing missing values (`geom_point()`). ## Warning: Removed 5777 rows containing missing values (`geom_point()`). ## Warning: Removed 3486 rows containing non-finite values (`stat_density()`). Let’s now fit a multiple regression model.\nadm_mult_reg \u0026lt;- lm(adm_rate ~ actcmmid + costt4_a, data = college) summary(adm_mult_reg) ## ## Call: ## lm(formula = adm_rate ~ actcmmid + costt4_a, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65484 -0.12230 0.02291 0.13863 0.37054 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.135e+00 3.326e-02 34.130 \u0026lt; 2e-16 *** ## actcmmid -1.669e-02 1.650e-03 -10.114 \u0026lt; 2e-16 *** ## costt4_a -2.304e-06 4.047e-07 -5.693 1.55e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1813 on 1278 degrees of freedom ## (5777 observations deleted due to missingness) ## Multiple R-squared: 0.1836,\tAdjusted R-squared: 0.1824 ## F-statistic: 143.8 on 2 and 1278 DF, p-value: \u0026lt; 2.2e-16 This model is now the following form:\n\\[ Admission\\ Rate = 1.1 + -0.017 ACT + -0.000002 cost + \\epsilon \\]\nWhy are these parameter estimates so small? How well is the overall model doing? Are both terms important in understanding variation in admission rates? How can you tell?   Variance decomposition Ultimately, multiple regression is a decomposition of variance. Recall,\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (\\hat{Y} - \\bar{Y})^2 + \\sum (Y - \\hat{Y})^2 \\\\[10pt] SS_{Total} = SS_{Regression} + SS_{Error} \\]\nMultiple regression still does this, but now, there are two attributes going into helping explain variation in the regression portion of the variance decomposition. How is this variance decomposed by default? For linear regression, the variance decomposition is commonly done using type I sum of square decomposition. What does this mean? Essentially, this means that additional terms are added to determine if they help explain variation over and above the other terms in the model. This is a conditional variance added. For example, given the model above, the variance decomposition could be broken down into the following.\n\\[ SS_{Total} = SS_{Regression} + SS_{Error} \\\\[10pt] SS_{Total} = SS_{X_{1}} + SS_{X_{2} | X_{1}} + SS_{Error} \\]\nact_lm \u0026lt;- lm(adm_rate ~ actcmmid, data = college) summary(act_lm) ## ## Call: ## lm(formula = adm_rate ~ actcmmid, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71615 -0.12521 0.02024 0.14490 0.37314 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.180914 0.032988 35.80 \u0026lt;2e-16 *** ## actcmmid -0.022162 0.001391 -15.93 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1844 on 1287 degrees of freedom ## (5769 observations deleted due to missingness) ## Multiple R-squared: 0.1647,\tAdjusted R-squared: 0.1641 ## F-statistic: 253.8 on 1 and 1287 DF, p-value: \u0026lt; 2.2e-16 anova(act_lm) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## actcmmid 1 8.635 8.6351 253.84 \u0026lt; 2.2e-16 *** ## Residuals 1287 43.781 0.0340 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 anova(adm_mult_reg) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## actcmmid 1 8.386 8.3861 255.092 \u0026lt; 2.2e-16 *** ## costt4_a 1 1.065 1.0655 32.411 1.546e-08 *** ## Residuals 1278 42.014 0.0329 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 cost_lm \u0026lt;- lm(adm_rate ~ costt4_a, data = college) summary(cost_lm) ## ## Call: ## lm(formula = adm_rate ~ costt4_a, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.72108 -0.12656 0.02474 0.14459 0.38059 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 8.233e-01 1.151e-02 71.52 \u0026lt;2e-16 *** ## costt4_a -4.147e-06 3.022e-07 -13.73 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1965 on 1820 degrees of freedom ## (5236 observations deleted due to missingness) ## Multiple R-squared: 0.0938,\tAdjusted R-squared: 0.09331 ## F-statistic: 188.4 on 1 and 1820 DF, p-value: \u0026lt; 2.2e-16 anova(cost_lm) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## costt4_a 1 7.278 7.2781 188.4 \u0026lt; 2.2e-16 *** ## Residuals 1820 70.309 0.0386 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1   ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"ace1d2e881456a0b7e0b2bd453b786f4","permalink":"https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/lectures/08-multiple-regression/","section":"lectures","summary":"Multiple Regression So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single \\(X\\) attribute.\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]","tags":null,"title":"Multiple Regression","type":"book"},{"authors":null,"categories":null,"content":"  Model Comparison This section of multiple regression is going to explore model comparison, to guide the selection of the best fitting model from a set of competing models.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) college \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv\u0026quot;) %\u0026gt;% mutate(act_mean = actcmmid - mean(actcmmid, na.rm = TRUE), cost_mean = costt4_a - mean(costt4_a, na.rm = TRUE)) %\u0026gt;% drop_na(act_mean, cost_mean) ## Rows: 7058 Columns: 16 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (6): instnm, city, stabbr, preddeg, region, locale ## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(college) ## # A tibble: 6 × 18 ## instnm city stabbr preddeg region locale adm_rate actcmmid ugds costt4_a ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Alabama A… Norm… AL Bachel… South… City:… 0.903 18 4824 22886 ## 2 Universit… Birm… AL Bachel… South… City:… 0.918 25 12866 24129 ## 3 Universit… Hunt… AL Bachel… South… City:… 0.812 28 6917 22108 ## 4 Alabama S… Mont… AL Bachel… South… City:… 0.979 18 4189 19413 ## 5 The Unive… Tusc… AL Bachel… South… City:… 0.533 28 32387 28836 ## 6 Auburn Un… Mont… AL Bachel… South… City:… 0.825 22 4211 19892 ## # ℹ 8 more variables: costt4_p \u0026lt;dbl\u0026gt;, tuitionfee_in \u0026lt;dbl\u0026gt;, ## # tuitionfee_out \u0026lt;dbl\u0026gt;, debt_mdn \u0026lt;dbl\u0026gt;, grad_debt_mdn \u0026lt;dbl\u0026gt;, female \u0026lt;dbl\u0026gt;, ## # act_mean \u0026lt;dbl\u0026gt;, cost_mean \u0026lt;dbl\u0026gt; dim(college) ## [1] 1281 18 adm_mult_reg \u0026lt;- lm(adm_rate ~ act_mean + cost_mean, data = college) summary(adm_mult_reg) ## ## Call: ## lm(formula = adm_rate ~ act_mean + cost_mean, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65484 -0.12230 0.02291 0.13863 0.37054 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.834e-01 6.346e-03 107.681 \u0026lt; 2e-16 *** ## act_mean -1.669e-02 1.650e-03 -10.114 \u0026lt; 2e-16 *** ## cost_mean -2.304e-06 4.047e-07 -5.693 1.55e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1813 on 1278 degrees of freedom ## Multiple R-squared: 0.1836,\tAdjusted R-squared: 0.1824 ## F-statistic: 143.8 on 2 and 1278 DF, p-value: \u0026lt; 2.2e-16 Omnibus Hypothesis In linear regression, I omitted a portion of the output from above that typically comes with most statistical output. That is, there is a test statistic that aims to test if the model is explaining variation over and above a simple mean. More specifically, this omnibus hypothesis tests the following:\n\\[ H_{0}: All\\ \\beta = 0 \\\\[10pt] H_{A}: Any\\ \\beta \\neq 0 \\]\nThis hypothesis can be formally tested with an F-statistic which is distributed as an F distribution with \\(p\\) predictor attributes and \\(n - p - 1\\) degrees of freedom.\nf_data \u0026lt;- data.frame(value = seq(0, 5, by = .01)) %\u0026gt;% mutate(dens = df(value, 2, 1278)) gf_line(dens ~ value, data = f_data, size = 2) f_data \u0026lt;- data.frame(value = seq(0, 5, by = .01)) %\u0026gt;% mutate(dens = df(value, 5, 50)) gf_line(dens ~ value, data = f_data, size = 2)  Adjusted R-squared The adjusted R-squared is typically used when comparing models. This statistic is commonly used as R-square represents the ratio between explained and total variance, therefore, this will always increase, even if the new attribute entered is not helpful. The adjusted R-squared tries to adjust for model complexity. There are many ways to do this, but the most common will be defined here.\n\\[ \\bar{R}^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\]\nor\n\\[ \\bar{R}^2 = 1 - \\frac{SS_{res} / df_{e}}{SS_{tot} / df_{t}} \\] where \\(p\\) is the number of predictors (excluding the intercept), \\(n\\) is the sample size, \\(SS_{e}\\) and \\(SS_{tot}\\) are sum of square residual and total respectively, and \\(df_{e}\\) and \\(df_{t}\\) are degrees of freedom for the error (\\(n - p - 1\\)) and total (\\(n - 1\\)) respectively.\nsummary(adm_mult_reg) ## ## Call: ## lm(formula = adm_rate ~ act_mean + cost_mean, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65484 -0.12230 0.02291 0.13863 0.37054 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.834e-01 6.346e-03 107.681 \u0026lt; 2e-16 *** ## act_mean -1.669e-02 1.650e-03 -10.114 \u0026lt; 2e-16 *** ## cost_mean -2.304e-06 4.047e-07 -5.693 1.55e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1813 on 1278 degrees of freedom ## Multiple R-squared: 0.1836,\tAdjusted R-squared: 0.1824 ## F-statistic: 143.8 on 2 and 1278 DF, p-value: \u0026lt; 2.2e-16 1 - (1 - .1836) * (1281 - 1) / (1281 - 2 - 1) ## [1] 0.1823224 anova(adm_mult_reg) ## Analysis of Variance Table ## ## Response: adm_rate ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## act_mean 1 8.386 8.3861 255.092 \u0026lt; 2.2e-16 *** ## cost_mean 1 1.065 1.0655 32.411 1.546e-08 *** ## Residuals 1278 42.014 0.0329 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 1 - (42.01 / 1278) / ((8.39 + 1.066 + 42.014) / 1280) ## [1] 0.1825191  Model Comparison There are a variety of statistics used to provide statistical evidence for competing models. If the models are nested, then the variance decomposition can be used to determine if the added predictors helped to explain significant variation over and above the simpler model.\nIn this situation, another F statistic can be derived where\n\\[ F = \\frac{SS_{res}^{R} - SS_{res}^{F} / \\Delta p}{SS_{res}^{F} / df_{F}} \\]\nact_lm \u0026lt;- lm(adm_rate ~ act_mean, data = college) summary(act_lm) ## ## Call: ## lm(formula = adm_rate ~ act_mean, data = college) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71229 -0.12483 0.01999 0.14317 0.37289 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.661593 0.005128 129.02 \u0026lt;2e-16 *** ## act_mean -0.021905 0.001388 -15.78 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.1835 on 1279 degrees of freedom ## Multiple R-squared: 0.1629,\tAdjusted R-squared: 0.1623 ## F-statistic: 249 on 1 and 1279 DF, p-value: \u0026lt; 2.2e-16 anova(act_lm, adm_mult_reg) ## Analysis of Variance Table ## ## Model 1: adm_rate ~ act_mean ## Model 2: adm_rate ~ act_mean + cost_mean ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 1279 43.079 ## 2 1278 42.014 1 1.0655 32.411 1.546e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ((43.08 - 42.01) / 1) ## [1] 1.07 1.07 / (42.01 / 1278) ## [1] 32.55082 Non-nested models For non-nested models, the F-statistic defined above will not work. Instead other statistics are needed to evaluate which model is the best. The one that I prefer for this is the AIC (Akaike information criteria) or the related small sample form, AICc. The equations for these aren’t all that useful, utilizing software is the best way to compute these statistics. In general, smaller AIC values indicate a better fitting model.\nlibrary(AICcmodavg) cost_lm \u0026lt;- lm(adm_rate ~ cost_mean, data = college) aictab(list(cost_lm, act_lm), modnames = c(\u0026#39;cost\u0026#39;, \u0026#39;act\u0026#39;)) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## act 3 -704.26 0.00 1 1 355.14 ## cost 3 -637.70 66.56 0 1 321.86    ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"44f745bd4705674f7b0a127d4d4d1182","permalink":"https://psqf6243.brandonlebeau.org/lectures/09-model-comparison/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/lectures/09-model-comparison/","section":"lectures","summary":"Model Comparison This section of multiple regression is going to explore model comparison, to guide the selection of the best fitting model from a set of competing models.","tags":null,"title":"Multiple Comparisons","type":"book"},{"authors":null,"categories":null,"content":"  Regression with Categorical Predictors This set of notes will explore using linear regression for a single predictor attribute that is categorical instead of continuous. To explore this first, let’s explore some data.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(Lahman) library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) career \u0026lt;- Batting %\u0026gt;% filter(AB \u0026gt; 100) %\u0026gt;% anti_join(Pitching, by = \u0026quot;playerID\u0026quot;) %\u0026gt;% filter(yearID \u0026gt; 1990) %\u0026gt;% group_by(playerID, lgID) %\u0026gt;% summarise(H = sum(H), AB = sum(AB)) %\u0026gt;% mutate(average = H / AB) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. career \u0026lt;- People %\u0026gt;% tbl_df() %\u0026gt;% dplyr::select(playerID, nameFirst, nameLast) %\u0026gt;% unite(name, nameFirst, nameLast, sep = \u0026quot; \u0026quot;) %\u0026gt;% inner_join(career, by = \u0026quot;playerID\u0026quot;) %\u0026gt;% dplyr::select(-playerID) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tibble::as_tibble()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. head(career) ## # A tibble: 6 × 5 ## name lgID H AB average ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jeff Abbott AL 127 459 0.277 ## 2 Kurt Abbott AL 33 123 0.268 ## 3 Kurt Abbott NL 455 1780 0.256 ## 4 Reggie Abercrombie NL 54 255 0.212 ## 5 Brent Abernathy AL 194 767 0.253 ## 6 Shawn Abner AL 81 309 0.262 Question Suppose we are interested in the batting average of baseball players since 1990, that is, the average is:\n\\[ average = \\frac{number\\ of\\ hits}{number\\ of\\ atbats} \\]\nLet’s first visualize this.\ngf_density(~ average, data = career) %\u0026gt;% gf_labs(x = \u0026quot;Batting Average\u0026quot;) What if we hypothesized that the batting average will differ based on the league that players played in.\ngf_violin(lgID ~ average, data = career, fill = \u0026#39;gray80\u0026#39;, draw_quantiles = c(\u0026#39;0.1\u0026#39;, \u0026#39;0.5\u0026#39;, \u0026#39;0.9\u0026#39;)) %\u0026gt;% gf_labs(x = \u0026quot;Batting Average\u0026quot;, y = \u0026quot;League\u0026quot;) The distributions seem similar, but what if we wanted to go a step further and estimate a model to explore if there are really differences or not. For example, suppose we were interested in:\n\\[ H_{0}: \\mu_{NL} = \\mu_{AL} \\]\nWhat type of model could we use? What about linear regression?\n Linear Regression with Categorical Attributes Since these notes are happening, you can assume it is possible. But how can a categorical attribute with categories rather than numbers be included in the linear regression model?\nThe answer is that they can’t. We need a new representation of the categorical attribute, enter dummy or indicator coding.\nDummy/Indicator Coding Suppose we use the following logic:\nIf NL, then give a value of 1, else give a value of 0.\nDoes this give the same information as before?\n  League ID Dummy League ID    AL 0  NL 1    What would this look like for the actual data?\ncareer \u0026lt;- career %\u0026gt;% mutate(league_dummy = ifelse(lgID == \u0026#39;NL\u0026#39;, 1, 0)) head(career, n = 10) ## # A tibble: 10 × 6 ## name lgID H AB average league_dummy ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jeff Abbott AL 127 459 0.277 0 ## 2 Kurt Abbott AL 33 123 0.268 0 ## 3 Kurt Abbott NL 455 1780 0.256 1 ## 4 Reggie Abercrombie NL 54 255 0.212 1 ## 5 Brent Abernathy AL 194 767 0.253 0 ## 6 Shawn Abner AL 81 309 0.262 0 ## 7 Shawn Abner NL 19 115 0.165 1 ## 8 CJ Abrams NL 70 284 0.246 1 ## 9 Bobby Abreu AL 858 3061 0.280 0 ## 10 Bobby Abreu NL 1602 5373 0.298 1 Now that there is a numeric attribute, these can be added into the linear regression model.\naverage_lm \u0026lt;- lm(average ~ league_dummy, data = career) broom::tidy(average_lm) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.252 0.000758 333. 0 ## 2 league_dummy 0.000558 0.00107 0.522 0.602 How are these terms interpreted now?\ndf_stats(average ~ league_dummy, data = career, mean, sd, length) ## response league_dummy mean sd length ## 1 average 0 0.2518959 0.02895094 1461 ## 2 average 1 0.2524535 0.02896021 1477 average_lm2 \u0026lt;- lm(average ~ lgID, data = career) broom::tidy(average_lm2) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.252 0.000758 333. 0 ## 2 lgIDNL 0.000558 0.00107 0.522 0.602 t.test(average ~ lgID, data = career, var.equal = TRUE) ## ## Two Sample t-test ## ## data: average by lgID ## t = -0.52189, df = 2936, p-value = 0.6018 ## alternative hypothesis: true difference in means between group AL and group NL is not equal to 0 ## 95 percent confidence interval: ## -0.002652537 0.001537330 ## sample estimates: ## mean in group AL mean in group NL ## 0.2518959 0.2524535   Values other than 0/1 First, I want to build off of the first part of the notes on regression with categorical predictors. Before generalizing to more than two groups, let’s first explore what happens when values other than 0/1 are used for the categorical attribute. The following three dummy/indicator attributes will be used:\n1 = NL, 0 = AL 1 = NL, 2 = AL 100 = NL, 0 = AL  Make some predictions about what you think will happen in the three separate regressions?\nlibrary(tidyverse) library(Lahman) library(ggformula) theme_set(theme_bw(base_size = 18)) career \u0026lt;- Batting %\u0026gt;% filter(AB \u0026gt; 100) %\u0026gt;% anti_join(Pitching, by = \u0026quot;playerID\u0026quot;) %\u0026gt;% filter(yearID \u0026gt; 1990) %\u0026gt;% group_by(playerID, lgID) %\u0026gt;% summarise(H = sum(H), AB = sum(AB)) %\u0026gt;% mutate(average = H / AB) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. career \u0026lt;- People %\u0026gt;% tbl_df() %\u0026gt;% dplyr::select(playerID, nameFirst, nameLast) %\u0026gt;% unite(name, nameFirst, nameLast, sep = \u0026quot; \u0026quot;) %\u0026gt;% inner_join(career, by = \u0026quot;playerID\u0026quot;) %\u0026gt;% dplyr::select(-playerID) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tibble::as_tibble()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. career \u0026lt;- career %\u0026gt;% mutate(league_dummy = ifelse(lgID == \u0026#39;NL\u0026#39;, 1, 0), league_dummy_12 = ifelse(lgID == \u0026#39;NL\u0026#39;, 1, 2), league_dummy_100 = ifelse(lgID == \u0026#39;NL\u0026#39;, 100, 0)) head(career, n = 10) ## # A tibble: 10 × 8 ## name lgID H AB average league_dummy league_dummy_12 league_dummy_100 ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jeff… AL 127 459 0.277 0 2 0 ## 2 Kurt… AL 33 123 0.268 0 2 0 ## 3 Kurt… NL 455 1780 0.256 1 1 100 ## 4 Regg… NL 54 255 0.212 1 1 100 ## 5 Bren… AL 194 767 0.253 0 2 0 ## 6 Shaw… AL 81 309 0.262 0 2 0 ## 7 Shaw… NL 19 115 0.165 1 1 100 ## 8 CJ A… NL 70 284 0.246 1 1 100 ## 9 Bobb… AL 858 3061 0.280 0 2 0 ## 10 Bobb… NL 1602 5373 0.298 1 1 100 average_lm \u0026lt;- lm(average ~ league_dummy, data = career) broom::tidy(average_lm) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.252 0.000758 333. 0 ## 2 league_dummy 0.000558 0.00107 0.522 0.602 average_lm_12 \u0026lt;- lm(average ~ league_dummy_12, data = career) broom::tidy(average_lm_12) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.253 0.00169 150. 0 ## 2 league_dummy_12 -0.000558 0.00107 -0.522 0.602 average_lm_100 \u0026lt;- lm(average ~ league_dummy_100, data = career) broom::tidy(average_lm_100) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.252 0.000758 333. 0 ## 2 league_dummy_100 0.00000558 0.0000107 0.522 0.602 Before moving to more than 2 groups, any thoughts on how we could run a one-sample t-test using a linear regression? For example, suppose this null hypothesis wanted to be explored.\n\\[ H_{0}: \\mu_{BA} = .2 \\]\n\\[ H_{A}: \\mu_{BA} \\neq .2 \\]\n Generalize to more than 2 groups The ability to use regression with categorical attributes of more than 2 groups is possible and an extension of the 2 groups model shown above. First, let’s think about how we could represent three categories as numeric attributes. Suppose we had the following 4 categories of baseball players.\n  Position    Outfield  Infield  Catcher  Designated Hitter    library(GeomMLBStadiums) ggplot() + geom_mlb_stadium(stadium_segments = \u0026quot;all\u0026quot;) + facet_wrap(~team) + coord_fixed() + theme_void() library(tidyverse) library(Lahman) library(ggformula) theme_set(theme_bw(base_size = 18)) career \u0026lt;- Batting %\u0026gt;% filter(AB \u0026gt; 100) %\u0026gt;% anti_join(Pitching, by = \u0026quot;playerID\u0026quot;) %\u0026gt;% filter(yearID \u0026gt; 1990) %\u0026gt;% group_by(playerID, lgID) %\u0026gt;% summarise(H = sum(H), AB = sum(AB)) %\u0026gt;% mutate(average = H / AB) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. career \u0026lt;- Appearances %\u0026gt;% filter(yearID \u0026gt; 1990) %\u0026gt;% select(-GS, -G_ph, -G_pr, -G_batting, -G_defense, -G_p, -G_lf, -G_cf, -G_rf) %\u0026gt;% rowwise() %\u0026gt;% mutate(g_inf = sum(c_across(G_1b:G_ss))) %\u0026gt;% select(-G_1b, -G_2b, -G_3b, -G_ss) %\u0026gt;% group_by(playerID, lgID) %\u0026gt;% summarise(catcher = sum(G_c), outfield = sum(G_of), dh = sum(G_dh), infield = sum(g_inf), total_games = sum(G_all)) %\u0026gt;% pivot_longer(catcher:infield, names_to = \u0026quot;position\u0026quot;) %\u0026gt;% filter(value \u0026gt; 0) %\u0026gt;% group_by(playerID, lgID) %\u0026gt;% slice_max(value) %\u0026gt;% select(playerID, lgID, position) %\u0026gt;% inner_join(career) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. ## Joining with `by = join_by(playerID, lgID)` career \u0026lt;- People %\u0026gt;% tbl_df() %\u0026gt;% dplyr::select(playerID, nameFirst, nameLast) %\u0026gt;% unite(name, nameFirst, nameLast, sep = \u0026quot; \u0026quot;) %\u0026gt;% inner_join(career, by = \u0026quot;playerID\u0026quot;) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tibble::as_tibble()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. career \u0026lt;- career %\u0026gt;% mutate(league_dummy = ifelse(lgID == \u0026#39;NL\u0026#39;, 1, 0)) count(career, position) ## # A tibble: 4 × 2 ## position n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 catcher 415 ## 2 dh 83 ## 3 infield 1279 ## 4 outfield 1165 gf_violin(position ~ average, data = career, fill = \u0026#39;gray85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %\u0026gt;% gf_labs(x = \u0026quot;Batting Average\u0026quot;, y = \u0026quot;\u0026quot;) career \u0026lt;- career %\u0026gt;% mutate(outfield = ifelse(position == \u0026#39;outfield\u0026#39;, 1, 0), infield = ifelse(position == \u0026#39;infield\u0026#39;, 1, 0), catcher = ifelse(position == \u0026#39;catcher\u0026#39;, 1, 0)) head(career) ## # A tibble: 6 × 11 ## playerID name lgID position H AB average league_dummy outfield ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 abbotje01 Jeff Abbott AL outfield 127 459 0.277 0 1 ## 2 abbotku01 Kurt Abbott AL infield 33 123 0.268 0 0 ## 3 abbotku01 Kurt Abbott NL infield 455 1780 0.256 1 0 ## 4 abercre01 Reggie Abe… NL outfield 54 255 0.212 1 1 ## 5 abernbr01 Brent Aber… AL infield 194 767 0.253 0 0 ## 6 abnersh01 Shawn Abner AL outfield 81 309 0.262 0 1 ## # ℹ 2 more variables: infield \u0026lt;dbl\u0026gt;, catcher \u0026lt;dbl\u0026gt; position_lm \u0026lt;- lm(average ~ 1 + outfield + infield + catcher, data = career) broom::tidy(position_lm) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.255 0.00314 81.3 0 ## 2 outfield -0.000610 0.00325 -0.188 0.851 ## 3 infield -0.00218 0.00324 -0.672 0.501 ## 4 catcher -0.0144 0.00344 -4.19 0.0000288 df_stats(average ~ position, data = career, mean) ## response position mean ## 1 average catcher 0.2409461 ## 2 average dh 0.2553577 ## 3 average infield 0.2531783 ## 4 average outfield 0.2547475   ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"dcb11ffab5d55f67c223c7cc6822b64a","permalink":"https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/lectures/10-regression-categorical/","section":"lectures","summary":"Regression with Categorical Predictors This set of notes will explore using linear regression for a single predictor attribute that is categorical instead of continuous. To explore this first, let’s explore some data.","tags":null,"title":"Regression with Categorical Predictors","type":"book"},{"authors":null,"categories":null,"content":"Quiz 1 can be taken on ICON, due September 18th, 2023. The quiz covers content from Week 1.\nQuiz 1 Link\n","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"ac83706d5690d70e2da1268514b156a4","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz1/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/assignments/quizzes/quiz1/","section":"assignments","summary":"Quiz 1 can be taken on ICON, due September 18th, 2023. The quiz covers content from Week 1.\nQuiz 1 Link","tags":null,"title":"Quiz 1","type":"book"},{"authors":null,"categories":null,"content":"  More details on pairwise contrasts Last week the idea of pairwise contrasts were introduced. Here is a more formal discussion of pairwise contrasts and controlling of familywise type I error rates.\nBonferroni Method This method is introduced first, primarily due to its simplicity. This is not the best measure to use however as will be discussed below. The Bonferroni method makes the following adjustment:\n\\[ \\alpha_{new} = \\frac{\\alpha}{m} \\]\nWhere \\(m\\) is the number of hypotheses being tested. Let’s use a specific example to frame this based on the baseball data.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(Lahman) library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) career \u0026lt;- Batting |\u0026gt; filter(AB \u0026gt; 100) |\u0026gt; anti_join(Pitching, by = \u0026quot;playerID\u0026quot;) |\u0026gt; filter(yearID \u0026gt; 1990) |\u0026gt; group_by(playerID, lgID) |\u0026gt; summarise(H = sum(H), AB = sum(AB)) |\u0026gt; mutate(average = H / AB) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. career \u0026lt;- Appearances |\u0026gt; filter(yearID \u0026gt; 1990) |\u0026gt; select(-GS, -G_ph, -G_pr, -G_batting, -G_defense, -G_p, -G_lf, -G_cf, -G_rf) |\u0026gt; rowwise() |\u0026gt; mutate(g_inf = sum(c_across(G_1b:G_ss))) |\u0026gt; select(-G_1b, -G_2b, -G_3b, -G_ss) |\u0026gt; group_by(playerID, lgID) |\u0026gt; summarise(catcher = sum(G_c), outfield = sum(G_of), dh = sum(G_dh), infield = sum(g_inf), total_games = sum(G_all)) |\u0026gt; pivot_longer(catcher:infield, names_to = \u0026quot;position\u0026quot;) |\u0026gt; filter(value \u0026gt; 0) |\u0026gt; group_by(playerID, lgID) |\u0026gt; slice_max(value) |\u0026gt; select(playerID, lgID, position) |\u0026gt; inner_join(career) ## `summarise()` has grouped output by \u0026#39;playerID\u0026#39;. You can override using the ## `.groups` argument. ## Joining with `by = join_by(playerID, lgID)` career \u0026lt;- People |\u0026gt; tbl_df() |\u0026gt; dplyr::select(playerID, nameFirst, nameLast) |\u0026gt; unite(name, nameFirst, nameLast, sep = \u0026quot; \u0026quot;) |\u0026gt; inner_join(career, by = \u0026quot;playerID\u0026quot;) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tibble::as_tibble()` instead. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. career \u0026lt;- career |\u0026gt; mutate(league_dummy = ifelse(lgID == \u0026#39;NL\u0026#39;, 1, 0)) head(career) ## # A tibble: 6 × 8 ## playerID name lgID position H AB average league_dummy ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 abbotje01 Jeff Abbott AL outfield 127 459 0.277 0 ## 2 abbotku01 Kurt Abbott AL infield 33 123 0.268 0 ## 3 abbotku01 Kurt Abbott NL infield 455 1780 0.256 1 ## 4 abercre01 Reggie Abercrombie NL outfield 54 255 0.212 1 ## 5 abernbr01 Brent Abernathy AL infield 194 767 0.253 0 ## 6 abnersh01 Shawn Abner AL outfield 81 309 0.262 0 position_lm \u0026lt;- lm(average ~ 1 + position, data = career) broom::tidy(position_lm) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.241 0.00140 172. 0 ## 2 positiondh 0.0144 0.00344 4.19 2.88e- 5 ## 3 positioninfield 0.0122 0.00162 7.57 5.03e-14 ## 4 positionoutfield 0.0138 0.00164 8.44 4.97e-17 count(career, position) ## # A tibble: 4 × 2 ## position n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 catcher 415 ## 2 dh 83 ## 3 infield 1279 ## 4 outfield 1165 Given that there are 4 groups, if all pairwise comparisons were of interest, the following would be all possible NULL hypotheses. Note, I am assuming that each of these has a matching alternative hypothesis that states the groups differences are different from 0.\n\\[ H_{0}: \\mu_{catcher} - \\mu_{dh} = 0 \\\\ H_{0}: \\mu_{catcher} - \\mu_{infield} = 0 \\\\ H_{0}: \\mu_{catcher} - \\mu_{outfield} = 0 \\\\ H_{0}: \\mu_{dh} - \\mu_{infield} = 0 \\\\ H_{0}: \\mu_{dh} - \\mu_{outfield} = 0 \\\\ H_{0}: \\mu_{infield} - \\mu_{outfield} = 0 \\\\ \\]\nIn general, to find the total number of hypotheses, you can use the combinations formula:\n\\[ C(n, r) = \\binom{n}{r} = \\frac{n!}{r!(n - r)!} \\]\nFor our example this would lead to:\n\\[ \\binom{4}{2} = \\frac{4!}{2!(4 - 2)!} = \\frac{4 * 3 * 2 * 1}{2 * 1 * (2 * 1)} = \\frac{24}{4} = 6 \\]\nTherefore, using bonferroni’s correction, our new alpha would be:\n\\[ \\alpha_{new} = \\frac{.05}{6} = .0083 \\]\nAlternatively, you could also adjust the p-values to make them smaller and still use .05 (or any other predetermined familywise \\(\\alpha\\) value).\n\\[ p_{new} = \\frac{p_{original}}{m} \\]\nThis is what most software programs do automatically for you.\npairwise.t.test(career$average, career$position, p.adjust = \u0026#39;bonf\u0026#39;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: career$average and career$position ## ## catcher dh infield ## dh 0.00017 - - ## infield 3e-13 1.00000 - ## outfield 3e-16 1.00000 1.00000 ## ## P value adjustment method: bonferroni  What is a type I error? Before moving to why you should care, let’s more formally talk about type I errors. Suppose we have the following table of possible outcomes:\n     \\(H_{0}\\) true \\(H_{0}\\) false    Reject \\(H_{0}\\) Type I Error (\\(\\alpha\\)) Correct - Power (\\(1 - \\beta\\))  Retain \\(H_{0}\\) Correct (\\(1 - \\alpha\\)) Type II Error (\\(\\beta\\))    Type I error is when the null hypothesis is true in the population, but the statistical evidence supports the alternative hypothesis. Type II error is when the null hypothesis is false in the population, but the statistical evidence supports the null hypothesis.\nHere is a good link for an interactive app that shows many of these terms visually: https://rpsychologist.com/d3/nhst/.\nType M and Type S errors These were first defined by Gelman and Tuerlinckx (2000) and is also in section 4.4 of Regression and Other Stories textbook.\n Type S (sign) occurs when the sign of the estimated effect is in the opposite direction as the true effect. Type M (magnitude) occurse when the magnitude of the estimated effect is much different (often larger) than the true effect.  These really shift the narrative away from achieving statistical signficiance and moving toward proper estimation of effects or precision, this is a better goal in my opinion.\n  Holm-Bonferroni Procedure The boneferroni procedure will be overly conservative and I wouldn’t recommend it’s use in practice. If you want a simple approach to p-value adjustment, I’d recommend just setting a specific value for the alpha value to be more conservative, for example setting it to 0.01.\nThe Holm-Bonferroni approach is an adjustment method that is more powerful then the original bonferroni procedure, but does not come with onerous assumptions. The Holm-Boneferroni procedure uses the following steps for adjustment.\nSuppose there are \\(m\\) p-values (ie, \\(m\\) null hypotheses). First, order these from smallest to largest, be sure to keep track of which hypothese the p-value is associated with. Then, 1. Is the smallest p-value less than \\(\\alpha / m\\), if yes, provide evidence for the alternative hypothesis and proceed to the next p-value, it not stop. 2. Is the second smallest p-value less than $ / m - 1$, if eys, provide evidence for the alternative hypothesis and proceed to the next p-value, if not stop. 3. Continue, comparing the p-values in order to the following adjust alpha,\n\\[ \\alpha_{new_{k}} = \\frac{\\alpha}{m + 1 - k} \\]\nwhere is \\(k\\) is the rank of the p-values when ordered from smallest to largest.\nSimilar to the bonferroni, the p-values can be adjusted with the following formula:\n\\[ p_{adj} = min(1, max((m + 1 - k) * p_{k} )) \\]\nwhere is \\(k\\) is the rank of the p-values when ordered from smallest to largest.\npairwise.t.test(career$average, career$position, p.adjust = \u0026#39;holm\u0026#39;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: career$average and career$position ## ## catcher dh infield ## dh 0.00012 - - ## infield 2.5e-13 1.00000 - ## outfield 3.0e-16 1.00000 0.52730 ## ## P value adjustment method: holm pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value ## catcher dh infield ## dh 2.880340e-05 NA NA ## infield 5.033217e-14 0.5013190 NA ## outfield 4.974152e-17 0.8510978 0.1757679 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[3] * (6 + 1 - 1) ## [1] 2.984491e-16 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[2] * (6 + 1 - 2) ## [1] 2.516609e-13 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[1] * (6 + 1 - 3) ## [1] 0.0001152136 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[9] * (6 + 1 - 4) ## [1] 0.5273037 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[5] * (6 + 1 - 5) ## [1] 1.002638 pairwise.t.test(career$average, career$position, p.adjust = \u0026#39;none\u0026#39;)$p.value[6] * (6 + 1 - 6) ## [1] 0.8510978  Exploring assumptions/Data Conditions for model with categorical attributes plot(position_lm, which = 1:5)   ","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"514cdbd530895816f4e29e2c55a35cdc","permalink":"https://psqf6243.brandonlebeau.org/lectures/11-pairwise/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/lectures/11-pairwise/","section":"lectures","summary":"More details on pairwise contrasts Last week the idea of pairwise contrasts were introduced. Here is a more formal discussion of pairwise contrasts and controlling of familywise type I error rates.","tags":null,"title":"Pairwise Contrasts","type":"book"},{"authors":null,"categories":null,"content":"Quiz 2 can be taken on ICON, due September 25th, 2023. The quiz covers content from Week 2.\nQuiz 2 Link\n","date":1694736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694736000,"objectID":"89e911fba74fe6a1dc69de07ae2db619","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz2/","publishdate":"2023-09-15T00:00:00Z","relpermalink":"/assignments/quizzes/quiz2/","section":"assignments","summary":"Quiz 2 can be taken on ICON, due September 25th, 2023. The quiz covers content from Week 2.\nQuiz 2 Link","tags":null,"title":"Quiz 2","type":"book"},{"authors":null,"categories":null,"content":"Quiz 3 can be taken on ICON, due October 2nd, 2023. The quiz covers content from Week 3.\nQuiz 3 Link\n","date":1694736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694736000,"objectID":"ffe02f7f4b85dd056013551821b59d03","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz3/","publishdate":"2023-09-15T00:00:00Z","relpermalink":"/assignments/quizzes/quiz3/","section":"assignments","summary":"Quiz 3 can be taken on ICON, due October 2nd, 2023. The quiz covers content from Week 3.\nQuiz 3 Link","tags":null,"title":"Quiz 3","type":"book"},{"authors":null,"categories":null,"content":"  Add more than one categorical predictor Let’s explore the ability to add more than one categorical predictor to our model. This will then build into thinking through interactions. Let’s load some new data. These are bike data from Philadelphia program, called Indego. The data loaded below are from Q3 of 2023.\nHere is information about the columns in the data.\n trip_id: Locally unique integer that identifies the trip duration: Length of trip in minutes start_time: The date/time when the trip began, presented in ISO 8601 format in local time end_time: The date/time when the trip ended, presented in ISO 8601 format in local time start_station: The station ID where the trip originated (for station name and more information on each station see the Station Table) start_lat: The latitude of the station where the trip originated start_lon: The longitude of the station where the trip originated end_station: The station ID where the trip terminated (for station name and more information on each station see the Station Table) end_lat: The latitude of the station where the trip terminated end_lon: The longitude of the station where the trip terminated bike_id: Locally unique integer that identifies the bike plan_duration: The number of days that the plan the passholder is using entitles them to ride; 0 is used for a single ride plan (Walk-up) trip_route_category: “Round Trip” for trips starting and ending at the same station or “One Way” for all other trips passholder_type: The name of the passholder’s plan bike_type: The kind of bike used on the trip, including standard pedal-powered bikes or electric assist bikes  library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) temp \u0026lt;- tempfile() download.file(\u0026quot;https://bicycletransit.wpenginepowered.com/wp-content/uploads/2023/10/indego-trips-2023-q3.zip\u0026quot;, temp) bike \u0026lt;- readr::read_csv(unz(temp, \u0026quot;indego-trips-2023-q3-2.csv\u0026quot;)) |\u0026gt; filter(duration \u0026lt;= 120 \u0026amp; passholder_type != \u0026#39;Walk-up\u0026#39;) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, ## e.g.: ## dat \u0026lt;- vroom(...) ## problems(dat) ## Rows: 353256 Columns: 15 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): start_time, end_time, trip_route_category, passholder_type, bike_type ## dbl (10): trip_id, duration, start_station, start_lat, start_lon, end_statio... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. unlink(temp) head(bike) ## # A tibble: 6 × 15 ## trip_id duration start_time end_time start_station start_lat start_lon ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 677293140 2 7/1/2023 0:00 7/1/2023 0… 3271 39.9 -75.2 ## 2 677304406 27 7/1/2023 0:00 7/1/2023 0… 3060 40.0 -75.2 ## 3 677304584 32 7/1/2023 0:00 7/1/2023 0… 3057 40.0 -75.2 ## 4 677302282 6 7/1/2023 0:00 7/1/2023 0… 3038 39.9 -75.2 ## 5 677304444 27 7/1/2023 0:01 7/1/2023 0… 3060 40.0 -75.2 ## 6 677303703 9 7/1/2023 0:01 7/1/2023 0… 3158 39.9 -75.2 ## # ℹ 8 more variables: end_station \u0026lt;dbl\u0026gt;, end_lat \u0026lt;dbl\u0026gt;, end_lon \u0026lt;dbl\u0026gt;, ## # bike_id \u0026lt;dbl\u0026gt;, plan_duration \u0026lt;dbl\u0026gt;, trip_route_category \u0026lt;chr\u0026gt;, ## # passholder_type \u0026lt;chr\u0026gt;, bike_type \u0026lt;chr\u0026gt; dim(bike) ## [1] 351189 15 Suppose we were interested in the following research questions.\n Does the trip route, type of pass, and bike type explain variation in the duration the bike was rented?  Let’s explore this descriptively.\ngf_density(~ duration, data = bike) |\u0026gt; gf_labs(x = \u0026quot;Duration, in minutes\u0026quot;) gf_violin(bike_type ~ duration, data = bike, fill = \u0026#39;gray85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_labs(x = \u0026quot;Duration, in minutes\u0026quot;, y = \u0026quot;\u0026quot;)  gf_violin(passholder_type ~ duration, data = bike, fill = \u0026#39;gray85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_labs(x = \u0026quot;Duration, in minutes\u0026quot;, y = \u0026quot;\u0026quot;)  gf_violin(trip_route_category ~ duration, data = bike, fill = \u0026#39;gray85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_labs(x = \u0026quot;Duration, in minutes\u0026quot;, y = \u0026quot;\u0026quot;)  Regression model with multiple categorical predictors Similar to multiple linear regression, including multiple categorical predictors is similar to that case. The simplest model is the additive model, also commonly referred to as the main effect model. Let’s start with two categorical attributes that take on two different values.\n\\[ duration = \\beta_{0} + \\beta_{1} bike\\_type + \\beta_{2} trip\\_route\\_category + \\epsilon \\]\nbike_lm \u0026lt;- lm(duration ~ bike_type + trip_route_category, data = bike) broom::glance(bike_lm) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0214 0.0214 13.6 3832. 0 2 -1414715. 2.83e6 2.83e6 ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; broom::tidy(bike_lm) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 12.5 0.032 393. 0 ## 2 bike_typestandard 2.49 0.046 54.1 0 ## 3 trip_route_categoryRound Trip 6.00 0.088 68.4 0 two_way_predict \u0026lt;- broom::augment(bike_lm) |\u0026gt; distinct(bike_type, trip_route_category, .fitted) two_way_predict ## # A tibble: 4 × 3 ## bike_type trip_route_category .fitted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 electric One Way 12.5 ## 2 standard One Way 15.0 ## 3 electric Round Trip 18.5 ## 4 standard Round Trip 21.0 mean_duration \u0026lt;- df_stats(duration ~ bike_type + trip_route_category, data =bike, mean, length) mean_duration ## response bike_type trip_route_category mean length ## 1 duration electric One Way 12.62121 176083 ## 2 duration standard One Way 14.88991 149160 ## 3 duration electric Round Trip 17.21428 13674 ## 4 duration standard Round Trip 22.46178 12272 gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = two_way_predict, size = 5) |\u0026gt; gf_line(size = 1.5, group = ~ trip_route_category) |\u0026gt; gf_labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Model Predicted Values\u0026quot;, color = \u0026#39;Trip Route\u0026#39;) |\u0026gt; gf_point(mean ~ bike_type, color = ~ trip_route_category, data = mean_duration, shape = 15, size = 5) Back to the coefficients to really understand what these are.\nbroom::tidy(bike_lm) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 12.5 0.032 393. 0 ## 2 bike_typestandard 2.49 0.046 54.1 0 ## 3 trip_route_categoryRound Trip 6.00 0.088 68.4 0  Intercept: The average bike rental duration for the reference group (electric bikes and one-way trips). bike_typestandard: This is like a slope, so for a one unit change in bike type (ie, moving from an electric bike to a standard bike), the estimated mean change in bike duration decreased by 0.375 minutes, holding other attributes constant. trip_route_categoryRound Trip: This is again like a slope, for a one unit change in trip route (ie, moving from a one-way trip to a round trip), the estimated mean change in bike duraction increased by 7.246 minutes, holder other attributes constant.  These are like weighted marginal means, where the weighting is occuring due to different sample sizes among the groups.\ncount(bike, bike_type, trip_route_category) ## # A tibble: 4 × 3 ## bike_type trip_route_category n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 electric One Way 176083 ## 2 electric Round Trip 13674 ## 3 standard One Way 149160 ## 4 standard Round Trip 12272 df_stats(duration ~ bike_type + trip_route_category, data = bike, mean) |\u0026gt; pivot_wider(names_from = \u0026#39;trip_route_category\u0026#39;, values_from = \u0026quot;mean\u0026quot;) ## # A tibble: 2 × 4 ## response bike_type `One Way` `Round Trip` ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 duration electric 12.6 17.2 ## 2 duration standard 14.9 22.5 Interaction Model The interaction model expands on the main effect only model, by allowing the effect of one category to differ based on elements of the other category. One way to think about this in the case with two categorical attributes, is that the mean change differs based on levels of the second attribute. This model, in the case where the attributes are each two groups, adds one additional term.\n\\[ duration = \\beta_{0} + \\beta_{1} bike\\_type + \\beta_{2} trip\\_route\\_category + \\beta_{3} bike\\_type:trip\\_route\\_category + \\epsilon \\]\nbike_lm_int \u0026lt;- lm(duration ~ bike_type + trip_route_category + bike_type:trip_route_category, data = bike) broom::glance(bike_lm_int) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0222 0.0221 13.6 2653. 0 3 -1414571. 2.83e6 2.83e6 ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; broom::tidy(bike_lm_int) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 12.6 0.032 390. 0 ## 2 bike_typestandard 2.27 0.048 47.5 0 ## 3 trip_route_categoryRound Trip 4.59 0.121 38.1 0 ## 4 bike_typestandard:trip_route_categoryRou… 2.98 0.176 17.0 0 two_way_predict_int \u0026lt;- broom::augment(bike_lm_int) |\u0026gt; distinct(bike_type, trip_route_category, .fitted) two_way_predict_int ## # A tibble: 4 × 3 ## bike_type trip_route_category .fitted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 electric One Way 12.6 ## 2 standard One Way 14.9 ## 3 electric Round Trip 17.2 ## 4 standard Round Trip 22.5 gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = two_way_predict_int, size = 5) |\u0026gt; gf_line(size = 1.5, group = ~ trip_route_category) |\u0026gt; gf_labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Model Predicted Values\u0026quot;, color = \u0026#39;Trip Route\u0026#39;) |\u0026gt; gf_point(mean ~ bike_type, color = ~ trip_route_category, data = mean_duration, shape = 15, size = 5) Back to the coefficients and what do these mean here.\nbroom::tidy(bike_lm_int) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 12.6 0.032 390. 0 ## 2 bike_typestandard 2.27 0.048 47.5 0 ## 3 trip_route_categoryRound Trip 4.59 0.121 38.1 0 ## 4 bike_typestandard:trip_route_categoryRou… 2.98 0.176 17.0 0  Intercept: The average bike rental duration for the reference group (electric bikes and one-way trips). bike_typestandard: This is like a slope, so for a one unit change in bike type (ie, moving from an electric bike to a standard bike), the estimated mean change in bike duration decreased by .05 minutes, holding other attributes constant. trip_route_categoryRound Trip: This is again like a slope, for a one unit change in trip route (ie, moving from a one-way trip to a round trip), the estimated mean change in bike duraction increased by 4.3 minutes, holder other attributes constant. bike_typestandard:trip_route_categoryRound Trip: This is the interaction effect and is the additional mean level change for standard bikes and round trips, holding other attributes constant.  We can get the estimated means for the 4 groups as follows:\n\\[ \\hat{\\mu}_{elec-1way} = 15.2 \\] \\[ \\hat{\\mu}_{elec-RT} = 15.2 + 4.3 \\] \\[ \\hat{\\mu}_{stand-1way} = 15.2 - .05 \\] \\[ \\hat{\\mu}_{stand-RT} = 15.2 - 0.05 + 4.3 + 4.7 \\]\n Example What about the example with two categorical attributes bike type and passholder type? Fit this model below and interpret the parameter estimates, both one without and with interaction effects.\n  Second order (three-way) interaction The more attributes that interact with one another makes the model more complicated and difficult to interpret. Still, let’s try a second order or three way interaction.\nbike_lm_3way \u0026lt;- lm(duration ~ bike_type * trip_route_category * passholder_type, data = bike) broom::glance(bike_lm_3way) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.112 0.112 12.9 4035. 0 11 -1397605. 2.80e6 2.80e6 ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; broom::tidy(bike_lm_3way) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 12 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 27.4 0.12 229. 0 ## 2 bike_typestandard 0.256 0.189 1.35 0.177 ## 3 trip_route_categoryRound Trip 6.74 0.284 23.7 0 ## 4 passholder_typeIndego30 -15.5 0.126 -123. 0 ## 5 passholder_typeIndego365 -17.1 0.136 -125. 0 ## 6 bike_typestandard:trip_route_categoryRo… -0.851 0.46 -1.85 0.064 ## 7 bike_typestandard:passholder_typeIndego… 3.28 0.197 16.6 0 ## 8 bike_typestandard:passholder_typeIndego… 0.718 0.209 3.44 0.001 ## 9 trip_route_categoryRound Trip:passholde… -4.34 0.317 -13.7 0 ## 10 trip_route_categoryRound Trip:passholde… -8.16 0.41 -19.9 0 ## 11 bike_typestandard:trip_route_categoryRo… 5.49 0.502 10.9 0 ## 12 bike_typestandard:trip_route_categoryRo… 4.85 0.619 7.84 0 Interpreting these coefficients can be challenging, visualizing them can be a more effective way to undersand the impact these may have. The following steps will be used to visualize these model results:\nGenerate model-implied or predicted values for each combination of model values Plot those model implied means  three_way_predict_int \u0026lt;- broom::augment(bike_lm_3way) |\u0026gt; distinct(bike_type, trip_route_category, passholder_type, .fitted) three_way_predict_int ## # A tibble: 12 × 4 ## bike_type trip_route_category passholder_type .fitted ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 electric One Way Indego30 12.0 ## 2 standard One Way Indego30 15.5 ## 3 standard One Way Day Pass 27.7 ## 4 electric One Way Indego365 10.4 ## 5 standard One Way Indego365 11.4 ## 6 electric One Way Day Pass 27.4 ## 7 electric Round Trip Indego30 14.4 ## 8 standard Round Trip Day Pass 33.6 ## 9 standard Round Trip Indego365 13.9 ## 10 standard Round Trip Indego30 22.5 ## 11 electric Round Trip Day Pass 34.2 ## 12 electric Round Trip Indego365 8.96 gf_point(.fitted ~ bike_type, color = ~ trip_route_category, data = three_way_predict_int, size = 5) |\u0026gt; gf_line(size = 1.5, group = ~ trip_route_category) |\u0026gt; gf_facet_wrap(~ passholder_type) |\u0026gt; gf_labs(y = \u0026quot;\u0026quot;, x = \u0026quot;Model Predicted Values\u0026quot;, color = \u0026#39;Trip Route\u0026#39;)   ","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"e3888e1d144adde15372036fc7a8c5bb","permalink":"https://psqf6243.brandonlebeau.org/lectures/12-multiple-categorical/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/lectures/12-multiple-categorical/","section":"lectures","summary":"Add more than one categorical predictor Let’s explore the ability to add more than one categorical predictor to our model. This will then build into thinking through interactions. Let’s load some new data.","tags":null,"title":"Multiple Categorical Predictors","type":"book"},{"authors":null,"categories":null,"content":"Quiz 4 can be taken on ICON, due October 16th, 2023. The quiz covers content from Week 4.\nQuiz 4 Link\n","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"c160bc7b80914ff7c52bbe5d6dff1a09","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz4/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/assignments/quizzes/quiz4/","section":"assignments","summary":"Quiz 4 can be taken on ICON, due October 16th, 2023. The quiz covers content from Week 4.\nQuiz 4 Link","tags":null,"title":"Quiz 4","type":"book"},{"authors":null,"categories":null,"content":"Quiz 5 can be taken on ICON, due October 23rd, 2023. The quiz covers content from Week 6.\nQuiz 5 Link\n","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"cda06e7833bad261249d58564bf0c9c1","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz5/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/assignments/quizzes/quiz5/","section":"assignments","summary":"Quiz 5 can be taken on ICON, due October 23rd, 2023. The quiz covers content from Week 6.\nQuiz 5 Link","tags":null,"title":"Quiz 5","type":"book"},{"authors":null,"categories":null,"content":"  Analysis of Covariance This section of notes will explore an analysis procedure classically known as Analysis of Covariance (ANCOVA). This methodology is aimed to answer the following question, Is there a difference in groups, after controlling for other covariates. Here, covariates refer to another attribute in the data that is thought to influence or explain variation in the outcome, therefore, this method is meant to remove, control, or partition the variance associated with the covariate (or multiple covariates) to evaluate the conditional or adjusted means.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(broom) theme_set(theme_bw(base_size = 18)) baby \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/baby.csv\u0026quot;) %\u0026gt;% mutate(gestational_mean = gestational_days - mean(gestational_days), maternal_weight_mean = maternal_pregnancy_weight - mean(maternal_pregnancy_weight), maternal_age_mean = maternal_age - mean(maternal_age), maternal_height_mean = maternal_height - mean(maternal_height)) ## Rows: 1174 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## dbl (5): birth_weight, gestational_days, maternal_age, maternal_height, mate... ## lgl (1): maternal_smoker ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(baby) ## # A tibble: 6 × 10 ## birth_weight gestational_days maternal_age maternal_height ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 120 284 27 62 ## 2 113 282 33 64 ## 3 128 279 28 64 ## 4 108 282 23 67 ## 5 136 286 25 62 ## 6 138 244 33 62 ## # ℹ 6 more variables: maternal_pregnancy_weight \u0026lt;dbl\u0026gt;, maternal_smoker \u0026lt;lgl\u0026gt;, ## # gestational_mean \u0026lt;dbl\u0026gt;, maternal_weight_mean \u0026lt;dbl\u0026gt;, ## # maternal_age_mean \u0026lt;dbl\u0026gt;, maternal_height_mean \u0026lt;dbl\u0026gt; Let’s first explore the model that only includes gestational days in the model, that is, this is a simple linear regression model. Note: I mean centered gestational days for this model.\n\\[ weight = \\beta_{0} + \\beta_{1} days + \\epsilon \\]\nbaby_lm \u0026lt;- lm(birth_weight ~ gestational_mean, data = baby) glance(baby_lm) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.166 0.165 16.7 233. 3.40e-48 1 -4973. 9953. 9968. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; tidy(baby_lm) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 119. 0.489 244. 0 ## 2 gestational_mean 0.467 0.031 15.3 0 Let’s next explore another simple linear regression model that only includes whether a mother smoked or not. This model is:\n\\[ weight = \\beta_{0} + \\beta_{2} smoker + \\epsilon \\]\nbaby_smoker \u0026lt;- lm(birth_weight ~ maternal_smoker, data = baby) glance(baby_smoker) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.0609 0.0601 17.8 76.0 9.46e-18 1 -5043. 10092. 10107. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; tidy(baby_smoker) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 123. 0.665 185. 0 ## 2 maternal_smokerTRUE -9.27 1.06 -8.72 0 The idea behind ANCOVA is that we know that gestational days is an important predictor in understanding variation in the birth weight of the baby. This may not be a predictor that is of most interest to our research question of knowing whether or not there are differences in birth weight between those that smoked vs did not smoke when the baby was in the womb. There may be some sampling bias occurring, that is, those that smoked may have fewer gestational days compared to those that did not smoke. This fact may drive the differences in the mean difference of birth weight. As a results, we would want to adjust for the effect of gestational days prior to evaluating the mean difference of smoker status.\nBelow is a descriptive figure trying to show the relationship between birth weight and gestational days by smoker status.\ngf_point(birth_weight ~ gestational_days, data = baby, color = ~maternal_smoker, size = 4) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Below, let’s fit the traditional ANCOVA model, that contains a single continuous covariate (gestational days) and the categorical predictor. The model would be:\n\\[ weight = \\beta_{0} + \\beta_{1} days + \\beta_{2} smoker + \\epsilon \\]\nbaby_ancova \u0026lt;- lm(birth_weight ~ gestational_mean + maternal_smoker, data = baby) glance(baby_ancova) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.216 0.214 16.2 161. 1.71e-62 2 -4937. 9883. 9903. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; tidy(baby_ancova) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 123. 0.608 202. 0 ## 2 gestational_mean 0.451 0.03 15.2 0 ## 3 maternal_smokerTRUE -8.37 0.973 -8.60 0 The model above can be partioned to better show the specific terms in the model.\n\\[ weight_{smoker} = (\\beta_{0} + \\beta_{2}) + \\beta_{1} days + \\epsilon \\]\n\\[ weight_{non-smoker} = \\beta_{0} + \\beta_{1} days + \\epsilon \\]\nMore explicitly, the term, \\(\\beta_{2}\\) above reflects a mean level change on the intercept after controlling for the effects of gestational days.\nAdjusted Means Let’s compare the means from this ANCOVA model compared to the unconditional means.\nuncond_mean \u0026lt;- df_stats(birth_weight ~ maternal_smoker, data = baby, mean) %\u0026gt;% mutate(maternal_smoker = as.character(maternal_smoker)) cond_mean \u0026lt;- data.frame(maternal_smoker = c(\u0026#39;FALSE\u0026#39;, \u0026#39;TRUE\u0026#39;), cond_mean = c(122.74, 122.74 - 8.37)) combine_means \u0026lt;- full_join(uncond_mean, cond_mean) ## Joining with `by = join_by(maternal_smoker)` combine_means ## response maternal_smoker mean cond_mean ## 1 birth_weight FALSE 123.0853 122.74 ## 2 birth_weight TRUE 113.8192 114.37 Note, the conditional mean used the fact that the gestational days were mean centered, therefore, a value of 0 was included in the above equations.\n Interaction between continuous and categorical predictor One assumption in classical ANCOVA is to assume that the effect of the adjusting covariate is the same for each group. Although, this is a classical assumption, this is an empirical question that we can explore given the data. This can be explored by the introduction of an interaction.\nThis new model would be:\n\\[ weight = \\beta_{0} + \\beta_{1} days + \\beta_{2} smoker + \\beta_{3} days:smoker + \\epsilon \\]\nwhere the \\(:\\) in the equation above represents the interaction effect between gestational days and smoker status. An interaction is testing if the slope for gestational days is the same for those that smoke vs those that did not smoke.\nMore explicitly, we can partition the model above into the following two regression equations.\n\\[ weight_{smoker} = (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3}) days + \\epsilon \\]\n\\[ weight_{non-smoker} = \\beta_{0} + \\beta_{1} days + \\epsilon \\]\nbaby_lm_int \u0026lt;- lm(birth_weight ~ gestational_mean * maternal_smoker, data = baby) # baby_lm_int \u0026lt;- lm(birth_weight ~ gestational_mean + maternal_smoker + gestational_mean:maternal_smoker, # data = baby) glance(baby_lm_int) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.225 0.223 16.2 113. 2.41e-64 3 -4930. 9871. 9896. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; tidy(baby_lm_int) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 123. 0.605 203. 0 ## 2 gestational_mean 0.37 0.037 10.1 0 ## 3 maternal_smokerTRUE -8.26 0.969 -8.52 0 ## 4 gestational_mean:maternal_smokerTRUE 0.231 0.062 3.74 0 What do the 4 terms represent now?\n\\(\\beta_{0}\\) is the intercept for the reference group (non-smokers) when all other attributes in the model are equal to 0. \\(\\beta_{1}\\) is the slope for the reference group (non-smokers), controlling for the smoker status. \\(\\beta_{2}\\) is the average change for those that smoke, controlling for differences in gestational days. \\(\\beta_{3}\\) is the average slope difference for those that smoke compared to those that did not smoke.  Often, it is easier to combine the terms to be compared directly. For example, given the example above, we could combine \\(\\beta_{0}\\) and \\(\\beta_{2}\\) to get the estimated intercept for those that smoke. Then we could do something similar for the slope terms for those that smoke.\nHypotheses being tested The hypotheses being tested here for the \\(\\beta_{2}\\) and \\(\\beta_{3}\\) are as follows:\n\\[ H_{0}: \\beta_{2} = 0 \\]\nand\n\\[ H_{0}: \\beta_{3} = 0 \\]\nThese can be a little bit untenable, therefore stating them in words can help.\n \\(H_{0}: \\beta_{2} = 0\\), is really testing, is there a mean difference between the reference and focal groups, controlling for the other terms in the model. This is inherently a conditional or adjusted means question. \\(H_{0}: \\beta_{3} = 0\\), is really testing, is there a slope difference comparing the focal and reference groups, controlling for other terms in the model.    Including more than 1 covariate to adjust/condition It is also possible to adjust for more than one covariate. For example, maybe we thought adjusting for the weight, height, and age of the mother would also be a good idea. This can be done to remove any variation associated with that attribute prior to making exploring the mean difference in the attributes of interest.\nThis model would now be:\n\\[ weight = \\beta_{0} + \\beta_{1} days + \\beta_{4} mother\\_weight + \\beta_{5} mother\\_height + \\beta_{6} mother\\_age + \\beta_{2} smoker + \\beta_{3} days:smoker + \\epsilon \\]\nThese can also be partitioned into two separate equations.\n\\[ weight_{smoker} = (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3}) days + \\beta_{4} mother\\_weight + \\beta_{5} mother\\_height + \\beta_{6} mother\\_age + \\epsilon \\]\n\\[ weight_{non-smoker} = \\beta_{0} + \\beta_{1} days + \\beta_{4} mother\\_weight + \\beta_{5} mother\\_height + \\beta_{6} mother\\_age + \\epsilon \\]\nbaby_lm_int_w \u0026lt;- lm(birth_weight ~ gestational_mean + maternal_pregnancy_weight + maternal_height + maternal_age + maternal_smoker + gestational_mean:maternal_smoker, data = baby) glance(baby_lm_int_w) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.260 0.256 15.8 68.3 6.14e-73 6 -4903. 9822. 9863. ## # ℹ 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; tidy(baby_lm_int_w) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 43.5 12.3 3.54 0 ## 2 gestational_mean 0.365 0.036 10.1 0 ## 3 maternal_pregnancy_weight 0.052 0.025 2.06 0.039 ## 4 maternal_height 1.10 0.204 5.41 0 ## 5 maternal_age 0.07 0.081 0.873 0.383 ## 6 maternal_smokerTRUE -8.20 0.952 -8.62 0 ## 7 gestational_mean:maternal_smokerTRUE 0.208 0.061 3.44 0.001 How could the appropriate conditional means be computed here? Also, what happened to the intercept in this model, why is it so small?\n  ","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"ff5e86705c85039a1c11f1b1e7feff48","permalink":"https://psqf6243.brandonlebeau.org/lectures/13-ancova/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/lectures/13-ancova/","section":"lectures","summary":"Analysis of Covariance This section of notes will explore an analysis procedure classically known as Analysis of Covariance (ANCOVA). This methodology is aimed to answer the following question, Is there a difference in groups, after controlling for other covariates.","tags":null,"title":"Analysis of Covariance","type":"book"},{"authors":null,"categories":null,"content":"Quiz 6 can be taken on ICON, due October 30th, 2023. The quiz covers content from Week 7.\nQuiz 6 Link\n","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"b9c34714991f0282a19cd5d3a42d282a","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz6/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/assignments/quizzes/quiz6/","section":"assignments","summary":"Quiz 6 can be taken on ICON, due October 30th, 2023. The quiz covers content from Week 7.\nQuiz 6 Link","tags":null,"title":"Quiz 6","type":"book"},{"authors":null,"categories":null,"content":"Quiz 7 can be taken on ICON, due October 30th, 2023. The quiz covers content from Week 8.\nQuiz 7 Link\n","date":1695859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695859200,"objectID":"ec4ec95d4d7d6867324ec59356db3f84","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz7/","publishdate":"2023-09-28T00:00:00Z","relpermalink":"/assignments/quizzes/quiz7/","section":"assignments","summary":"Quiz 7 can be taken on ICON, due October 30th, 2023. The quiz covers content from Week 8.\nQuiz 7 Link","tags":null,"title":"Quiz 7","type":"book"},{"authors":null,"categories":null,"content":"Quiz 8 can be taken on ICON, due December 11th, 2023. The quiz covers content on interaction and ANCOVA.\nQuiz 8 Link\n","date":1700092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700092800,"objectID":"9a04d1c61f4ca09fe926945674a12b96","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz8/","publishdate":"2023-11-16T00:00:00Z","relpermalink":"/assignments/quizzes/quiz8/","section":"assignments","summary":"Quiz 8 can be taken on ICON, due December 11th, 2023. The quiz covers content on interaction and ANCOVA.\nQuiz 8 Link","tags":null,"title":"Quiz 8","type":"book"},{"authors":null,"categories":null,"content":"  Pre / Post Designs Pre / Post designs are common designs in the social sciences and beyond. These types of designs are such that two measurement occasions are collected for each person in the study. Typically, the pre-measurement occasion occurs before a treatment and then the post-measurement occasions occurs after the treatment has happened. This gives a glimpse to whether the treatment impacts the a specific outcome.\nBelow are a few scenarios that could be studied with a pre / post design.\n Does a new drug reduce blood pressure in those with high blood pressure? Does a yoga regimine reduce pain in those that have fibromyalgia? Does professional development increase knowledge related to acceleration options for students? Does a finance workshop improve financial health and wealth of individuals?  Each of these could be conducted so that two measurement occasions could be used. First, the measurement before the treatment would be collected, then the treatment is given, then the post-measurement could explore if the treatment is effective. This is a type of within study design that is popular because each individual acts as its own control group. They are also easier to implement than longitudinal studies that would collect more than 2 measurement occasions.\nAnalyzing Pre / Post Designs There are more than one way to analyze pre/post designs. These are largely derived from the research questions and also a statistical consideration related to balance. Let’s first think about the research questions.\nThere first class of research questions talk about change. For example, using the scenarios above, how much does the drug lower blood pressure or how much does the financial workshop improve financial health?\nThe second class of research questions talk about what is the average post score after adjusting for pre scores. This is inherently a conditional mean problem whereas the previous example is an unconditional problem. For example, using the scenarios above, do those with the same initial blood pressure have different blood pressure levels at follow-up?\n  Example We are going to use simulated data based on a real study. The study is:\nKathryn Curtis, Anna Osadchuk \u0026amp; Joel Katz (2011) An eight-week yoga intervention is associated with improvements in pain, psychological functioning and mindfulness, and changes in cortisol levels in women with fibromyalgia, Journal of Pain Research, , 189-201, DOI: 10.2147/JPR.S22761\nFor this analysis we are going to focus on pain as a single outcome. The study itself had many different outcomes that they explored the impact of the 8-week yoga intervention. I’m also simplifying the study’s actual design (they collected survey measurements at 3 time points). I am going to focus the data simulation from a self-report measure of 11 items named the Pain Disability Index. This measure quantifies the extent to which an individual’s pain impacts seven daily activities.\nTable 3 of the paper contains the means and standard deviations for the outcome that we will use for the simulation. Most notably, the mean PDI score for the group before the yoga intervention was 38.14 (SD was 17.17) and follow up was 33.81 (14.43). This resulted in a reductino in the self-report measure of pain of 4.33.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(simglm) theme_set(theme_bw(base_size = 16)) sim_args \u0026lt;- list( formula = PDI ~ PDI_pre, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86) ), error = list(variance = 150), sample_size = 50, reg_weights = c(-4.33, -0.5) ) pain_data \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(pain_data) ## X.Intercept. PDI_pre level1_id error fixed_outcome random_effects ## 1 1 26.177713 1 -14.605374 -17.418856 0 ## 2 1 31.860000 2 5.237191 -20.260000 0 ## 3 1 -10.781113 3 19.573814 1.060556 0 ## 4 1 5.887948 4 8.266637 -7.273974 0 ## 5 1 -2.136521 5 9.081863 -3.261739 0 ## 6 1 -3.093359 6 10.952453 -2.783321 0 ## PDI ## 1 -32.0242303 ## 2 -15.0228087 ## 3 20.6343705 ## 4 0.9926633 ## 5 5.8201233 ## 6 8.1691323 gf_point(PDI ~ PDI_pre, data = pain_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;) df_stats(~ PDI, data = pain_data, mean, sd, min, max) ## response mean sd min max ## 1 PDI -4.246456 14.08487 -33.93156 20.63437 df_stats(~ PDI_pre, data = pain_data, mean, sd, min, max) ## response mean sd min max ## 1 PDI_pre 1.941678 15.06807 -25.38113 31.86 cor(PDI ~ PDI_pre, data = pain_data) ## [,1] ## [1,] -0.685983 Two competing models There are two competing models here that are often described in the literature. As discussed above, one is performed on the change scores, the other is done via statistical control. These two models are shown below for the simplest design with pre / post measurement occasions for a single group.\nChange Scores \\[ Post - Pre = \\beta_{0} + \\epsilon \\]\n ANCOVA \\[ Post = \\beta_{0} + \\beta_{1} Pre + \\epsilon \\]\nFor both models, the term of interest is \\(\\beta_{0}\\). This term would represent the average change score for the change score model and would represent the average post score after adjusting for the pre score respectively.\nsummary( lm(I(PDI - PDI_pre) ~ 1, data = pain_data) ) ## ## Call: ## lm(formula = I(PDI - PDI_pre) ~ 1, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -59.603 -19.662 5.908 17.637 50.488 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -6.188 3.786 -1.635 0.109 ## ## Residual standard error: 26.77 on 49 degrees of freedom pain_data \u0026lt;- pain_data |\u0026gt; mutate(change = PDI - PDI_pre) t.test(pain_data$change) ## ## One Sample t-test ## ## data: pain_data$change ## t = -1.6346, df = 49, p-value = 0.1085 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -13.79594 1.41967 ## sample estimates: ## mean of x ## -6.188134 summary( lm(PDI ~ PDI_pre, data = pain_data) ) ## ## Call: ## lm(formula = PDI ~ PDI_pre, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.8861 -6.7831 -0.4253 7.3212 26.1145 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.00141 1.47672 -2.032 0.0477 * ## PDI_pre -0.64122 0.09817 -6.532 3.84e-08 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 10.35 on 48 degrees of freedom ## Multiple R-squared: 0.4706,\tAdjusted R-squared: 0.4595 ## F-statistic: 42.66 on 1 and 48 DF, p-value: 3.844e-08   Iterate both models multiple times These are one instance of simulated data. We can perform this process many times to get a sense as to how much variation there are in these estimates as well as typical values for the parameter of interest. Again, I’m going to use the simglm package to do this automatically. The code is not really that interesting here, unless you are interesting in learning R code. We will focus on the ideas from the output.\nlibrary(future) ## ## Attaching package: \u0026#39;future\u0026#39; ## The following object is masked from \u0026#39;package:mosaic\u0026#39;: ## ## value plan(multisession) sim_args \u0026lt;- list( formula = PDI ~ PDI_pre, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86) ), error = list(variance = 150), sample_size = 50, reg_weights = c(-4.33, -0.5), replications = 1000 ) pain_data_repl \u0026lt;- replicate_simulation(sim_args) change_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(I(PDI - PDI_pre) ~ 1, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;change\u0026#39;) ancova_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(PDI ~ PDI_pre, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;ancova\u0026#39;) combine_data \u0026lt;- dplyr::bind_rows( change_mod, ancova_mod ) combine_data |\u0026gt; dplyr::filter(term == \u0026#39;(Intercept)\u0026#39;) |\u0026gt; dplyr::mutate(term2 = \u0026#39;Intercept\u0026#39;) |\u0026gt; gf_violin(model ~ estimate, fill = \u0026#39;grey85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_refine(scale_x_continuous(\u0026quot;\u0026quot;, breaks = seq(-18, 9, 3))) |\u0026gt; gf_vline(xintercept = 0, linewidth = 2, linetype = \u0026#39;dotted\u0026#39;)   Increase strength of design A rather simple adjustment to the above design (not done in the original study) is to add a control group. The control group would not receive the treatment/intervention, but the same measurements would occur for those groups. This removes the counterfactual of having time pass. For example, adding the control group helps to answer the question about what would occur without any treatment just with the passage of time.\nHaving a control group does not automatically give a causal type research design. The key for those types of considerations is really in how the control group was selected. If it was done via random assignment, then this would be a causal type design due to the balanced nature of the control and treatment/intervention group at baseline or pre measurement. If random assignment was not done, then causal type claims can not be made.\nAnalysis of pre / post design with control group Unlike above when there was no control group, the choice of ANCOVA and change scores can provide different results. Much of the difference is due to whether the control and treatment/intervention group are balanced at baseline. If random assignment was done and the groups are balanced (similar characteristics at baseline), then both ANCOVA and change scores should give similar results. However, if random assignment was not performed, then it is likely that there is sampling bias and the control and treatment/intervention groups would likely be non-equivalent at baseline. This would then need to be adjusted via ANCOVA. For example, if one group has a higher mean at baseline, this could mean they could be more likely to have bigger change, have regression to the mean, or be close to the ceiling for the measure.\nChange Scores \\[ Post - Pre = \\beta_{0} + \\beta_{2} treat + \\epsilon \\]\n ANCOVA \\[ Post = \\beta_{0} + \\beta_{1} Pre + \\beta_{2} treat + \\epsilon \\]\nFor both models, the term of interest is \\(\\beta_{2}\\). This term would represent the average treatment effect.\nsim_args \u0026lt;- list( formula = PDI ~ PDI_pre + treatment, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86), treatment = list(var_type = \u0026#39;factor\u0026#39;, levels = c(\u0026#39;control\u0026#39;, \u0026#39;treatment\u0026#39;)) ), error = list(variance = 150), sample_size = 50, reg_weights = c(0, -0.5, -10) ) pain_data \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(pain_data) ## X.Intercept. PDI_pre treatment_1 treatment level1_id error ## 1 1 12.393182 1 treatment 1 -24.389649 ## 2 1 1.191523 1 treatment 2 1.378228 ## 3 1 4.903155 0 control 3 9.665112 ## 4 1 3.749031 1 treatment 4 4.035373 ## 5 1 -19.631032 0 control 5 -8.365153 ## 6 1 19.844034 1 treatment 6 7.325367 ## fixed_outcome random_effects PDI ## 1 -16.196591 0 -40.586240 ## 2 -10.595762 0 -9.217534 ## 3 -2.451578 0 7.213534 ## 4 -11.874515 0 -7.839142 ## 5 9.815516 0 1.450363 ## 6 -19.922017 0 -12.596650 gf_point(PDI ~ PDI_pre, data = pain_data, color = ~ treatment) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2)  df_stats(PDI ~ treatment, data = pain_data, mean, sd, min, max, length) ## response treatment mean sd min max length ## 1 PDI control -0.3724105 16.50961 -29.58298 31.26927 27 ## 2 PDI treatment -6.9647708 19.21465 -40.99505 23.38336 23 df_stats(PDI_pre ~ treatment, data = pain_data, mean, sd, min, max, length) ## response treatment mean sd min max length ## 1 PDI_pre control 2.846196 15.02157 -27.34416 31.10210 27 ## 2 PDI_pre treatment -2.626947 18.24654 -38.14000 21.23145 23 summary(lm(PDI ~ PDI_pre + treatment, data = pain_data)) ## ## Call: ## lm(formula = PDI ~ PDI_pre + treatment, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.230 -12.332 2.273 8.841 25.926 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.4121 2.8337 0.498 0.6206 ## PDI_pre -0.6270 0.1272 -4.931 1.06e-05 *** ## treatmenttreatment -10.0239 4.2019 -2.386 0.0211 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 14.6 on 47 degrees of freedom ## Multiple R-squared: 0.3635,\tAdjusted R-squared: 0.3365 ## F-statistic: 13.42 on 2 and 47 DF, p-value: 2.447e-05 summary(lm(I(PDI - PDI_pre) ~ treatment, data = pain_data)) ## ## Call: ## lm(formula = I(PDI - PDI_pre) ~ treatment, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.642 -26.591 0.632 23.497 65.861 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.219 5.889 -0.547 0.587 ## treatmenttreatment -1.119 8.683 -0.129 0.898 ## ## Residual standard error: 30.6 on 48 degrees of freedom ## Multiple R-squared: 0.0003461,\tAdjusted R-squared: -0.02048 ## F-statistic: 0.01662 on 1 and 48 DF, p-value: 0.898   Iterate Again, let’s look at any differences by performing the simulation multiple times.\nlibrary(future) plan(multicore) ## Warning in supportsMulticoreAndRStudio(...): [ONE-TIME WARNING] Forked ## processing (\u0026#39;multicore\u0026#39;) is not supported when running R from RStudio because ## it is considered unstable. For more details, how to control forked processing ## or not, and how to silence this warning in future R sessions, see ## ?parallelly::supportsMulticore sim_args \u0026lt;- list( formula = PDI ~ PDI_pre + treatment, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86), treatment = list(var_type = \u0026#39;factor\u0026#39;, levels = c(\u0026#39;control\u0026#39;, \u0026#39;treatment\u0026#39;)) ), error = list(variance = 150), sample_size = 50, reg_weights = c(0, -0.5, -10), replications = 1000 ) pain_data_repl \u0026lt;- replicate_simulation(sim_args) change_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(I(PDI - PDI_pre) ~ 1 + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;change\u0026#39;) ancova_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(PDI ~ PDI_pre + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;ancova\u0026#39;) combine_data \u0026lt;- dplyr::bind_rows( change_mod, ancova_mod ) combine_data |\u0026gt; dplyr::filter(term == \u0026#39;treatment\u0026#39;) |\u0026gt; gf_violin(model ~ estimate, fill = \u0026#39;grey85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_refine(scale_x_continuous(\u0026quot;\u0026quot;, breaks = seq(-40, 20, 5)))  Non-equivalent control groups What happens if the control group is not equivalent to the treatment group? One way this can be simulated is to have the treatment allocation not be random, but instead have the treatment allocation be dependent on the pre test score.\nsim_args \u0026lt;- list( formula = PDI ~ PDI_pre + treatment, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86), treatment = list(var_type = \u0026#39;ordinal\u0026#39;, levels = c(0:1)) ), error = list(variance = 150), sample_size = 50, reg_weights = c(0, -0.5, -10), correlate = list(fixed = data.frame(x = c(\u0026#39;PDI_pre\u0026#39;), y = c(\u0026#39;treatment\u0026#39;), corr = c(0.6))) ) pain_data \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; correlate_variables(sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(pain_data) ## X.Intercept. PDI_pre_old treatment_old level1_id PDI_pre treatment_corr ## 1 1 19.257004 1 1 -19.250228 -0.2036771 ## 2 1 -2.944162 1 2 2.950935 0.1880703 ## 3 1 -23.220403 1 3 23.227174 0.5458516 ## 4 1 7.932409 0 4 -7.939746 0.7959004 ## 5 1 26.999222 1 5 -26.992446 -0.3402913 ## 6 1 9.247233 0 6 -9.254570 0.7726999 ## treatment error fixed_outcome random_effects PDI ## 1 0 -5.685938 9.625114 0 3.939176 ## 2 0 9.306598 -1.475468 0 7.831130 ## 3 1 -7.864505 -21.613587 0 -29.478092 ## 4 1 -2.186711 -6.030127 0 -8.216837 ## 5 0 11.409636 13.496223 0 24.905859 ## 6 1 8.867300 -5.372715 0 3.494585 df_stats(PDI_pre ~ treatment, data = pain_data, mean, median, sd, length) ## response treatment mean median sd length ## 1 PDI_pre 0 -6.514712 -5.738969 14.64270 26 ## 2 PDI_pre 1 2.927690 3.479403 15.52462 24 df_stats(PDI_pre ~ treatment_old, data = pain_data, mean, median, sd, length) ## response treatment_old mean median sd length ## 1 PDI_pre 0 -2.029533 -5.109481 16.22231 24 ## 2 PDI_pre 1 -1.938814 -4.942758 15.44459 26 summary(lm(PDI ~ PDI_pre + treatment, data = pain_data)) ## ## Call: ## lm(formula = PDI ~ PDI_pre + treatment, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.228 -6.277 -0.474 5.854 27.912 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.2652 2.3613 0.959 0.3423 ## PDI_pre -0.5152 0.1099 -4.688 2.39e-05 *** ## treatment -8.7478 3.4096 -2.566 0.0135 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 11.47 on 47 degrees of freedom ## Multiple R-squared: 0.4569,\tAdjusted R-squared: 0.4338 ## F-statistic: 19.77 on 2 and 47 DF, p-value: 5.872e-07 summary(lm(I(PDI - PDI_pre) ~ treatment, data = pain_data)) ## ## Call: ## lm(formula = I(PDI - PDI_pre) ~ treatment, data = pain_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.825 -18.488 -1.543 16.803 52.878 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 12.136 5.002 2.426 0.01905 * ## treatment -23.055 7.219 -3.194 0.00248 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 25.5 on 48 degrees of freedom ## Multiple R-squared: 0.1752,\tAdjusted R-squared: 0.1581 ## F-statistic: 10.2 on 1 and 48 DF, p-value: 0.002482 library(future) plan(multicore) sim_args \u0026lt;- list( formula = PDI ~ PDI_pre + treatment, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86), treatment = list(var_type = \u0026#39;ordinal\u0026#39;, levels = c(0:1)) ), error = list(variance = 150), sample_size = 50, reg_weights = c(0, -0.5, -10), correlate = list(fixed = data.frame(x = c(\u0026#39;PDI_pre\u0026#39;), y = c(\u0026#39;treatment\u0026#39;), corr = c(0.6))), replications = 1000 ) pain_data_repl \u0026lt;- replicate_simulation(sim_args) change_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(I(PDI - PDI_pre) ~ 1 + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;change\u0026#39;) ancova_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(PDI ~ PDI_pre + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;ancova\u0026#39;) combine_data \u0026lt;- dplyr::bind_rows( change_mod, ancova_mod ) combine_data |\u0026gt; dplyr::filter(term == \u0026#39;treatment\u0026#39;) |\u0026gt; gf_violin(model ~ estimate, fill = \u0026#39;grey85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_refine(scale_x_continuous(\u0026quot;\u0026quot;, breaks = seq(-55, 5, 5))) library(future) plan(multicore) sim_args \u0026lt;- list( formula = PDI ~ PDI_pre + treatment, fixed = list( PDI_pre = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 17.17, floor = -38.14, ceiling = 31.86), treatment = list(var_type = \u0026#39;ordinal\u0026#39;, levels = c(0:1)) ), error = list(variance = 150), sample_size = 50, reg_weights = c(0, -0.5, -10), correlate = list(fixed = data.frame(x = c(\u0026#39;PDI_pre\u0026#39;), y = c(\u0026#39;treatment\u0026#39;), corr = c(-0.4))), replications = 1000 ) pain_data_repl \u0026lt;- replicate_simulation(sim_args) change_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(I(PDI - PDI_pre) ~ 1 + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;change\u0026#39;) ancova_mod \u0026lt;- lapply(seq_along(pain_data_repl), function(xx) broom::tidy(lm(PDI ~ PDI_pre + treatment, data = pain_data_repl[[xx]]))) |\u0026gt; dplyr::bind_rows() |\u0026gt; dplyr::mutate(model = \u0026#39;ancova\u0026#39;) combine_data \u0026lt;- dplyr::bind_rows( change_mod, ancova_mod ) combine_data |\u0026gt; dplyr::filter(term == \u0026#39;treatment\u0026#39;) |\u0026gt; gf_violin(model ~ estimate, fill = \u0026#39;grey85\u0026#39;, draw_quantiles = c(0.1, 0.5, 0.9)) |\u0026gt; gf_refine(scale_x_continuous(\u0026quot;\u0026quot;, breaks = seq(-35, 45, 5)))   ","date":1698883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698883200,"objectID":"c4398cf6816071c31afe30b652fe23e0","permalink":"https://psqf6243.brandonlebeau.org/lectures/14-prepost/","publishdate":"2023-11-02T00:00:00Z","relpermalink":"/lectures/14-prepost/","section":"lectures","summary":"Pre / Post Designs Pre / Post designs are common designs in the social sciences and beyond. These types of designs are such that two measurement occasions are collected for each person in the study.","tags":null,"title":"Pre / Post Designs","type":"book"},{"authors":null,"categories":null,"content":"  Polynomial Regression Polynomial regression adds additional flexibility to the linear regression model to model non-linear trends. It should be noted specifically here that the data modeled here are not longitudinal data. These are data that are collected at a single time point, but the form of relationship is non-linear.\nThere are non-linear regression models that can be specified directly. Those are more challenging, difficult to interpret, and difficult to estimate. I also do not personally use those types of methods, therefore, the focus in this section of notes is to show that non-linear trends can be modeled using linear regression and maintain the additivity of the model.\nThe key to modeling non-linear trends using a linear regression model, is to add polynomial terms. For example, suppose we think that believe there is a non-linear trend. The simplest model would look like the following:\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\beta_{2} X^{2} + \\epsilon \\]\nIn this model, the single \\(X\\) attribute/predictor is included. The model includes the linear association, but also includes a quadratic association which would estimate/allow the regression line to be curvilinear rather than straight. Further polynomial terms (i.e., cubic, quartic, etc.) could be added. These terms are challenging to interpret, therefore, often visualizing the relationship helps to identify the effect of the non-linearity.\n\\[ Y = \\beta_{0}^{\\beta_{1} X} + \\epsilon \\]\nExample We will again use simulated data to show the model, but will base this on a real research paper:\nGlomb, T. M., \u0026amp; Welsh, E. T. (2005). Can opposites attract? Personality heterogeneity in supervisor-subordinate dyads as a predictor of subordinate outcomes. Journal of Applied Psychology, 90(4), 749.\nThis study looked to explore “Hypothesis 1: Differences between the supervisor and the subordinate in control traits (with the supervisor being higher) will be related to higher subordinate satisfaction with the supervisor.”\n\\[ satisfaction = \\beta_{0} + \\beta_{1} sup\\_control + \\beta_{2} sub\\_control + \\beta_{3} sup\\_control^2 + \\beta_{4} sub\\_control^2 + \\epsilon \\]\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(simglm) theme_set(theme_bw(base_size = 16)) sim_args \u0026lt;- list( formula = satisfaction ~ 1 + poly(sup_control, degree = 2, raw = TRUE) + poly(sub_control, degree = 2, raw = TRUE), fixed = list( sup_control = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 10.93), sub_control = list(var_type = \u0026#39;continuous\u0026#39;, mean = 0, sd = 9.81) ), error = list(variance = 50), sample_size = 200, reg_weights = c(0, .109, -0.03, -.109, 0) ) satis_data \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(satis_data) ## X.Intercept. sup_control_1 sup_control_2 sub_control_1 sub_control_2 ## 1 1 -13.9617859 194.9314655 -0.2837289 0.08050211 ## 2 1 -8.2984744 68.8646780 7.0317688 49.44577284 ## 3 1 0.7074059 0.5004231 4.8235063 23.26621266 ## 4 1 -16.9107782 285.9744198 9.0577360 82.04258111 ## 5 1 -6.1872259 38.2817643 -24.4958453 600.04643630 ## 6 1 -9.1987041 84.6161580 -3.8108439 14.52253144 ## sup_control sub_control level1_id error fixed_outcome random_effects ## 1 -13.9617859 -0.2837289 1 11.901865 -7.3388522 0 ## 2 -8.2984744 7.0317688 2 1.400131 -3.7369369 0 ## 3 0.7074059 4.8235063 3 3.316370 -0.4636676 0 ## 4 -16.9107782 9.0577360 4 5.516255 -11.4098006 0 ## 5 -6.1872259 -24.4958453 5 4.363578 0.8471866 0 ## 6 -9.1987041 -3.8108439 6 -5.641339 -3.1257615 0 ## satisfaction ## 1 4.563012 ## 2 -2.336806 ## 3 2.852702 ## 4 -5.893546 ## 5 5.210765 ## 6 -8.767101 gf_point(satisfaction ~ sup_control, data = satis_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, linewidth = 2) gf_point(satisfaction ~ sub_control, data = satis_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, linewidth = 2) lm(satisfaction ~ 1 + sup_control + sub_control, data = satis_data) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -3.89 0.582 -6.68 0 ## 2 sup_control 0.174 0.051 3.39 0.001 ## 3 sub_control -0.114 0.057 -2.00 0.046 lm(satisfaction ~ 1 + sup_control + sub_control + I(sup_control^2) + I(sub_control^2) + I(sup_control^3), data = satis_data) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.144 0.676 -0.214 0.831 ## 2 sup_control 0.1 0.071 1.41 0.159 ## 3 sub_control -0.132 0.048 -2.75 0.006 ## 4 I(sup_control^2) -0.026 0.003 -9.41 0 ## 5 I(sub_control^2) -0.004 0.004 -1.10 0.273 ## 6 I(sup_control^3) 0 0 0.623 0.534 tmp \u0026lt;- lm(satisfaction ~ 1 + sup_control + sub_control + I(sup_control^2) + I(sub_control^2), data = satis_data) |\u0026gt; broom::augment() head(tmp) ## # A tibble: 6 × 11 ## satisfaction[,1] sup_control sub_control `I(sup_control^2)` `I(sub_control^2)` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;I\u0026lt;dbl\u0026gt;\u0026gt; \u0026lt;I\u0026lt;dbl\u0026gt;\u0026gt; ## 1 4.56 -14.0 -0.284 195. 0.0805 ## 2 -2.34 -8.30 7.03 68.9 49.4 ## 3 2.85 0.707 4.82 0.500 23.3 ## 4 -5.89 -16.9 9.06 286. 82.0 ## 5 5.21 -6.19 -24.5 38.3 600. ## 6 -8.77 -9.20 -3.81 84.6 14.5 ## # ℹ 6 more variables: .fitted \u0026lt;dbl\u0026gt;, .resid \u0026lt;dbl\u0026gt;, .hat \u0026lt;dbl\u0026gt;, .sigma \u0026lt;dbl\u0026gt;, ## # .cooksd \u0026lt;dbl\u0026gt;, .std.resid \u0026lt;dbl\u0026gt; gf_point(.fitted ~ sub_control, data = tmp, size = 2) |\u0026gt; gf_smooth()  ## `geom_smooth()` using method = \u0026#39;loess\u0026#39;   Spline Models There are also a type of model called spline models that can also help with the flexibility of the linear regression model. The simplest type of spline model are called linear-linear spline models. These types of models are purely linear, but the linear slope does not need to be the same across the entire span. These can be of interest when something occurs at a specific value of an attribute.\nFor example (this example is completely hypothetical), imagine a situation where you are predicting anxiety symptomolgy by the age of the individual. Suppose we are interested in knowing if there are differential associations across different age spans, spline models can aid in this type of estimation.\nsim_args \u0026lt;- list( formula = anxiety ~ 1 + age + age_group_post + age:age_group_post, fixed = list( age = list(var_type = \u0026#39;ordinal\u0026#39;, levels = 20:80) ), post = list(age_group_post = list(variable = \u0026#39;age\u0026#39;, fun = \u0026#39;ifelse\u0026#39;, condition = \u0026#39;\u0026gt;= 50\u0026#39;, yes = 1, no = 0)), error = list(variance = 50), sample_size = 1000, reg_weights = c(5, 0.05, -1, -0.1) ) anxiety_data \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(anxiety_data) ## X.Intercept. age age_group_post age.age_group_post level1_id error ## 1 1 69 1 69 1 10.2760352 ## 2 1 20 0 0 2 -11.3948145 ## 3 1 77 1 77 3 3.0602168 ## 4 1 44 0 0 4 5.2575760 ## 5 1 66 1 66 5 2.2060265 ## 6 1 76 1 76 6 0.6198986 ## fixed_outcome random_effects anxiety ## 1 0.55 0 10.8260352 ## 2 6.00 0 -5.3948145 ## 3 0.15 0 3.2102168 ## 4 7.20 0 12.4575760 ## 5 0.70 0 2.9060265 ## 6 0.20 0 0.8198986 gf_point(anxiety ~ age, data = anxiety_data, size = 2, alpha = 0.5) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) gf_point(anxiety ~ age, data = anxiety_data, color = ~factor(age_group_post), size = 2, alpha = 0.5) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) |\u0026gt; gf_labs(color = \u0026quot;Age Group\u0026quot;) lm(anxiety ~ 1 + age + age_group_post, data = anxiety_data) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 8.04 0.923 8.71 0 ## 2 age -0.029 0.025 -1.16 0.244 ## 3 age_group_post -5.13 0.881 -5.82 0 lm(anxiety ~ 1 + age + age_group_post + age:age_group_post, data = anxiety_data) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 5.53 1.28 4.30 0 ## 2 age 0.043 0.036 1.2 0.23 ## 3 age_group_post 1.79 2.62 0.682 0.495 ## 4 age:age_group_post -0.14 0.05 -2.80 0.005 sim_args \u0026lt;- list( formula = anxiety ~ 1 + age + age_group_post + age:age_group_post, fixed = list( age = list(var_type = \u0026#39;ordinal\u0026#39;, levels = -30:30) ), post = list(age_group_post = list(variable = \u0026#39;age\u0026#39;, fun = \u0026#39;ifelse\u0026#39;, condition = \u0026#39;\u0026gt;= 0\u0026#39;, yes = 1, no = 0)), error = list(variance = 10), sample_size = 1000, reg_weights = c(5, 0.05, 0, -0.15) ) anxiety_data2 \u0026lt;- simulate_fixed(data = NULL, sim_args) |\u0026gt; simulate_error(sim_args) |\u0026gt; generate_response(sim_args) head(anxiety_data2) ## X.Intercept. age age_group_post age.age_group_post level1_id error ## 1 1 -3 0 0 1 -2.45298470 ## 2 1 23 1 23 2 -2.57248631 ## 3 1 9 1 9 3 -2.14147572 ## 4 1 18 1 18 4 -2.01237008 ## 5 1 12 1 12 5 0.00368541 ## 6 1 5 1 5 6 -4.33236128 ## fixed_outcome random_effects anxiety ## 1 4.85 0 2.3970153 ## 2 2.70 0 0.1275137 ## 3 4.10 0 1.9585243 ## 4 3.20 0 1.1876299 ## 5 3.80 0 3.8036854 ## 6 4.50 0 0.1676387 gf_point(anxiety ~ age, data = anxiety_data2, size = 2, alpha = 0.5) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) gf_point(anxiety ~ age, data = anxiety_data2, color = ~factor(age_group_post), size = 2, alpha = 0.5) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2) |\u0026gt; gf_labs(color = \u0026quot;Age Group\u0026quot;) lm(anxiety ~ 1 + age + age_group_post, data = anxiety_data2) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 3.91 0.232 16.8 0 ## 2 age -0.034 0.012 -2.91 0.004 ## 3 age_group_post 0.021 0.415 0.05 0.96 lm(anxiety ~ 1 + age + age_group_post + age:age_group_post, data = anxiety_data2) |\u0026gt; broom::tidy()|\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 5.13 0.295 17.4 0 ## 2 age 0.043 0.016 2.6 0.009 ## 3 age_group_post -0.11 0.407 -0.271 0.786 ## 4 age:age_group_post -0.148 0.023 -6.47 0 lm(anxiety ~ 1 + age + I(age^2), data = anxiety_data2) |\u0026gt; broom::tidy() |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 4.63 0.151 30.6 0 ## 2 age -0.033 0.006 -6.03 0 ## 3 I(age^2) -0.002 0 -6.21 0 gf_point(anxiety ~ age, data = anxiety_data2) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, linewidth = 2)  ","date":1698883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698883200,"objectID":"4caa22fe4be3bed557720fbc55de4484","permalink":"https://psqf6243.brandonlebeau.org/lectures/15-non-linear/","publishdate":"2023-11-02T00:00:00Z","relpermalink":"/lectures/15-non-linear/","section":"lectures","summary":"Polynomial Regression Polynomial regression adds additional flexibility to the linear regression model to model non-linear trends. It should be noted specifically here that the data modeled here are not longitudinal data.","tags":null,"title":"Non-Linear Trends","type":"book"},{"authors":null,"categories":null,"content":"  Chi-Square Tests There are a total of 2 different chi-square tests for categorical attributes.\nChi-square goodness of fit - Useful for 1 categorical attribute Chi-square test of independence - Useful for 2 categorical attributes  Let’s explore some data related to the General Social Survey.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors head(gss_cat) ## # A tibble: 6 × 9 ## year marital age race rincome partyid relig denom tvhours ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near r… Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str re… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independent Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near r… Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str de… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong dem… Prot… Sout… NA Chi-square goodness of fit Suppose we were interested in knowing if the proportion of people from the GSS data are similar to the political affiliation currently in the United States. This link gives the breakdown of the current political affiliation in the United States according to a continuously running Gallup poll.\nThe chi-square goodness of fit explores the extent to which a categorical attribute follows a specified distribution. The chi-square goodness of fit test is often framed as a way to evaluate if the sample data is representative of the population. As such, the following hypotheses would be a way to frame this analysis.\n\\[ H_{0}:\\ All\\ categories\\ follow\\ the\\ population\\ distribution \\]\nor\n\\[ H_{0}:\\ All\\ categories\\ follow\\ the\\ specified\\ percentages\\ or\\ proportions \\]\nTo frame this analysis in the context of the current data, let’s look at the partyid attribute in the data.\ncount(gss_cat, partyid) %\u0026gt;% mutate(prop = n / sum(n)) ## # A tibble: 10 × 3 ## partyid n prop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 No answer 154 0.00717 ## 2 Don\u0026#39;t know 1 0.0000465 ## 3 Other party 393 0.0183 ## 4 Strong republican 2314 0.108 ## 5 Not str republican 3032 0.141 ## 6 Ind,near rep 1791 0.0834 ## 7 Independent 4119 0.192 ## 8 Ind,near dem 2499 0.116 ## 9 Not str democrat 3690 0.172 ## 10 Strong democrat 3490 0.162 For this example, I’m going to collapse categories and assign a few to NA to omit from the analysis. The following code does this task and then returns a similar table to that shown above. In particular, since the Gallup poll only includes asks about republicans, independents, or democrats, I’m going to group others into an “other” category.\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(partyid_collapse = fct_collapse(partyid, other = c(\u0026quot;No answer\u0026quot;, \u0026quot;Don\u0026#39;t know\u0026quot;, \u0026quot;Other party\u0026quot;), rep = c(\u0026quot;Strong republican\u0026quot;, \u0026quot;Not str republican\u0026quot;), ind = c(\u0026quot;Ind,near rep\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Ind,near dem\u0026quot;), dem = c(\u0026quot;Not str democrat\u0026quot;, \u0026quot;Strong democrat\u0026quot;) )) count(gss_cat, partyid_collapse) %\u0026gt;% mutate(prop = n / sum(n)) ## # A tibble: 4 × 3 ## partyid_collapse n prop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 other 548 0.0255 ## 2 rep 5346 0.249 ## 3 ind 8409 0.391 ## 4 dem 7180 0.334 Chi-Square GoF Mechanics The chi-square goodness of fit compares the observed cell counts with the expected cell counts. More formally, the chi-square test statistic is as follows:\n\\[ \\chi^2 = \\sum \\frac{( O - E ) ^ 2}{E} \\]\nwhere \\(O\\) is the observed cell counts and \\(E\\) are the expected cell counts. The expected cell counts are defined as the sample size times the hypothesized proportions/percentages (this is not completely statistically accurate, however, in many social science situations, this should be sufficient). For example:\n\\[ E = p_{H_{0}} * N \\]\nwhere \\(p_{H_{0}}\\) is the hypothesized proportions from the null hypothesis. The \\(\\chi^2\\) statistic follows a chi-square distribution with \\(k - 1\\) degrees of freedom, where \\(k\\) is the number of categories.\nUsing the table above, these can be computed from the data and assuming the following as population proportions/percentages from the Gallup poll: Rep = 27%, Ind = 45%, Dem = 27%, other = 1%.\nchi_tab \u0026lt;- count(gss_cat, partyid_collapse) %\u0026gt;% mutate(prop = n / sum(n), prop_h0 = c(.01, .27, .45, .27), E = prop_h0 * sum(n)) chi_tab ## # A tibble: 4 × 5 ## partyid_collapse n prop prop_h0 E ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 other 548 0.0255 0.01 215. ## 2 rep 5346 0.249 0.27 5800. ## 3 ind 8409 0.391 0.45 9667. ## 4 dem 7180 0.334 0.27 5800. The \\(\\chi^2\\) statistic can be computed manually.\nchi_tab %\u0026gt;% mutate(num = (n - E)^2, chi_cell = num / E) %\u0026gt;% summarise(chi_square = sum(chi_cell)) ## # A tibble: 1 × 1 ## chi_square ## \u0026lt;dbl\u0026gt; ## 1 1044. More readily, using the chisq.test() function in R is easier. This function takes one primary argument, the attribute to do the chi-square goodness of fit test on. Optionally, the specific proportions need to be passed as well, which is typically desired unless equal percentages/proportions are desired.\nxsq_got \u0026lt;- chisq.test(table(gss_cat$partyid_collapse), p = c(.01, .27, .45, .27)) xsq_got ## ## Chi-squared test for given probabilities ## ## data: table(gss_cat$partyid_collapse) ## X-squared = 1044.2, df = 3, p-value \u0026lt; 2.2e-16  Explore Differences It is often of interest to explore differences, particularly if the chi-square goodness of fit test has a small p-value. This would indicate that the counts likely do not follow the assumed distribution, but where are the differences found? Residuals can help with this. The residuals are the difference in the observed and expected values divided by the square root of the expected values.\n\\[ \\chi^2_{resid} = \\frac{(O - E)}{\\sqrt{E}} \\]\nThese can be extracted directly from the model object saved when running the chi-square test.\nxsq_got$residuals ## ## other rep ind dem ## 22.730994 -5.966485 -12.798166 18.114264   Chi-square Test of Independence The chi-square test of independence is similar to that of the goodness of fit test, except now instead of a single attribute of interest, there are now more than one categorical attribute to be explored. The test of independence explores if the observed attributes are independent from one another. That is, if the two categorical attributes are indpendent, this would assume that the two attributes are proportionally distributed across all categories. The form of the \\(\\chi^2\\) test is the same as the GoT test:\n\\[ \\chi^2 = \\sum \\frac{( O - E ) ^ 2}{E} \\]\nHowever, different from the goodness of fit test, the expected values are computed differently. The expected cell counts are now defined as:\n\\[ E = N * p_{r} * p_{c} \\]\nwhere \\(p_{r}\\) is the margin proportion for the rows, ignoring the columns (that is, marginal row proportion) and \\(p_{c}\\) is the margin proportion for the columns, ignoring the rows. Finally, the test has degrees of freedom equal to \\((r - 1)(c - 1)\\).\nData Example To explore this example, let’s see if the political party affiliation differs (or is associated with) across years that the GSS data were collected. The data are collected over 14 years, collected every other year.\ncount(gss_cat, year) ## # A tibble: 8 × 2 ## year n ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 2000 2817 ## 2 2002 2765 ## 3 2004 2812 ## 4 2006 4510 ## 5 2008 2023 ## 6 2010 2044 ## 7 2012 1974 ## 8 2014 2538 Suppose we were interested in exploring if there was a difference in political affiliation before and after 2010.\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(year_2 = ifelse(year \u0026lt; 2010, \u0026quot;2000 to 2008\u0026quot;, \u0026quot;2010 to 2014\u0026quot;)) addmargins(table(gss_cat$year_2, gss_cat$partyid_collapse)) ## ## other rep ind dem Sum ## 2000 to 2008 327 3906 5734 4960 14927 ## 2010 to 2014 221 1440 2675 2220 6556 ## Sum 548 5346 8409 7180 21483 Giving this table, a single expected value could be computed manually.\n\\[ E_{1,1} = 21483 * (14927 / 21483) * (548 / 21483) = 380.766 \\]\n21483 * (14927 / 21483) * (548 / 21483) ## [1] 380.766 These could be computed for subsequent cell expected values, but these can be extracted directly when fitting the chi-square using the chisq.test() function.\nxsq_ind \u0026lt;- chisq.test(table(gss_cat$year_2, gss_cat$partyid_collapse)) xsq_ind$expected ## ## other rep ind dem ## 2000 to 2008 380.766 3714.553 5842.813 4988.868 ## 2010 to 2014 167.234 1631.447 2566.187 2191.132 xsq_ind ## ## Pearson\u0026#39;s Chi-squared test ## ## data: table(gss_cat$year_2, gss_cat$partyid_collapse) ## X-squared = 64.399, df = 3, p-value = 6.745e-14 xsq_ind$residuals ## ## other rep ind dem ## 2000 to 2008 -2.7553619 3.1411979 -1.4235351 -0.4087162 ## 2010 to 2014 4.1576263 -4.7398226 2.1480035 0.6167208   Effect Sizes for chi-square tests Effect sizes for chi-square tests can be important, particularly with large sample sizes as the chi-square can be highly sensitive to large sample sizes. In particular, very small differences can be found with small samples sizes.\nFor the goodness of fit test, \\(Cohens\\ W\\) can be estimated as an effect size measure. This is computed as:\n\\[ Cohens\\ W = \\sqrt{\\frac{\\chi^2}{N}} \\]\nFor the test of independence, Cramer’s V can be used.\n\\[ V = \\sqrt{\\frac{\\chi^2}{N * df^{*}}} \\] where \\(df^{*}\\) is the smallest of \\(r - 1\\) or \\(c - 1\\).\nCramers V ranges from 0 to 1, where values closer to 1 indicate more variation is explained (ie, the attributes are not independent).\nCohen’s W is similar to Cramers V, but it is not limited to range between 0 and 1.\nsqrt(693.4 / sum(chi_tab$n)) ## [1] 0.1796571 sqrt(64.399 / 21483 * 1) ## [1] 0.05475101 The DescTools package can be used for Cramer’s V as well.\n#install.packages(\u0026quot;DescTools\u0026quot;) library(DescTools) CramerV(table(gss_cat$year_2, gss_cat$partyid_collapse)) ## [1] 0.05475087   ","date":1700092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700092800,"objectID":"d0e1a074dc0f0ccc4d1816fdc06bfc83","permalink":"https://psqf6243.brandonlebeau.org/lectures/16-chi-square/","publishdate":"2023-11-16T00:00:00Z","relpermalink":"/lectures/16-chi-square/","section":"lectures","summary":"Chi-Square Tests There are a total of 2 different chi-square tests for categorical attributes.\nChi-square goodness of fit - Useful for 1 categorical attribute Chi-square test of independence - Useful for 2 categorical attributes  Let’s explore some data related to the General Social Survey.","tags":null,"title":"Chi-Square Tests","type":"book"},{"authors":null,"categories":null,"content":"  Logistic Regression Building off of the previous set of notes that looked for associations between categorical attributes using the chi-square test. This final section of the course will discuss logistic regression. Logistic regression is largely a generalization of linear regression, except instead of the outcome being continuous, for logistic regression, the outcome is dichotomous.\nWe are going to use the General Social Survey again to explore this model.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors head(gss_cat) ## # A tibble: 6 × 9 ## year marital age race rincome partyid relig denom tvhours ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near r… Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str re… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independent Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near r… Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str de… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong dem… Prot… Sout… NA The general framework of the model is as follows:\n\\[ log\\left(\\frac{P(Y = 1)}{1 - P(Y = 1)}\\right) = \\beta_{0} + \\beta_{1} X + \\beta_{k} X_{k} \\]\nThe left-hand side of the equation above is read as: take the log (natural log) of the probability of the data being equal to 1 compared to the data being equal to 0. The left hand side is typically referred to as a logit.\nThe right hand side is similar to linear regression, representing the attributes that are thought to be associated with the likelihood of the data being equal to 1. By default, these are on the logistic metric and are interepreted just like linear regression coefficients on the logistic metric. The non-linearity is done through the log transformation which keeps the probabilities between 0 and 1 inclusive.\nThe model also does not predict a specific value, instead it predicts the probability or likelihood of being a 1 based on the values of the attributes.\ngss_cat \u0026lt;- gss_cat %\u0026gt;% mutate(partyid_collapse = fct_collapse(partyid, other = c(\u0026quot;No answer\u0026quot;, \u0026quot;Don\u0026#39;t know\u0026quot;, \u0026quot;Other party\u0026quot;), rep = c(\u0026quot;Strong republican\u0026quot;, \u0026quot;Not str republican\u0026quot;), ind = c(\u0026quot;Ind,near rep\u0026quot;, \u0026quot;Independent\u0026quot;, \u0026quot;Ind,near dem\u0026quot;), dem = c(\u0026quot;Not str democrat\u0026quot;, \u0026quot;Strong democrat\u0026quot;) ), ind_binary = ifelse(partyid_collapse == \u0026#39;ind\u0026#39;, 1, 0) ) |\u0026gt; filter(partyid_collapse != \u0026#39;other\u0026#39;) count(gss_cat, partyid_collapse) %\u0026gt;% mutate(prop = n / sum(n)) ## # A tibble: 3 × 3 ## partyid_collapse n prop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 rep 5346 0.255 ## 2 ind 8409 0.402 ## 3 dem 7180 0.343 Continuous Attribute Let’s explore and see if the individual’s age helps predict if they are an independent political affiliation.\ncount(gss_cat, ind_binary, partyid_collapse) ## # A tibble: 3 × 3 ## ind_binary partyid_collapse n ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 0 rep 5346 ## 2 0 dem 7180 ## 3 1 ind 8409 tv_ind \u0026lt;- glm(ind_binary ~ I(age - 30), data = gss_cat, family = \u0026quot;binomial\u0026quot;) broom::tidy(tv_ind) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.148 0.02 -7.50 0 ## 2 I(age - 30) -0.015 0.001 -17.8 0 broom::glance(tv_ind) ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 28124. 20870 -13899. 27803. 27819. 27799. 20869 20871 The interpretation for the intercept and slope are similar to before. The intercept says that when age is 30 (notice how I centered the term at age 30 above), the model implied logit is -0.148. The slope says for every unit increase in age, the logit decreases by -0.0149 units. These by default are difficult to interpret as we typically don’t think in logit metrics.\nFor continuous predictors, interpreting these on a probability scale is often helpful. The easiest way to do this is with the fitted function. Before doing that however, it is possible to compute the probability for the intercept (age 30).\n\\[ prob = \\frac{1}{(1 + exp(-\\hat{\\beta}_{0} + \\hat{\\beta}_{1}Age)} \\]\nround( 1 / ( 1 + exp(0.148)), 3) ## [1] 0.463 Different Age Values new_age = data.frame( age = 18:89 ) new_age \u0026lt;- new_age |\u0026gt; mutate(prob = predict(tv_ind, newdata = new_age, type = \u0026#39;response\u0026#39;)) head(new_age, n = 15) ## age prob ## 1 18 0.5078715 ## 2 19 0.5041365 ## 3 20 0.5004009 ## 4 21 0.4966654 ## 5 22 0.4929302 ## 6 23 0.4891958 ## 7 24 0.4854625 ## 8 25 0.4817310 ## 9 26 0.4780014 ## 10 27 0.4742743 ## 11 28 0.4705501 ## 12 29 0.4668291 ## 13 30 0.4631119 ## 14 31 0.4593987 ## 15 32 0.4556900 library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) gf_line(prob ~ age, data = new_age, linewidth = 2) |\u0026gt; gf_labs(x = \u0026quot;Age\u0026quot;, y = \u0026#39;Probability\u0026#39;) new_age = data.frame( age = -1000:1000 ) new_age \u0026lt;- new_age |\u0026gt; mutate(prob = predict(tv_ind, newdata = new_age, type = \u0026#39;response\u0026#39;)) gf_line(prob ~ age, data = new_age, linewidth = 2) |\u0026gt; gf_labs(x = \u0026quot;Age\u0026quot;, y = \u0026#39;Probability\u0026#39;)   Categorical Predictor count(gss_cat, relig) ## # A tibble: 15 × 2 ## relig n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 No answer 60 ## 2 Don\u0026#39;t know 12 ## 3 Inter-nondenominational 107 ## 4 Native american 23 ## 5 Christian 662 ## 6 Orthodox-christian 94 ## 7 Moslem/islam 98 ## 8 Other eastern 32 ## 9 Hinduism 69 ## 10 Buddhism 137 ## 11 Other 206 ## 12 None 3389 ## 13 Jewish 379 ## 14 Catholic 5033 ## 15 Protestant 10634 gss_cat_r \u0026lt;- gss_cat |\u0026gt; filter(!relig %in% c(\u0026#39;No answer\u0026#39;, \u0026quot;Don\u0026#39;t know\u0026quot;)) nrow(gss_cat) ## [1] 20935 nrow(gss_cat_r) ## [1] 20863 count(gss_cat_r, relig) ## # A tibble: 13 × 2 ## relig n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Inter-nondenominational 107 ## 2 Native american 23 ## 3 Christian 662 ## 4 Orthodox-christian 94 ## 5 Moslem/islam 98 ## 6 Other eastern 32 ## 7 Hinduism 69 ## 8 Buddhism 137 ## 9 Other 206 ## 10 None 3389 ## 11 Jewish 379 ## 12 Catholic 5033 ## 13 Protestant 10634 relig_ind \u0026lt;- glm(ind_binary ~ relig, data = gss_cat_r, family = \u0026#39;binomial\u0026#39;) broom::tidy(relig_ind) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 13 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.131 0.194 -0.676 0.499 ## 2 religNative american 0.044 0.46 0.096 0.924 ## 3 religChristian -0.161 0.209 -0.77 0.441 ## 4 religOrthodox-christian -0.126 0.284 -0.442 0.658 ## 5 religMoslem/islam 0.049 0.28 0.176 0.86 ## 6 religOther eastern 0.919 0.428 2.15 0.032 ## 7 religHinduism 0.453 0.311 1.45 0.146 ## 8 religBuddhism 0.351 0.259 1.36 0.176 ## 9 religOther 0.484 0.24 2.02 0.044 ## 10 religNone 0.346 0.197 1.76 0.079 ## 11 religJewish -0.776 0.225 -3.46 0.001 ## 12 religCatholic -0.232 0.196 -1.18 0.237 ## 13 religProtestant -0.53 0.195 -2.72 0.007 broom::tidy(relig_ind) |\u0026gt; mutate_if(is.double, round, 3) |\u0026gt; mutate(odds = exp(estimate)) ## # A tibble: 13 × 6 ## term estimate std.error statistic p.value odds ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.131 0.194 -0.676 0.499 0.877 ## 2 religNative american 0.044 0.46 0.096 0.924 1.04 ## 3 religChristian -0.161 0.209 -0.77 0.441 0.851 ## 4 religOrthodox-christian -0.126 0.284 -0.442 0.658 0.882 ## 5 religMoslem/islam 0.049 0.28 0.176 0.86 1.05 ## 6 religOther eastern 0.919 0.428 2.15 0.032 2.51 ## 7 religHinduism 0.453 0.311 1.45 0.146 1.57 ## 8 religBuddhism 0.351 0.259 1.36 0.176 1.42 ## 9 religOther 0.484 0.24 2.02 0.044 1.62 ## 10 religNone 0.346 0.197 1.76 0.079 1.41 ## 11 religJewish -0.776 0.225 -3.46 0.001 0.460 ## 12 religCatholic -0.232 0.196 -1.18 0.237 0.793 ## 13 religProtestant -0.53 0.195 -2.72 0.007 0.589 gss_cat_r ## # A tibble: 20,863 × 11 ## year marital age race rincome partyid relig denom tvhours ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 2000 Never married 26 White $8000 to 9999 Ind,near … Prot… Sout… 12 ## 2 2000 Divorced 48 White $8000 to 9999 Not str r… Prot… Bapt… NA ## 3 2000 Widowed 67 White Not applicable Independe… Prot… No d… 2 ## 4 2000 Never married 39 White Not applicable Ind,near … Orth… Not … 4 ## 5 2000 Divorced 25 White Not applicable Not str d… None Not … 1 ## 6 2000 Married 25 White $20000 - 24999 Strong de… Prot… Sout… NA ## 7 2000 Never married 36 White $25000 or more Not str r… Chri… Not … 3 ## 8 2000 Divorced 44 White $7000 to 7999 Ind,near … Prot… Luth… NA ## 9 2000 Married 44 White $25000 or more Not str d… Prot… Other 0 ## 10 2000 Married 47 White $25000 or more Strong re… Prot… Sout… 3 ## # ℹ 20,853 more rows ## # ℹ 2 more variables: partyid_collapse \u0026lt;fct\u0026gt;, ind_binary \u0026lt;dbl\u0026gt; levels(gss_cat_r$relig) ## [1] \u0026quot;No answer\u0026quot; \u0026quot;Don\u0026#39;t know\u0026quot; ## [3] \u0026quot;Inter-nondenominational\u0026quot; \u0026quot;Native american\u0026quot; ## [5] \u0026quot;Christian\u0026quot; \u0026quot;Orthodox-christian\u0026quot; ## [7] \u0026quot;Moslem/islam\u0026quot; \u0026quot;Other eastern\u0026quot; ## [9] \u0026quot;Hinduism\u0026quot; \u0026quot;Buddhism\u0026quot; ## [11] \u0026quot;Other\u0026quot; \u0026quot;None\u0026quot; ## [13] \u0026quot;Jewish\u0026quot; \u0026quot;Catholic\u0026quot; ## [15] \u0026quot;Protestant\u0026quot; \u0026quot;Not applicable\u0026quot; unique(gss_cat_r$relig) ## [1] Protestant Orthodox-christian None ## [4] Christian Jewish Catholic ## [7] Other Inter-nondenominational Hinduism ## [10] Native american Buddhism Moslem/islam ## [13] Other eastern ## 16 Levels: No answer Don\u0026#39;t know Inter-nondenominational ... Not applicable new_relig \u0026lt;- data.frame( relig = unique(gss_cat_r$relig) ) new_relig \u0026lt;- new_relig |\u0026gt; mutate(prob = predict(relig_ind, newdata = new_relig, type = \u0026#39;response\u0026#39;) ) new_relig ## relig prob ## 1 Protestant 0.3406056 ## 2 Orthodox-christian 0.4361702 ## 3 None 0.5535556 ## 4 Christian 0.4274924 ## 5 Jewish 0.2875989 ## 6 Catholic 0.4102921 ## 7 Other 0.5873786 ## 8 Inter-nondenominational 0.4672897 ## 9 Hinduism 0.5797101 ## 10 Native american 0.4782609 ## 11 Buddhism 0.5547445 ## 12 Moslem/islam 0.4795918 ## 13 Other eastern 0.6875000 head(gss_cat) ## # A tibble: 6 × 11 ## year marital age race rincome partyid relig denom tvhours partyid_collapse ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; ## 1 2000 Never … 26 White $8000 … Ind,ne… Prot… Sout… 12 ind ## 2 2000 Divorc… 48 White $8000 … Not st… Prot… Bapt… NA rep ## 3 2000 Widowed 67 White Not ap… Indepe… Prot… No d… 2 ind ## 4 2000 Never … 39 White Not ap… Ind,ne… Orth… Not … 4 ind ## 5 2000 Divorc… 25 White Not ap… Not st… None Not … 1 dem ## 6 2000 Married 25 White $20000… Strong… Prot… Sout… NA dem ## # ℹ 1 more variable: ind_binary \u0026lt;dbl\u0026gt; count(gss_cat, marital) ## # A tibble: 6 × 2 ## marital n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 No answer 12 ## 2 Never married 5264 ## 3 Separated 723 ## 4 Divorced 3286 ## 5 Widowed 1765 ## 6 Married 9885 gss_cat_m \u0026lt;- gss_cat |\u0026gt; filter(marital != \u0026#39;No answer\u0026#39;) nrow(gss_cat) - nrow(gss_cat_m) ## [1] 12 count(gss_cat_m, marital) ## # A tibble: 5 × 2 ## marital n ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Never married 5264 ## 2 Separated 723 ## 3 Divorced 3286 ## 4 Widowed 1765 ## 5 Married 9885 age_mod \u0026lt;- glm(ind_binary ~ I(age - 30), data = gss_cat_m, family = \u0026#39;binomial\u0026#39;) broom::tidy(age_mod) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.148 0.0197 -7.51 5.78e-14 ## 2 I(age - 30) -0.0149 0.000839 -17.8 7.48e-71 marital_mod \u0026lt;- glm(ind_binary ~ marital, data = gss_cat_m, family = \u0026#39;binomial\u0026#39;) broom::tidy(marital_mod) |\u0026gt; mutate(odds = exp(estimate)) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 5 × 6 ## term estimate std.error statistic p.value odds ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.172 0.028 -6.22 0 0.842 ## 2 maritalSeparated 0.103 0.079 1.30 0.195 1.11 ## 3 maritalDivorced -0.197 0.045 -4.38 0 0.821 ## 4 maritalWidowed -0.634 0.058 -10.8 0 0.531 ## 5 maritalMarried -0.316 0.035 -9.14 0 0.729 1 / (1 + exp(0.172)) ## [1] 0.4571057 age_mar_mod \u0026lt;- glm(ind_binary ~ I(age - 30) + marital, data = gss_cat_m, family = \u0026quot;binomial\u0026quot;) broom::tidy(age_mar_mod) |\u0026gt; mutate(odds = exp(estimate)) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 6 × 6 ## term estimate std.error statistic p.value odds ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.118 0.028 -4.21 0 0.888 ## 2 I(age - 30) -0.014 0.001 -13.7 0 0.986 ## 3 maritalSeparated 0.262 0.081 3.25 0.001 1.3 ## 4 maritalDivorced 0.046 0.048 0.953 0.341 1.05 ## 5 maritalWidowed -0.108 0.07 -1.54 0.125 0.898 ## 6 maritalMarried -0.114 0.038 -3.03 0.002 0.892 library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum df_stats(age ~ marital, data = gss_cat_m, mean, sd) ## Warning: Excluding 60 rows due to missing data [df_stats()]. ## response marital mean sd ## 1 age Never married 33.95069 13.49365 ## 2 age Separated 45.31761 13.34522 ## 3 age Divorced 51.11686 13.15657 ## 4 age Widowed 71.72541 13.02820 ## 5 age Married 48.70672 15.07430 age_mar_mod_int \u0026lt;- glm(ind_binary ~ I(age - 30) + marital + I(age - 30):marital, data = gss_cat_m, family = \u0026quot;binomial\u0026quot;) broom::tidy(age_mar_mod_int) |\u0026gt; mutate(odds = exp(estimate)) |\u0026gt; mutate_if(is.double, round, 3) ## # A tibble: 10 × 6 ## term estimate std.error statistic p.value odds ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -0.119 0.029 -4.13 0 0.888 ## 2 I(age - 30) -0.014 0.002 -6.51 0 0.986 ## 3 maritalSeparated 0.372 0.118 3.14 0.002 1.45 ## 4 maritalDivorced 0.05 0.073 0.685 0.493 1.05 ## 5 maritalWidowed 0.009 0.169 0.055 0.956 1.01 ## 6 maritalMarried -0.126 0.044 -2.90 0.004 0.881 ## 7 I(age - 30):maritalSeparated -0.007 0.006 -1.22 0.223 0.993 ## 8 I(age - 30):maritalDivorced 0 0.003 -0.116 0.908 1 ## 9 I(age - 30):maritalWidowed -0.003 0.004 -0.703 0.482 0.997 ## 10 I(age - 30):maritalMarried 0 0.003 0.182 0.856 1 AIC(age_mod) ## [1] 27791.78 AIC(marital_mod) ## [1] 28029.35 AIC(age_mar_mod) ## [1] 27762.63 AIC(age_mar_mod_int) ## [1] 27768.23 head(predict(age_mar_mod, type = \u0026#39;response\u0026#39;)) ## 1 2 3 4 5 6 ## 0.4844355 0.4197377 0.3222517 0.4392628 0.4994827 0.4594331 gss_cat_m \u0026lt;- gss_cat_m |\u0026gt; drop_na(ind_binary, age, marital) |\u0026gt; mutate(prob = predict(age_mar_mod, type = \u0026#39;response\u0026#39;)) head(gss_cat_m) ## # A tibble: 6 × 12 ## year marital age race rincome partyid relig denom tvhours partyid_collapse ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; ## 1 2000 Never … 26 White $8000 … Ind,ne… Prot… Sout… 12 ind ## 2 2000 Divorc… 48 White $8000 … Not st… Prot… Bapt… NA rep ## 3 2000 Widowed 67 White Not ap… Indepe… Prot… No d… 2 ind ## 4 2000 Never … 39 White Not ap… Ind,ne… Orth… Not … 4 ind ## 5 2000 Divorc… 25 White Not ap… Not st… None Not … 1 dem ## 6 2000 Married 25 White $20000… Strong… Prot… Sout… NA dem ## # ℹ 2 more variables: ind_binary \u0026lt;dbl\u0026gt;, prob \u0026lt;dbl\u0026gt; gss_cat_m |\u0026gt; distinct(age, marital, prob) ## # A tibble: 342 × 3 ## age marital prob ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 26 Never married 0.484 ## 2 48 Divorced 0.420 ## 3 67 Widowed 0.322 ## 4 39 Never married 0.439 ## 5 25 Divorced 0.499 ## 6 25 Married 0.459 ## 7 36 Never married 0.450 ## 8 44 Divorced 0.433 ## 9 44 Married 0.394 ## 10 47 Married 0.385 ## # ℹ 332 more rows theme_set(theme_bw(base_size = 16)) gss_cat_m |\u0026gt; distinct(age, marital, prob) |\u0026gt; gf_point(prob ~ age, color = ~ marital, size = 4)   ","date":1701648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701648000,"objectID":"425b54b26edeb468c2f479a28c8d443d","permalink":"https://psqf6243.brandonlebeau.org/lectures/17-logistic-regression/","publishdate":"2023-12-04T00:00:00Z","relpermalink":"/lectures/17-logistic-regression/","section":"lectures","summary":"Logistic Regression Building off of the previous set of notes that looked for associations between categorical attributes using the chi-square test. This final section of the course will discuss logistic regression.","tags":null,"title":"Logistic Regression","type":"book"},{"authors":null,"categories":null,"content":"  Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes. One of those attributes will be the outcome or attribute of interest whereas the other will used as a predictor. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable explains variation in the dependent variable (more on this later).\nOf note, variable is a typical word used in statistics, I’ve come to like the word attribute instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.\nWe may write this general model as:\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nWhere \\(Y\\) is the outcome attribute. It is also known as the dependent variable. The \\(X\\) term is the predictor/covariate attribute. It is also known as the independent variable. The \\(\\epsilon\\) is a random error term, more on this later. Finally, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown population coefficients that we are interested in estimating. More on this later too.\nSpecific example The data used for this section of the course is from the 2019 WNBA season. These data are part of the bayesrules package/book. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(ggformula) basketball \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv\u0026quot;) ## Rows: 146 Columns: 32 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): player_name, team ## dbl (29): height, weight, year, age, games_played, games_started, avg_minute... ## lgl (1): starter ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. theme_set(theme_bw(base_size = 18)) head(basketball) ## # A tibble: 6 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, …  Guiding Question Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.\nOne first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.\nThe next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) |\u0026gt; gf_smooth() ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; cor(avg_points ~ total_minutes, data = basketball) |\u0026gt; round(3) ## [1] 0.848 round(cor(avg_points ~ total_minutes, data = basketball), 3) ## [1] 0.848 Questions to consider What can be noticed about the relationship between these two attributes? Does there appear to be a relationship between the two? Is this relationship perfect?    Adding a smoother line Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.\nI’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.\nDoes there appear to be much difference in the relationship across the two lines?\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(linewidth = 2.5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linewidth = 2.5, linetype = 2, color = \u0026#39;green\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; filter(basketball, total_minutes \u0026lt;= 0) ## # A tibble: 1 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Angel McCoughtry 73 173 2019 ATL 32 1 1 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, …  Estimating linear regression coefficients The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is lm() to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.\nThis equation could be written more directly for our specific problem.\n\\[ Avg\\_points = \\beta_{0} + \\beta_{1} Minutes\\_Played + \\epsilon \\]\nOne way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.\nFor the R formula, instead of an $ = $, you could insert a ~.\nwnba_reg \u0026lt;- lm(avg_points ~ total_minutes, data = basketball) coef(wnba_reg) |\u0026gt; round(4) ## (Intercept) total_minutes ## 1.1356 0.0101 100 * .01 ## [1] 1  Interpretting linear regression terms Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:\n\\[ \\hat{avg\\_points} = 1.1356 + .0101 min\\_played \\]\n\\[ avg\\_points = 1.1356 + .0101 min\\_played + \\epsilon \\]\nWhere instead of \\(\\beta_{0}\\) or \\(\\beta_{1}\\), the estimated values from this single season were inserted. Note the \\(\\hat{avg\\_points}\\), which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.\n1.1356 + .0101 * 0 ## [1] 1.1356 1.1356 + .0101 * 1 ## [1] 1.1457 1.1356 + .0101 * 100 ## [1] 2.1456 1.1356 + .0101 * mean(basketball$total_minutes) ## [1] 5.342042 1.1356 + .0101 * 5000 ## [1] 51.6356 1.1356 + .0101 * -50 ## [1] 0.6306 1.1356 + .0101 * -5000 ## [1] -49.3644 Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.\nFirst, for the intercept (\\(\\beta_{0}\\)), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.\nThe slope, (\\(\\beta_{1}\\)), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.\nThe predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, linewidth = 3, color = \u0026#39;blue\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_histogram(~ avg_points, data = basketball)  What about the error? So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?\nThe short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, \\(\\sigma^2\\). Sometimes this is written in math notation as:\n\\[ \\epsilon \\sim N(0, \\sigma^2) \\]\nFrom this notation, can you see why the error was disregarded earlier when generating predictions?\nIn short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.\nEstimating error in linear regression This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the sum of squares regression (SSR), the portion that is unexplained is referred to as the sum of squares error (SSE). This could be written in math notation as:\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (Y - \\hat{Y})^2 + \\sum (\\hat{Y} - \\bar{Y})^2 \\]\n\\[ \\hat{Y} = \\beta_{0} + \\beta_{1} minutes \\]\n\\[ SS_{reg} = \\sum (\\hat{Y} - \\bar{Y})^2 \\]\n\\[ SS_{error} = \\sum (Y - \\hat{Y})^2 \\]\n\\[ SS_{Total} = \\sum (Y - \\bar{Y})^2 \\]\nLet’s try to visualize what this means.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FF7F7F\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#65a765\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FFD580\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)   Another related measure of error Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.\n\\[ R^{2} = 1 - \\frac{SS_{error}}{SS_{total}} \\]\nor\n\\[ R^{2} = \\frac{SS_{reg}}{SS_{total}} \\]\nLet’s compute the sum of square and get a value for \\(R^2\\).\n\\[ \\sigma ^2 = \\frac{SS_{error}}{ n - 2} \\]\n\\[ \\sigma = \\sqrt{\\sigma^2} \\]\nbasketball %\u0026gt;% summarise(ss_total = sum((avg_points - mean(avg_points))^2), ss_error = sum((avg_points - fitted(wnba_reg))^2), ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %\u0026gt;% mutate(r_square = 1 - ss_error / ss_total, r_square2 = ss_reg / ss_total) ## # A tibble: 1 × 5 ## ss_total ss_error ss_reg r_square r_square2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004. 564. 1440. 0.719 0.719 summary(wnba_reg)$r.square ## [1] 0.7185315 summary(wnba_reg)$sigma ## [1] 1.979045 sigma_hat_square \u0026lt;- 563.9929 / (nrow(basketball) - 2) sigma_hat \u0026lt;- sqrt(sigma_hat_square) sigma_hat_square ## [1] 3.916617 sigma_hat ## [1] 1.979045 sd(basketball$avg_points) ## [1] 3.717388   ","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"64a37443461e2d642acc22a4242a27ff","permalink":"https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/lectures/02-linear-regression-inclass/","section":"lectures","summary":"Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes.","tags":null,"title":"Linear Regression - In Class","type":"book"},{"authors":null,"categories":null,"content":"The following activity is aimed to give you some practice with exploring data on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally, upload the final document to ICON.\nDue Date Due around September 18th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this activity comes from the Tidy Tuesday project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (see the Tidy Tuesday page for more information)\nThe data can be obtained in csv format. A short description for each attribute is as follows. These data are also found within the \u0026ldquo;data\u0026rdquo; folder inside the IDAS.\n   variable class description     om integer Tornado number. Effectively an ID for this tornado in this year.   yr integer Year, 1950-2022.   mo integer Month, 1-12.   dy integer Day of the month, 1-31.   date date Date.   time time Time.   tz character Canonical tz database timezone.   datetime_utc datetime Date and time normalized to UTC.   st character Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).   stf integer State FIPS (Federal Information Processing Standards) number.   mag integer Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).   inj integer Number of injuries. When summing for state totals, use sn == 1 (see below).   fat integer Number of fatalities. When summing for state totals, use sn == 1 (see below).   loss double Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.   slat double Starting latitude in decimal degrees.   slon double Starting longitude in decimal degrees.   elat double Ending latitude in decimal degrees.   elon double Ending longitude in decimal degrees.   len double Length in miles.   wid double Width in yards.   ns integer Number of states affected by this tornado. 1, 2, or 3.   sn integer State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).   f1 integer FIPS code for the 1st county.   f2 integer FIPS code for the 2nd county.   f3 integer FIPS code for the 3rd county.   f4 integer FIPS code for the 4th county.   fc logical Was the mag column estimated?   log_loss. double. The log of the loss attribute    Questions   Explore the distribution of the log_loss attribute. In a few sentences, summarize key elements of the distribution, for instance discussing elements related to shape, center, variation, and/or extreme values.\n  Pick a few attributes (i.e., pick your top 2 favorite) that are of the double or integer type and explore the bivariate association between those attributes. That is, estimate what is the bivariate association between the attributes you chose. In a few sentences, summarize key elements of the associations, paying attention to the strength and direction of association. A few attributes could be: len, wid, inj, fat.\n  Explore the bivariate associations you picked in #2 visually. Does the relationship appear to be linear for the associations that you picked? Why or why not?\n  Using the log_loss attribute, compute descriptive statistics for this attribute using one of the following attributes as a grouping attribute:\n    mag\n  mo\nMore specifically, compute descriptive statistics for the integer or double outcome for each unique value of either mag or mo. If you are using R for this, you may need to force the mag or mo attributes to be factors using the factor() function.\n  Summarize, in a few sentences, any notable differences or similarities in the descriptive statistics computed in #4 across the groups?  ","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661731200,"objectID":"1f0ddba60194db863d603a9e55d6d82d","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/activity1/","publishdate":"2022-08-29T00:00:00Z","relpermalink":"/assignments/activity/activity1/","section":"assignments","summary":"The following activity is aimed to give you some practice with exploring data on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Activity 1","type":"book"},{"authors":null,"categories":null,"content":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally, upload the final document to ICON.\nDue Date Due around October 2nd, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this activity comes from the Tidy Tuesday project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (see the Tidy Tuesday page for more information)\nThe data can be obtained in csv format. A short description for each attribute is as follows. These data are also found within the \u0026ldquo;data\u0026rdquo; folder inside the IDAS.\n   variable class description     om integer Tornado number. Effectively an ID for this tornado in this year.   yr integer Year, 1950-2022.   mo integer Month, 1-12.   dy integer Day of the month, 1-31.   date date Date.   time time Time.   tz character Canonical tz database timezone.   datetime_utc datetime Date and time normalized to UTC.   st character Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).   stf integer State FIPS (Federal Information Processing Standards) number.   mag integer Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).   inj integer Number of injuries. When summing for state totals, use sn == 1 (see below).   fat integer Number of fatalities. When summing for state totals, use sn == 1 (see below).   loss double Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.   slat double Starting latitude in decimal degrees.   slon double Starting longitude in decimal degrees.   elat double Ending latitude in decimal degrees.   elon double Ending longitude in decimal degrees.   len double Length in miles.   wid double Width in yards.   ns integer Number of states affected by this tornado. 1, 2, or 3.   sn integer State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).   f1 integer FIPS code for the 1st county.   f2 integer FIPS code for the 2nd county.   f3 integer FIPS code for the 3rd county.   f4 integer FIPS code for the 4th county.   fc logical Was the mag column estimated?   log_loss. double. The log of the loss attribute    Guiding Question Does the length or width of the tornado explain significant variation in the log loss of the tornado?\nQuestions   Refresh your descriptive analyses from the previous activity, if you did not explore the length or width of the tornado, please do that here.\n  Does the association between length or width of the tornado and log loss of the tornado appear to be linear? Why or why not?\n  Fit a linear regression to answer the research question highlighted above. Select only 1 attribute above to start for this activity Interpret the intercept and slope of the linear regression. That is, what do these terms mean?\n  What are the r-squared and sigma estimates from the linear regression? Interpret these two values in the context of the problem. That is, what do these two terms mean in the context of the data and this problem?\n  Finally, in a couple sentences, provide a summary of the overall model. Does the model appear to be useful to predict or explain variation in the log loss of the tornado with either length or width of the tornado? Use statistics from the analysis steps above to support your answer.\n  ","date":1663200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663200000,"objectID":"7fd38c470121608123f05614943a2100","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/activity2/","publishdate":"2022-09-15T00:00:00Z","relpermalink":"/assignments/activity/activity2/","section":"assignments","summary":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Activity 2","type":"book"},{"authors":null,"categories":null,"content":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nDue Date Due around October 16th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this activity comes from the Tidy Tuesday project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (see the Tidy Tuesday page for more information)\nThe data can be obtained in csv format. A short description for each attribute is as follows. These data are also found within the \u0026ldquo;data\u0026rdquo; folder inside the IDAS.\n   variable class description     om integer Tornado number. Effectively an ID for this tornado in this year.   yr integer Year, 1950-2022.   mo integer Month, 1-12.   dy integer Day of the month, 1-31.   date date Date.   time time Time.   tz character Canonical tz database timezone.   datetime_utc datetime Date and time normalized to UTC.   st character Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).   stf integer State FIPS (Federal Information Processing Standards) number.   mag integer Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).   inj integer Number of injuries. When summing for state totals, use sn == 1 (see below).   fat integer Number of fatalities. When summing for state totals, use sn == 1 (see below).   loss double Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.   slat double Starting latitude in decimal degrees.   slon double Starting longitude in decimal degrees.   elat double Ending latitude in decimal degrees.   elon double Ending longitude in decimal degrees.   len double Length in miles.   wid double Width in yards.   ns integer Number of states affected by this tornado. 1, 2, or 3.   sn integer State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).   f1 integer FIPS code for the 1st county.   f2 integer FIPS code for the 2nd county.   f3 integer FIPS code for the 3rd county.   f4 integer FIPS code for the 4th county.   fc logical Was the mag column estimated?   log_loss. double. The log of the loss attribute    Guiding Question Does the length or width of the tornado explain significant variation in the log loss of the tornado?\nQuestions   This assignment uses the same data from the second activity. Take a few minutes to reacquaint yourself with this analysis, including the linear regression estimates obtained.\n  Was the intercept from the linear regression fitted in activity 2 using the raw data as it was collected interpretable in the context of the data? Rephrasing slightly, could the intercept term be made more interpretable? If so, how?\n  Fit a modified linear regression that performs a centering. That is, center the distance in some fashion. Options could include, minimum, mean, median, maximum, etc centering. What changed for the estimates obtained from the model?\n Which model, the centered or the uncentered, do you feel has a stronger justification for its usage? Be as specific as possible in your rationale.    Use whichever model you feel is the best from #3, then extract from software or compute the standard errors for the estimated regression coefficients. Interpret these standard errors in the context of the problem.\n  Compute confidence intervals for the two regression coefficients. Justify your choice for level of confidence and interpret the confidence interval.\n  Finally, perform hypothesis testing for the two regression coefficients. In this, set up the null/alternative hypotheses and then interpret the statistical results to provide a statistical conclusion. More specifically, what does the hypothesis test suggest about the null and alternative hypotheses?\n  Question 6 talked about the statistical conclusions, now compare/contrast this with practical conclusions. More specifically, would these results be useful in practice? Be as specific as possible in your response.\n  ","date":1695686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695686400,"objectID":"2b4830c801d0b64f1e5498731a72e54f","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/activity3/","publishdate":"2023-09-26T00:00:00Z","relpermalink":"/assignments/activity/activity3/","section":"assignments","summary":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Activity 3","type":"book"},{"authors":null,"categories":null,"content":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nDue Date Due around October 30th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this activity is rideshare data from about three weeks in 2018 for Boston, MA. The data originally come from Kaggle. I have done some processing to randomly select 2,500 records from the larger data for our use and have trimmed down the data attributes.\nNote: Use the data linked here or posted to the IDAS. The data can be obtained in csv format. A short description for each attribute is as follows.\n   Attribute Name Description     id Unique identifier for each row   hour The hour of the ride   day The day of the ride   month The month of the ride, either 11 = November or 12 = December   price The total price of the ride, in $   temperature The outside temperature at the time of the ride, in Fahrenheit   distance Total distance of the ride, in miles   surge_multiplier The surge multiplier for the cost of rides   humidity The humidity at the time of the ride    Guiding Question Does the distance of the ride explain variation in the price of the ride?\nQuestions   Fit a centered or uncentered regression that explored the guiding question above. In a couple of sentences, provide an answer to the research question.\n  Extract the residuals from one of the linear regressions using the centered or uncentered regressions (it doesn\u0026rsquo;t matter which one). Perform a descriptive analysis on these residuals. This could include graphically visualizing the distribution and/or computing summary statistics on the residuals.\n  Evaluate the residuals for the assumption of normality of the residuals. How may the results be impacted by any deviation of normality?\n  Evaluate the residuals for the assumption of homogeneity of variance. How may the results be impacted by any deviation of homogeneity of variance?\n  Are there any noticeable trends in the residuals? What implications could this have for the analysis? If a trend in the residuals exists, what may be a cause for this trend?\n  Explore the model to identify if there are any points that have high leverage or could be identified as extreme values. It may be worth exploring studentized residuals, cook\u0026rsquo;s distance, and/or leverage values.\n  ","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"5332526da9a7c6e4edea1e846b3d68ba","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/activity4/","publishdate":"2022-10-18T00:00:00Z","relpermalink":"/assignments/activity/activity4/","section":"assignments","summary":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Activity 4","type":"book"},{"authors":null,"categories":null,"content":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. Please make sure to add everyone\u0026rsquo;s name to the submission, this can be a comment on ICON or on the document itself.\nInstructions What to turn in Please turn in a document that contains the following:\n answers to the questions below include any relevant statistics/figures that support your answer.  Finally,upload the final document to ICON.\nDue Date Due around November 13th, 2023. No penalty for late submissions as long as it is submitted by December 11th.\nData The data for this activity is San Francisco rental data. The data originally come from Tidy Tuesday. I have done some processing to drop some missing data and remove some attributes from the larger data for our use.\nNote: Use the data linked here or posted to the IDAS. The data can be obtained in csv format. A short description for each attribute is as follows.\n   Attribute Name Description     post_id Unique ID   date date   year year   nhood neighborhood   city city   county county   price price in USD   beds n of beds   sqft square feet of rental   room_in_apt room in apartment    Guiding Questions  Does the square footage (sqft) of the rental help explain variation in the price of the rental? After controlling for the square footage of the rental, does the number of bedrooms help explain any additional variation in the price of the rental?  Questions   Explore descriptively any relationship between square footage and price of the rental. Summarize the relationship in a couple sentences.\n  Fit a model to explore research question 1, summarize in a few sentences the primary regression estimates.\n  Fit a model to explore research question 2 (do not include an interaction),\n interpret the coefficients in a few sentences. is the number of bedrooms helpful over and above the square footage?    Add an interaction to the model from #3.\n Is there evidence of an interaction? Interpret the interaction term(s) in the context of the data.    Explore data conditions/assumptions for the model fitted in question #3 or #4. Use which model you feel is most useful for these data. Provide rationale, in a couple sentences, for why you picked the model from #3 or #4.\n  ","date":1666051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666051200,"objectID":"48b86e5a6c14564d8fb844ace7c0066a","permalink":"https://psqf6243.brandonlebeau.org/assignments/activity/activity5/","publishdate":"2022-10-18T00:00:00Z","relpermalink":"/assignments/activity/activity5/","section":"assignments","summary":"The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment.","tags":null,"title":"Activity 5","type":"book"},{"authors":null,"categories":null,"content":" Bootstrap The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework. The only real assumption when doing a bootstrap approach is that the sample is obtained randomly from the population, an assumption already made in the NHST framework. The primary drawback of the bootstrap approach is that it is computationally expensive, therefore, it can take time to peform the procedure.\nBootstrapping Steps The following are the steps when performing a bootstrap.\nTreat the sample data as the population. Resample, with replacement, from the sample data, ensuring the new sample is the same size as the original. Estimate the model using the resampled data from step 2. Repeat steps 2 and 3 many many times (eg, 10,000 or more). Visualize distribution of effect of interest   Resampling with replacement What is meant by sampling with replacement? Let’s do an example.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors fruit \u0026lt;- data.frame(name = c(\u0026#39;watermelon\u0026#39;, \u0026#39;apple\u0026#39;, \u0026#39;orange\u0026#39;, \u0026#39;kumquat\u0026#39;, \u0026#39;grapes\u0026#39;, \u0026#39;canteloupe\u0026#39;, \u0026#39;kiwi\u0026#39;, \u0026#39;banana\u0026#39;)) |\u0026gt; mutate(obs_num = 1:n()) fruit ## name obs_num ## 1 watermelon 1 ## 2 apple 2 ## 3 orange 3 ## 4 kumquat 4 ## 5 grapes 5 ## 6 canteloupe 6 ## 7 kiwi 7 ## 8 banana 8 slice_sample(fruit, n = nrow(fruit), replace = FALSE) ## name obs_num ## 1 kumquat 4 ## 2 orange 3 ## 3 banana 8 ## 4 apple 2 ## 5 grapes 5 ## 6 kiwi 7 ## 7 watermelon 1 ## 8 canteloupe 6 slice_sample(fruit, n = nrow(fruit), replace = TRUE) ## name obs_num ## 1 kiwi 7 ## 2 apple 2 ## 3 watermelon 1 ## 4 orange 3 ## 5 watermelon 1 ## 6 kiwi 7 ## 7 watermelon 1 ## 8 kiwi 7 slice_sample(fruit, n = nrow(fruit), replace = TRUE) ## name obs_num ## 1 kumquat 4 ## 2 watermelon 1 ## 3 canteloupe 6 ## 4 canteloupe 6 ## 5 watermelon 1 ## 6 kiwi 7 ## 7 grapes 5 ## 8 apple 2  More practical example Let’s load some data to do a more practical example. The following data come from a TIMSS on science achievement. The data provided is a subset of the available data and is not intended to be representative. Below is a short description of the data. Please don’t hesitate to send any data related questions, happy to provide additional help on interpreting the data appropriately.\n IDSTUD: A unique student ID ITBIRTHM: The birth month of the student ITBIRTHY: The birth year of the student ITSEX: The birth sex of the student ASDAGE: The age of the student (at time of test) ASSSCI01: Overall science scale score ASSEAR01: Earth science scale score ASSLIF01: Life science scale score ASSPHY01: Physics scale score ASSKNO01: Science knowing scale score ASSAPP01: Science applying scale score ASSREA01: Science reasoning scale score  library(tidyverse) timss \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/timss_grade4_science.csv\u0026#39;) |\u0026gt; filter(ASDAGE \u0026lt; 15) ## Rows: 10029 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## dbl (12): IDSTUD, ITBIRTHM, ITBIRTHY, ITSEX, ASDAGE, ASSSCI01, ASSEAR01, ASS... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(timss) ## # A tibble: 6 × 12 ## IDSTUD ITBIRTHM ITBIRTHY ITSEX ASDAGE ASSSCI01 ASSEAR01 ASSLIF01 ASSPHY01 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 10101 6 2005 2 9.92 505. 445. 408. 458. ## 2 10102 10 2005 2 9.58 401. 291. 323. 297. ## 3 10103 1 2005 1 10.3 542. 440. 529. 529. ## 4 10104 6 2005 2 9.92 544. 534. 552. 557. ## 5 10105 6 2005 2 9.92 588. 555. 500. 559. ## 6 10106 12 2005 1 9.42 583. 556. 585. 562. ## # ℹ 3 more variables: ASSKNO01 \u0026lt;dbl\u0026gt;, ASSAPP01 \u0026lt;dbl\u0026gt;, ASSREA01 \u0026lt;dbl\u0026gt;  Two Guiding Questions Is the age of a student related to their overall science scale score? Is the life science scale score related to the overall science scale score?   Bivariate exploration It is important to explore relationships bivariately before going to the model phase. To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”. You may also want to fill in an appropriate axis labels in place of “@@” below.\n Summarize the bivariate association  library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) theme_set(theme_bw(base_size = 18)) gf_point(ASSSCI01 ~ ASDAGE, data = timss, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5, color = \u0026#39;green\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;Age\u0026quot;, y = \u0026quot;Overall Science\u0026quot;) mosaic::cor(ASSSCI01 ~ ASDAGE, data = timss) |\u0026gt; round(2) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## [1] -0.04  Estimate Parameters To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”.\n Interpret the following parameter estimates in the context of the current problem. (ie., what do these parameter estimates mean?)  timss_model \u0026lt;- lm(ASSSCI01 ~ ASDAGE, data = timss) summary(timss_model) ## ## Call: ## lm(formula = ASSSCI01 ~ ASDAGE, data = timss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -305.769 -51.969 4.919 56.707 272.217 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 613.563 17.064 35.956 \u0026lt; 2e-16 *** ## ASDAGE -6.687 1.669 -4.007 6.18e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 80.89 on 10025 degrees of freedom ## Multiple R-squared: 0.001599,\tAdjusted R-squared: 0.0015 ## F-statistic: 16.06 on 1 and 10025 DF, p-value: 6.183e-05 confint(timss_model) ## 2.5 % 97.5 % ## (Intercept) 580.114047 647.012616 ## ASDAGE -9.957345 -3.415915  Resample these data For the astronauts data, to resample, a similar idea can be made. Essentially, we are treating these data as a random sample of some population of space missions. We again, would resample, with replacement, which means that multiple missions would likely show up in the resampling procedure.\nslice_sample(timss, n = nrow(timss), replace = TRUE) |\u0026gt; count(IDSTUD) |\u0026gt; arrange(-n) |\u0026gt; head() ## # A tibble: 6 × 2 ## IDSTUD n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 520418 7 ## 2 2430125 7 ## 3 60407 6 ## 4 700211 6 ## 5 1040624 6 ## 6 1810412 6 resamp_lm \u0026lt;- slice_sample(timss, n = nrow(timss), replace = TRUE) %\u0026gt;% lm(ASSSCI01 ~ ASDAGE, data = .) summary(resamp_lm) ## ## Call: ## lm(formula = ASSSCI01 ~ ASDAGE, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -303.908 -51.787 4.607 58.200 258.774 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 644.327 17.010 37.879 \u0026lt; 2e-16 *** ## ASDAGE -9.748 1.662 -5.864 4.66e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 81.3 on 10025 degrees of freedom ## Multiple R-squared: 0.003419,\tAdjusted R-squared: 0.003319 ## F-statistic: 34.39 on 1 and 10025 DF, p-value: 4.657e-09 confint(resamp_lm) ## 2.5 % 97.5 % ## (Intercept) 610.98417 677.670175 ## ASDAGE -13.00635 -6.489498 nrow(timss)^nrow(timss) ## [1] Inf Questions to consider Do the parameter estimates differ from before? Why or why not? Would you come to substantially different conclusions from the original analysis? Why or why not?  Let’s now repeat this a bunch of times.\nset.seed(2022) resample_timss \u0026lt;- function(data, model_equation) { timss_resample \u0026lt;- slice_sample(data, n = nrow(data), replace = TRUE) lm(model_equation, data = timss_resample) |\u0026gt; coef() } Run this function a bunch of times manually, what happens to the estimates? Why is this happening?\nresample_timss(data = timss, model_equation = ASSSCI01 ~ ASDAGE) ## (Intercept) ASDAGE ## 593.768824 -4.636762   Make sure future.apply is installed The following code chunk makes sure the future.apply function is installed for parallel processing in R. If you get an error, you can uncomment (delete the # symbol) in the first line of code to (hopefully) install the package yourself.\n# install.packages(\u0026quot;future.apply\u0026quot;) library(future.apply) ## Loading required package: future plan(multisession)  Replicate The following code replicates the analysis many times. Pick an initial value for N and fill in the model equation to match your code above. I will ask you to change this N value later.\ntimss_coef \u0026lt;- future.apply::future_replicate(10000, resample_timss(data = timss, model_equation = ASSSCI01 ~ ASDAGE), simplify = FALSE) |\u0026gt; dplyr::bind_rows() |\u0026gt; tidyr::pivot_longer(cols = everything()) #head(timss_coef)  Visualize results The following code visualizes the results of the analysis above. Explore the following questions.\nWhat does this distribution show/represent? What are key features of this distribution? How do these values compare to the original linear regression results?  Are there comparable statistics here compared to those?   gf_density(~ value, data = timss_coef) |\u0026gt; gf_facet_wrap(~ name, scales = \u0026#39;free\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;Regression Estimates\u0026quot;)  Change the N value Now, change the N value for the replicate step (you can either add a new cell to copy/paste the code or just change it in the code above).\n What happens to the resulting figure when the N increases? What value for N seems to be reasonable? Why?  head(timss_coef) ## # A tibble: 6 × 2 ## name value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 610. ## 2 ASDAGE -6.56 ## 3 (Intercept) 603. ## 4 ASDAGE -5.67 ## 5 (Intercept) 620. ## 6 ASDAGE -7.25 df_stats(value ~ name, data = timss_coef, mean, median, sd, IQR) ## response name mean median sd IQR ## 1 value (Intercept) 613.863905 614.021876 20.493175 27.714280 ## 2 value ASDAGE -6.714967 -6.740487 2.005943 2.728819 (-6.687 - 0) / 2.012 ## [1] -3.323559 pnorm(-3.3235, lower.tail = TRUE) ## [1] 0.0004444771 pnorm(-3.3235, lower.tail = TRUE) * 2 ## [1] 0.0008889542 summary(timss_model) ## ## Call: ## lm(formula = ASSSCI01 ~ ASDAGE, data = timss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -305.769 -51.969 4.919 56.707 272.217 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 613.563 17.064 35.956 \u0026lt; 2e-16 *** ## ASDAGE -6.687 1.669 -4.007 6.18e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 80.89 on 10025 degrees of freedom ## Multiple R-squared: 0.001599,\tAdjusted R-squared: 0.0015 ## F-statistic: 16.06 on 1 and 10025 DF, p-value: 6.183e-05 plot(timss_model, which = 1:5) confint(timss_model) ## 2.5 % 97.5 % ## (Intercept) 580.114047 647.012616 ## ASDAGE -9.957345 -3.415915 df_stats(value ~ name, data = timss_coef, quantile(c(0.025, 0.975))) ## response name 2.5% 97.5% ## 1 value (Intercept) 574.15367 653.425218 ## 2 value ASDAGE -10.57731 -2.829617   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37b251c77caf11697d80e6eefec02314","permalink":"https://psqf6243.brandonlebeau.org/lectures/07-bootstrap-class/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lectures/07-bootstrap-class/","section":"lectures","summary":"Bootstrap The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework. The only real assumption when doing a bootstrap approach is that the sample is obtained randomly from the population, an assumption already made in the NHST framework.","tags":null,"title":"An R Markdown document converted from \"content/lectures/07-bootstrap-class.ipynb\"","type":"lectures"}]