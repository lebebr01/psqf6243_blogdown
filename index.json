[{"authors":["admin"],"categories":null,"content":"\nThe course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.g., Pearson correlation).\nThis course will focus on using statistical methods to answer research questions and make decisions from data. General linear models (i.e., regression) will serve as the fundamental building block that students will be exposed to answer questions about associations of quantitative outcomes with quantitative and categorical predictors. Statistical estimation, model building, and inference using statistical software will serve as primary topics which students will gain a stronger understanding by the end of the course.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://psqf6243.brandonlebeau.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.","tags":null,"title":"","type":"authors"},{"authors":["brandon"],"categories":null,"content":"Brandon LeBeau\nI\u0026rsquo;m interested in computational methods, longitudinal data, statistical software development with R, and quantitative program evaluation. You can see more about my interests on my website: https://brandonlebeau.org/.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a6b366d06474d85d9f788b8d18e8310d","permalink":"https://psqf6243.brandonlebeau.org/authors/brandon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brandon/","section":"authors","summary":"Brandon LeBeau\nI\u0026rsquo;m interested in computational methods, longitudinal data, statistical software development with R, and quantitative program evaluation. You can see more about my interests on my website: https://brandonlebeau.org/.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"A list of the notes used for the semester. These are updated as we go. I do my best to ge them to you before class.\n Review Linear Regression Intro  Class Notes   Linear Regression - In Class Regression Estimates  Class Notes   Least Squares Simulation Regression Inference Regression Data Conditions  ","date":1661126400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1661126400,"objectID":"87e0410d95d74ae914d8a58b3f6a1716","permalink":"https://psqf6243.brandonlebeau.org/lectures/","publishdate":"2022-08-22T00:00:00Z","relpermalink":"/lectures/","section":"lectures","summary":"Lecture Notes","tags":null,"title":"Lecture Notes","type":"book"},{"authors":null,"categories":null,"content":"This page contains the syllabus for the course.\n This syllabus is an attempt early in the semester to plan for the course. This syllabus is subject to change at the Instructors discretion.  Course Information PSQF 6243: Intermediate Statistical Methods - Spring 2024\nInstructor Information  Brandon LeBeau, Ph.D. Email: brandon-lebeau at uiowa.edu Virtual Office Hours (Zoom): Tues 10 am to 11:30 am or by appointment  See ICON for office hours zoom link   Department: Psychological and Quantitative Foundations, 361 LC  Interim DEO: Dr. Dennis Martin Kivlighan,, 361 LC    TA Information  Alfonso J. Martinez email: alfonso-martinez at uiowa.edu Virtual Office Hours (Zoom): Mon / Wed 10 am to 12 pm or by appointment  See ICON for office hours zoom link    Course Quote Data does not give up their secrets easily. They must be tortured to confess. \u0026ndash; Jeff Hooper, Bell Labs\nCourse Description The course aims to get students familiar with statistical reasoning, fitting and interpreting statistical models, and using data to make decisions. Students are assumed to have some background in statistical concepts and have a basic understanding of common descriptive statistics and bivariate association (e.g., Pearson correlation).\nThis course will focus on using statistical methods to answer research questions and make decisions from data. General linear models (i.e., regression) will serve as the fundamental building block that students will be exposed to answer questions about associations of quantitative outcomes with quantitative and categorical predictors. Statistical estimation, model building, and inference using statistical software will serve as primary topics which students will gain a stronger understanding by the end of the course.\nCourse Objectives The course requirements and class materials are aimed to help students achieve the following course objectives:\n become more fluent in statistical terminology, use statistical software, turn research questions into actionable statistical methods, engage with statistical theory, connect model equations with statistical code, summarize uncertainty and variation in estimation.  Textbook  Regression and Other Stories by Gelman, Hill, and Vehtari: PDF and Book Page Introduction to Statistical Learning (2nd edition), by James, Witten, Hastie, and Tibshirani: https://www.statlearning.com/ Supplemental Text:  Statistical Reasoning through Computation and R, by LeBeau and Zieffler: https://lebebr01.github.io/stat_thinking/ Introduction to Modern Statistics, by Cetinkaya-Rundel and Hardin. Found online here: https://openintro-ims.netlify.app/ Applied Regression: An Introduction, by Lewis-Beck and Lewis-Beck. Online access via library: https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n1.   Class notes and Jupyter Notebooks used in class will also be shared through the course website.  Course Requirements   Online Quizzes: There will be a total of 10 online quizzes taken through ICON. Each quiz will be available for 5 to 7 days and are open note/book, but should be completed independently and will need to be completed in a single occasion. Expect each quiz to be about 10 questions. The dates will be discussed in class and announcements will be sent by email to communicate when quizzes open and close. Quizzes will account for 40% of the course grade.\n  Assignments: 3 assignments spread throughout the course will be used to improve student understanding and give students practice to work with data. These assignments can be completed within small groups of no more than 3 students. If completed as a group, all students will receive the same score and only one assignment with everyone\u0026rsquo;s name on it needs to be turned in. Assignments will account for 40% of the course grade.\n  Practice Problems: Approximately 8 to 10 practice problems will be posted across the semester. These are complete/incomplete problems, therefore, completing these will give you 20% of the final course grade. Completion of these involve posting answers to the questions that show you interacted and engaged in the practice problem. Answers that are not on topic or missing will not count as \u0026ldquo;passing\u0026rdquo; for that practice problem. These practice problems are meant to be formative assessments to give you time to practice interpreting statistics. Similar to the assignments, you may work in groups of up to 3 to complete these practice problems, in which you would all receive the same pass/fail grade for the activity. The practice problems will be short problems that will ask you to use statistical software and answer questions about the output generate.\n  Absences: Absences happen. Therefore, I ask you to be as transparent as possible with me. I promise to be compassionate and understanding. If at any point in the semester you are having difficulties, please reach out to me and I will do my best to be accommodating and provide support, which could include an extension on course deadlines as necessary.\n  Grading: Final grades will be based on the following weighting scheme:\n 10 quizzes, equally weighted, 40% of total grade 4 assignments, equally weighted, 40% of total grade About 6 activities, equally weighted, 20% of total grade    Percentage Breakdown: Guidelines are given below, plus and minus grades will be given as well. Changes may occur to the grade percentage breakdown below, but the percentages will not get higher. For example, the lower percentage to get a B, could drop to 75%, but will never be higher than 80%. I will do my best to communicate any shifts to these grade ranges as the semester progresses.\n A 94% and above A- 90% to 93% B+ 87% to 89% B 83% to 86% B- 80% to 82% C+ 77% to 79% C 73% to 76% C- 70% to 72% D+ 67% to 69% D 63% t0 66% D- 60% to 62% F Below 60%    Course and University Policies  Announcements and Communication: Any announcements regarding the course will be communicated via email so please check it daily.  Email Expectations: I want to be transparent regarding course email. Email is not meant to be an instantaneous exchange, rather, email is asynchronous. I typically will respond to an email within one business day. Note, I typically will not respond to email over the weekend or during evening hours, but there could be exceptions to this. If I do send an email during the evening or weekend, there is no expectation that you reply immediately as well. If a reply is warranted, reply on your own time. Please be advised that this applies to all elements of the semester, even around due dates. If you email a question the day an assignment is due, I may not be able to get back to you until the next day   Course Materials: Course materials will be posted to ICON or the course website. Go to icon.uiowa.edu for access to the ICON site and the primary course website for PSQF 6243. Adaptations and Modifications: Please inform me during the first two weeks if you require special adaptations or modifications to any assignment or due dates because of special circumstances such as learning disabilities, religious observances, or other appropriate needs. Contesting a Grade: To contest a grade, please send me an email detailing your reason within 48 hours of receiving the grade. This allows both of us time to think, reflect, and discuss the matter without taking class time from other students. When contesting a grade, provide a copy of the graded assignment. Academic Misconduct: Plagiarism and cheating may result in grade reduction and/or serious penalties. Unless you are otherwise instructed, your work should be entirely your own. Please take care in writing your final project. You should always be writing in your own words, citing others' ideas, and quoting text as appropriate. This link provides the College of Education policy on student academic misconduct. Student Complaint Procedures: The College of Education policy on student complaints about faculty action Use of AI: Students are invited to use AI platforms to help prepare for assignments and projects (e.g., to help with brainstorming or to test your own knowledge about specific course topics). I also welcome you to use AI tools to help revise and edit your work (e.g., to help identify flaws in reasoning, spot confusing or underdeveloped paragraphs, or to simply fix citations). If you use AI tools to aid in writing or editing any written material you submit for the course, please make sure you cite the tool and indicate how you fact checked the material.  University Policies The following University of Iowa course policies are published by the Provost office.\nAt the University of Iowa, we strive for a classroom or laboratory climate that encourages learning while also protecting the freedoms and rights of our students and faculty.\nPlease review the following course policies, expectations, and resources. Visit the Dean of Students website for additional student policies and procedures.\n Free Speech and Expression: The University of Iowa supports and upholds the First Amendment protection of freedom of speech and the principles of academic and artistic freedom. We are committed to open inquiry, vigorous debate, and creative expression inside and outside of the classroom. Visit the Free Speech at Iowa website for more information on the university’s policies on free speech and academic freedom. Accommodations for Students with Disabilities: The University is committed to providing an educational experience that is accessible to all students. If a student has a diagnosed disability or other disabling condition that may impact the student’s ability to complete the course requirements as stated in the syllabus, the student may seek accommodations through Student Disability Services (SDS). SDS is responsible for making Letters of Accommodation (LOA) available to the student. The student must provide a LOA to the instructor as early in the semester as possible, but requests not made at least two weeks prior to the scheduled activity for which an accommodation is sought may not be accommodated. The LOA will specify what reasonable course accommodations the student is eligible for and those the instructor should provide. Additional information can be found on the SDS website. Absences from Class: University regulations require that students be allowed to make up examinations which have been missed due to illness, religious holy days, military service obligations, including service-related medical appointments, jury duty, or other unavoidable circumstances or other University-sponsored activities. Students should work with faculty regarding making up other missed work, such as assignments, quizzes, and classroom attendance. Absences for Religious Holy Days: The University is prepared to make reasonable accommodations for students whose religious holy days coincide with their classroom assignments, test schedules, and classroom attendance expectations. Students must notify their instructors in writing of any such Religious Holy Day conflicts or absences within the first few days of the semester or session, and no later than the third week of the semester. If the conflict or absence will occur within the first three weeks of the semester, the student should notify the instructor as soon as possible. See Operations Manual 8.2 Absences for Religious Holy Days for additional information. Absences for Military Service Obligations: Students absent from class or class-related requirements due to U.S. veteran or U.S. military service obligations (including military service–related medical appointments, military orders, and National Guard Service obligations) shall be excused without any grading adjustment or other penalty. Instructors shall make reasonable accommodations to allow students to make up, without penalty, tests and assignments they missed because of veteran or military service obligations. Reasonable accommodations may include making up missed work following the service obligation; completing work in advance; completing an equivalent assignment; or waiver of the assignment without penalty. In all instances, students bear the responsibility to communicate with their instructors about such veteran or military service obligations, to meet course expectations and requirements. Classroom Expectations: Students are expected to comply with University policies regarding appropriate classroom behavior as outlined in the Code of Student Life. While students have the right to express themselves and participate freely in class, it is expected that students will behave with the same level of courtesy and respect in the virtual class setting (whether asynchronous or synchronous) as they would in an in-person classroom. Failure to follow behavior expectations as outlined in the Code of Student Life may be addressed by the instructor and may also result in discipline under the Code of Student Life policies governing E.5 Disruptive Behavior or E.6 Failure to Comply with University Directive. Non-Discrimination Statement: The University of Iowa prohibits discrimination in employment, educational programs, and activities on the basis of race, creed, color, religion, national origin, age, sex, pregnancy, disability, genetic information, status as a U.S. veteran, service in the U.S. military, sexual orientation, gender identity, associational preferences, or any other classification that deprives the person of consideration as an individual. The university also affirms its commitment to providing equal opportunities and equal access to university facilities. For additional information on nondiscrimination policies, contact the Director, Office of Institutional Equity, the University of Iowa, 202 Jessup Hall, Iowa City, IA 52242-1316, 319-335-0705, oie-ui@uiowa.edu. Students may share their pronouns and chosen/preferred names in MyUI, which is accessible to instructors and advisors. Sexual Harassment/Sexual Misconduct and Supportive Measures: The University of Iowa prohibits all forms of sexual harassment, sexual misconduct, and related retaliation. The Policy on Sexual Harassment and Sexual Misconduct governs actions by students, faculty, staff and visitors. Incidents of sexual harassment or sexual misconduct can be reported to the Title IX and Gender Equity Office or to the Department of Public Safety. Students impacted by sexual harassment or sexual misconduct may be eligible for academic supportive measures and can learn more by contacting the Title IX and Gender Equity Office. Information about confidential resources can be found here. Watch the video for an explanation of these resources. Mental Health: Students are encouraged to be mindful of their mental health and seek help as a preventive measure or if feeling overwhelmed and/or struggling to meet course expectations. Students are encouraged to talk to their instructor for assistance with specific class-related concerns. For additional support and counseling, students are encouraged to contact University Counseling Service (UCS). Information about UCS, including resources and how to schedule an appointment, can be found at counseling.uiowa.edu. Find out more about UI mental health services at: mentalhealth.uiowa.edu. Basic Needs and Support for Students: It can be difficult to maintain focus and be present if you are experiencing challenges with meeting basic needs or navigating personal crisis situations. The Office of the Dean of Students can help. Contact us for one-on-one support, identifying options, and to locate and access basic needs resources (such as food, rent, childcare, etc.). Student Care and Assistance, 132 IMU, dos-assistance@uiowa.edu, 319-335-1162. Basic Needs Info: Food Pantry at Iowa, Clothing Closet, Basic Needs and Support Resource Sharing of Class Recordings: Students may be enrolled in a class where some sessions will be recorded or live-streamed. Such recordings/streaming will only be available to students registered for the class. These recordings are the intellectual property of the faculty and they may not be shared or reproduced without the explicit, written consent of the faculty member. Further, students may not share these sessions with those not in the class or upload them to any other online environment. Doing so would be a breach of the Code of Student Conduct, and, in some cases, a violation of state and federal law, including the Federal Education Rights and Privacy Act (FERPA). This course is provided by the College of Education and cross-listed with the Statistics Department. Policies on matters such as course requirements, grading, and sanctions for academic dishonesty are governed by the College of Education. Students wishing to add or drop this course after the official deadline must receive approval of the Dean of the College of Education. See the College of Education policy on cross enrollments.  ","date":1660694400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1660694400,"objectID":"5538b9800f06ab7b29edaa22ab7e63cb","permalink":"https://psqf6243.brandonlebeau.org/syllabus/","publishdate":"2022-08-17T00:00:00Z","relpermalink":"/syllabus/","section":"syllabus","summary":"Course syllabus","tags":null,"title":"Syllabus","type":"book"},{"authors":null,"categories":null,"content":" R Resources  PSQF 6250 Course Materials - This is my course, usually offered during the spring semester. R for Data Science   SPSS Resources  Regression with SPSS SPSS Tutorials   SAS Resources  Regression with SAS SAS Tutorial   Stata Resources  Regression with Stata Stata Tutorials    ","date":1630022400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1630022400,"objectID":"b3827c9680770e0f03c58ccf7bc48d61","permalink":"https://psqf6243.brandonlebeau.org/code/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/code/","section":"code","summary":"Software Resources","tags":null,"title":"Software Resources","type":"book"},{"authors":null,"categories":null,"content":"A general overview of the semester. This is subject to change and we may not reach all of the course topics planned at the beginning of the semester. Readings/objects for specific topics will be posted to the course content pages.\nPart 1 - Linear Regression  Review / Exploring univariate and multivariate distributions Introduction to statistical software Simple linear regression / correlation Ordinary least squares estimation Inference for linear regression Multiple linear regression Data Conditions for regression  Part 2 - Regression Special Cases 2-group designs (i.e., 2-sample t-tests) More than 2 group designs (i.e., ANOVA) Nonlinear effects of quantitative predictors Interaction effects  Part 3 – Special Topics (as time permits) Chi-square for association and independence Dichotomous Attributes as outcomes  ","date":1629590400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629590400,"objectID":"ee8dfa775d2ca48042bf50fe0819e70c","permalink":"https://psqf6243.brandonlebeau.org/schedule/","publishdate":"2021-08-22T00:00:00Z","relpermalink":"/schedule/","section":"schedule","summary":"Course schedule","tags":null,"title":"Schedule","type":"book"},{"authors":null,"categories":null,"content":"The course content will be organized by weeks. Each week will contain some text to:\n discuss the goals of the week the content to be covered relevant R syntax/notebook files.  Each week may also contain some information about assignments and links directly to those on ICON/IDAS.\n Welcome  IDAS Introduction   Week 1 Week 2 Week 3 Week 4 Week 5 Week 6 Week 7 Week 8 Week 9 Week 10 Week 11 Week 12 Week 13 Week 14 Week 15  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"d5be68294f12f9cfecf81ad87009adc6","permalink":"https://psqf6243.brandonlebeau.org/content/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Course Content","tags":null,"title":"Content","type":"book"},{"authors":null,"categories":null,"content":"Here you can view all of the course assignments for the semester. This will include the hands on assignments and the quizzes. The quizzes will provide a link to ICON to complete.\n Assignments Practice problems Quizzes  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"5d370553e45c580541e007200292c8d8","permalink":"https://psqf6243.brandonlebeau.org/assignments/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/assignments/","section":"assignments","summary":"Course Requirements","tags":null,"title":"Course Requirements","type":"book"},{"authors":null,"categories":null,"content":"A list of the data used within the course.\nMore details to come soon \u0026hellip; .\n","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"77c2d2ba0979ac9954ceb502de85c1ce","permalink":"https://psqf6243.brandonlebeau.org/data/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/data/","section":"data","summary":"Data for the course","tags":null,"title":"Data","type":"book"},{"authors":null,"categories":null,"content":"Below are the course quizzes.\n Quiz 1 Quiz 2 Quiz 3 Quiz 4 Quiz 5 Quiz 6 Quiz 7 Quiz 8 Quiz 9 Quiz 10  ","date":1629331200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629331200,"objectID":"9ee2f1578158bc73334dba690602f1a2","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/","publishdate":"2021-08-19T00:00:00Z","relpermalink":"/assignments/quizzes/","section":"assignments","summary":"Course Quizzes","tags":null,"title":"Quizzes","type":"book"},{"authors":null,"categories":null,"content":" Practice Problem 1 Practice Problem 2 Practice Problem 3 Practice Problem 4 Practice Problem 5  ","date":1631577600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631577600,"objectID":"d7341cf8603c781f777bf04ce6433c9c","permalink":"https://psqf6243.brandonlebeau.org/assignments/practice/","publishdate":"2021-09-14T00:00:00Z","relpermalink":"/assignments/practice/","section":"assignments","summary":"Course Practice Problems","tags":null,"title":"Practice Problems","type":"book"},{"authors":null,"categories":null,"content":"  Review for PSQF 6243 This serves as a non-exhaustive review for the course. These are elements that I assume you have knowledge of prior to starting the course.\n Variable vs constant attributes Types of variables (ie., nominal, ordinal, integer, ratio) Descriptive Statistics (eg., mean, median, standard deviation, variance, percentiles) Higher order moments (eg., skewness and kurtosis) Exploring/summarizing univariate distributions (eg., histogram or density figure) What is a statistical model? Why do we use them? Population vs Sample  Examples Mario Kart 64 world record data:\n  variable class description    track character Track name  type factor Single or three lap record  shortcut factor Shortcut or non-shortcut record  player character Player’s name  system_played character Used system (NTSC or PAL)  date date World record date  time_period period Time as hms period  time double Time in seconds  record_duration double Record duration in days    # load some libraries library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(lubridate) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(e1071) theme_set(theme_bw(base_size = 18)) # load in some data mariokart \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv\u0026#39;) %\u0026gt;% mutate(year = year(date), month = month(date), day = month(date)) ## Rows: 2334 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (6): track, type, shortcut, player, system_played, time_period ## dbl (2): time, record_duration ## date (1): date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(mariokart) ## # A tibble: 6 × 12 ## track type shortcut player system_played date time_period time ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Luigi Raceway Thre… No Salam NTSC 1997-02-15 2M 12.99S 133. ## 2 Luigi Raceway Thre… No Booth NTSC 1997-02-16 2M 9.99S 130. ## 3 Luigi Raceway Thre… No Salam NTSC 1997-02-16 2M 8.99S 129. ## 4 Luigi Raceway Thre… No Salam NTSC 1997-02-28 2M 6.99S 127. ## 5 Luigi Raceway Thre… No Gregg… NTSC 1997-03-07 2M 4.51S 125. ## 6 Luigi Raceway Thre… No Rocky… NTSC 1997-04-30 2M 2.89S 123. ## # ℹ 4 more variables: record_duration \u0026lt;dbl\u0026gt;, year \u0026lt;dbl\u0026gt;, month \u0026lt;dbl\u0026gt;, day \u0026lt;dbl\u0026gt; # univariate distribution of time gf_histogram(~ time, data = mariokart, bins = 30) %\u0026gt;% gf_labs(x = \u0026quot;Time (in seconds)\u0026quot;) gf_density(~ time, data = mariokart) %\u0026gt;% gf_labs(x = \u0026quot;Time (in seconds)\u0026quot;) df_stats(~ time, data = mariokart, mean, median, sd, skewness, kurtosis, quantile(probs = c(0.1, 0.5, 0.9))) ## response mean median sd skewness kurtosis 10% 50% 90% ## 1 time 90.62383 86.19 66.6721 1.771732 3.844745 31.31 86.19 171.961   Bivariate Association cor(time ~ record_duration, data = mariokart) ## [1] -0.06736739 gf_point(time ~ record_duration, data = mariokart) %\u0026gt;% gf_labs(x = \u0026quot;How long the record was held\u0026quot;, y = \u0026quot;Time (in seconds)\u0026quot;) Questions What is problematic about the analyses above? Why? What could be done to improve the analyses above?    ","date":1692662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692662400,"objectID":"2824d49626915b3cf1d27b9ea2cf5334","permalink":"https://psqf6243.brandonlebeau.org/lectures/01-review/","publishdate":"2023-08-22T00:00:00Z","relpermalink":"/lectures/01-review/","section":"lectures","summary":"Review","tags":null,"title":"Review","type":"book"},{"authors":null,"categories":null,"content":"Getting Started  Review the syllabus Review the schedule Review, accessing the IDAS page  Jupyter Notebooks version of IDAS RStudio Server, to access you can switch via the \u0026ldquo;New\u0026rdquo; button on Jupyter Notebooks.   Optionally, but strongly encouraged, complete the course survey  ","date":1692316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692316800,"objectID":"1573444049fbe3c4964b394aa4c5174c","permalink":"https://psqf6243.brandonlebeau.org/content/00-getting-started/","publishdate":"2023-08-18T00:00:00Z","relpermalink":"/content/00-getting-started/","section":"content","summary":"Getting Started","tags":null,"title":"Welcome","type":"book"},{"authors":null,"categories":null,"content":"Introduction This page is meant as a way to give some introduction and how to access the IDAS used for the course.\nAccess The IDAS server can be accessed via the following link directly: Access IDAS. Upon going here, you will be prompted to log in with your HawkID. Finally, to use the IDAS, you will need to have DUO authentication activated.\nHere is a good link if you have not set up DUO authentication before from ITS. It may take a few hours or up to a day to be granted access to the IDAS after setting up DUO authentication for the first time, DUO Authentication Docs.\nIDAS Specifics The IDAS by default will launch in Jupyter Lab mode and will sync all course materials that I have posted for the course. The syncing of remote materials will never overwrite anything that you have manually edited by you, the student. For this reason, if I notice a breaking change, I usually post a new updated file with a new name.\nYou can also access RStudio server within the IDAS for the course as well. This can be accessed by clicking on \u0026ldquo;RStudio\u0026rdquo; from the launcher page. You should still have access to all of the course files that are automatically synced for you within the RStudio environment.\nShutting down IDAS When you are finished working in the IDAS, you should go to:\n File \u0026gt; Hub Control Panel \u0026gt; Stop My Server  This disconnects/stops the server instance so that the resources can be allocated to others wishing to use the service.\nCommon issues ITS has established a wiki page for common problems that occur and other documentation related to the IDAS.\nMarkdown Cheat Sheet For those that are interested in working with the Jupyter Notebooks for the assignments. I include the following resources that may be helpful.\n Markdown Cheat Sheet Jupyter Notebooks  ","date":1692316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692316800,"objectID":"4400f965f661bfec3d4ba2892522f833","permalink":"https://psqf6243.brandonlebeau.org/content/00a-idas/","publishdate":"2023-08-18T00:00:00Z","relpermalink":"/content/00a-idas/","section":"content","summary":"TBD","tags":null,"title":"IDAS Introduction","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to the course. Primary content will be a review to set the basis for the remainder of the semester.\nObjectives  Read syllabus, ask any questions Review introductory statistics content Explore R code from introductory slides  Activities  Review introductory statistics content  Chapters 2 - 4 of Statistical Reasoning through computation and R   Optional, complete course survey  Assignments None this week.\n","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"d1fbf25a3d7006b7a626817f4e8207b2","permalink":"https://psqf6243.brandonlebeau.org/content/01-week1/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/content/01-week1/","section":"content","summary":"TBD","tags":null,"title":"Week 1","type":"book"},{"authors":null,"categories":null,"content":"  Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes. One of those attributes will be the outcome or attribute of interest whereas the other will used as a predictor. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable explains variation in the dependent variable (more on this later).\nOf note, variable is a typical word used in statistics, I’ve come to like the word attribute instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.\nWe may write this general model as:\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nWhere \\(Y\\) is the outcome attribute. It is also known as the dependent variable. The \\(X\\) term is the predictor/covariate attribute. It is also known as the independent variable. The \\(\\epsilon\\) is a random error term, more on this later. Finally, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown population coefficients that we are interested in estimating. More on this later too.\nSpecific example The data used for this section of the course is from the 2019 WNBA season. These data are part of the bayesrules package/book. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(ggformula) basketball \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv\u0026quot;) ## Rows: 146 Columns: 32 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): player_name, team ## dbl (29): height, weight, year, age, games_played, games_started, avg_minute... ## lgl (1): starter ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. theme_set(theme_bw(base_size = 18)) head(basketball) ## # A tibble: 6 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, …  Guiding Question Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.\nOne first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.\nThe next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) Questions to consider What can be noticed about the relationship between these two attributes? Does there appear to be a relationship between the two? Is this relationship perfect?    Adding a smoother line Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.\nI’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.\nDoes there appear to be much difference in the relationship across the two lines?\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39;  Estimating linear regression coefficients The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is lm() to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.\nThis equation could be written more directly for our specific problem.\n\\[ Avg\\_points = \\beta_{0} + \\beta_{1} Minutes\\_Played + \\epsilon \\]\nOne way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.\nFor the R formula, instead of an $ = $, you could insert a ~.\nwnba_reg \u0026lt;- lm(avg_points ~ total_minutes, data = basketball) coef(wnba_reg) ## (Intercept) total_minutes ## 1.13562456 0.01014207  Interpretting linear regression terms Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:\n\\[ \\hat{avg\\_points} = 1.1356 + .0101 min\\_played \\]\nWhere instead of \\(\\beta_{0}\\) or \\(\\beta_{1}\\), the estimated values from this single season were inserted. Note the \\(\\hat{avg\\_points}\\), which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.\n1.1356 + .0101 * 0 ## [1] 1.1356 1.1356 + .0101 * 1 ## [1] 1.1457 1.1356 + .0101 * 100 ## [1] 2.1456 1.1356 + .0101 * mean(basketball$avg_points) ## [1] 1.189732 1.1356 + .0101 * 5000 ## [1] 51.6356 1.1356 + .0101 * -50 ## [1] 0.6306 Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.\nFirst, for the intercept (\\(\\beta_{0}\\)), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.\nThe slope, (\\(\\beta_{1}\\)), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.\nThe predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)  What about the error? So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?\nThe short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, \\(\\sigma^2\\). Sometimes this is written in math notation as:\n\\[ \\epsilon \\sim N(0, \\sigma^2) \\]\nFrom this notation, can you see why the error was disregarded earlier when generating predictions?\nIn short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.\nEstimating error in linear regression This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the sum of squares regression (SSR), the portion that is unexplained is referred to as the sum of squares error (SSE). This could be written in math notation as:\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (Y - \\hat{Y})^2 + \\sum (\\hat{Y} - \\bar{Y})^2 \\]\nLet’s try to visualize what this means.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FF7F7F\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#65a765\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FFD580\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)   Another related measure of error Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.\n\\[ R^{2} = 1 - \\frac{SS_{res}}{SS_{total}} \\]\nor\n\\[ R^{2} = \\frac{SS_{reg}}{SS_{total}} \\]\nLet’s compute the sum of square and get a value for \\(R^2\\).\nbasketball %\u0026gt;% summarise(ss_total = sum((avg_points - mean(avg_points))^2), ss_error = sum((avg_points - fitted(wnba_reg))^2), ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %\u0026gt;% mutate(r_square = 1 - ss_error / ss_total, r_square2 = ss_reg / ss_total) ## # A tibble: 1 × 5 ## ss_total ss_error ss_reg r_square r_square2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004. 564. 1440. 0.719 0.719 summary(wnba_reg)$r.square ## [1] 0.7185315 summary(wnba_reg)$sigma ## [1] 1.979045 sigma_hat_square \u0026lt;- 563.9929 / (nrow(basketball) - 2) sigma_hat \u0026lt;- sqrt(sigma_hat_square) sigma_hat_square ## [1] 3.916617 sigma_hat ## [1] 1.979045   ","date":1692748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692748800,"objectID":"1bd811c01f9ed8aa0de49dc4fbae8a29","permalink":"https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/","publishdate":"2023-08-23T00:00:00Z","relpermalink":"/lectures/02-linear-regression/","section":"lectures","summary":"Introduction to Linear Regression This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes.","tags":null,"title":"Introduction to Linear Regression","type":"book"},{"authors":null,"categories":null,"content":"Introduction No Class.\n","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"52d024887264e25c0a808c70b28505ad","permalink":"https://psqf6243.brandonlebeau.org/content/02-week2/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/content/02-week2/","section":"content","summary":"TBD","tags":null,"title":"Week 2","type":"book"},{"authors":null,"categories":null,"content":"  Introduction to Linear Regression - From class This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes. One of those attributes will be the outcome or attribute of interest whereas the other will used as a predictor. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable explains variation in the dependent variable (more on this later).\nOf note, variable is a typical word used in statistics, I’ve come to like the word attribute instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.\nWe may write this general model as:\n\\[ Y = \\beta_{0} + \\beta_{1} X + \\epsilon \\]\nWhere \\(Y\\) is the outcome attribute. It is also known as the dependent variable. The \\(X\\) term is the predictor/covariate attribute. It is also known as the independent variable. The \\(\\epsilon\\) is a random error term, more on this later. Finally, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown population coefficients that we are interested in estimating. More on this later too.\nSpecific example The data used for this section of the course is from the 2019 WNBA season. These data are part of the bayesrules package/book. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum library(ggformula) basketball \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv\u0026quot;) ## Rows: 146 Columns: 32 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): player_name, team ## dbl (29): height, weight, year, age, games_played, games_started, avg_minute... ## lgl (1): starter ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. theme_set(theme_bw(base_size = 18)) head(basketball) ## # A tibble: 6 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, …  Guiding Question  Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season.  That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.\nOne first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.\nThe next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) \\[ SD = \\sqrt{\\frac{(X - \\bar{X})}{n - 1}} \\]\nQuestions to consider What can be noticed about the relationship between these two attributes? Does there appear to be a relationship between the two? Is this relationship perfect?    Adding a smoother line Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.\nI’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.\nDoes there appear to be much difference in the relationship across the two lines?\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) |\u0026gt; gf_smooth(linewidth = 2, color = \u0026#39;black\u0026#39;) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;, linewidth = 2) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;, linewidth = 2) |\u0026gt; gf_hline(yintercept = mean(basketball$avg_points), linewidth = 2) |\u0026gt; gf_vline(xintercept = mean(basketball$total_minutes), linewidth = 2) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) \\[ (X - \\bar{X}) ( Y - \\bar{Y}) \\]\ncor(avg_points ~ total_minutes, data = basketball) |\u0026gt; round(3) ## [1] 0.848  Estimating linear regression coefficients The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is lm() to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.\nThis equation could be written more directly for our specific problem.\n\\[ Avg\\_points = \\beta_{0} + \\beta_{1} Minutes\\_Played + \\epsilon \\]\nOne way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.\nFor the R formula, instead of an $ = $, you could insert a ~.\nwnba_reg \u0026lt;- lm(avg_points ~ total_minutes, data = basketball) coef(wnba_reg) |\u0026gt; round(4) ## (Intercept) total_minutes ## 1.1356 0.0101  Interpretting linear regression terms Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:\n\\[ \\hat{avg\\_points} = 1.1356 + .0101 min\\_played \\]\nWhere instead of \\(\\beta_{0}\\) or \\(\\beta_{1}\\), the estimated values from this single season were inserted. Note the \\(\\hat{avg\\_points}\\), which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.\n1.1356 + .0101 * 0 ## [1] 1.1356 1.1356 + .0101 * 1 ## [1] 1.1457 1.1356 + .0101 * 100 ## [1] 2.1456 1.1356 + .0101 * mean(basketball$total_minutes) ## [1] 5.342042 1.1356 + .0101 * 5000 ## [1] 51.6356 1.1356 + .0101 * -50 ## [1] 0.6306 Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.\nFirst, for the intercept (\\(\\beta_{0}\\)), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.\nThe slope, (\\(\\beta_{1}\\)), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.\nThe predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.\ngf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;blue\u0026#39;, linewidth = 2) %\u0026gt;% gf_vline(xintercept = 0, linewidth = 4) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) cor(avg_points ~ I(total_minutes / 100), data = basketball) |\u0026gt; round(3) ## [1] 0.848 coef( lm(avg_points ~ I(total_minutes / 100), data = basketball) ) |\u0026gt; round(3) ## (Intercept) I(total_minutes/100) ## 1.136 1.014 gf_point(avg_points ~ I(total_minutes / 100), data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;blue\u0026#39;, linewidth = 2) %\u0026gt;% gf_vline(xintercept = 0, linewidth = 4) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played (by 100)\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) head(basketball) ## # A tibble: 6 × 32 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 24 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, … basketball \u0026lt;- basketball |\u0026gt; mutate(total_minutes_100 = total_minutes / 100) head(basketball) ## # A tibble: 6 × 33 ## player_name height weight year team age games_played games_started ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Natalie Achonwa 75 190 2019 IND 26 30 18 ## 2 Kayla Alexander 76 195 2019 CHI 28 3 0 ## 3 Rebecca Allen 74 162 2019 NYL 26 24 2 ## 4 Jillian Alleyne 74 193 2019 MIN 24 5 0 ## 5 Kristine Anigwe 76 200 2019 TOT 22 27 0 ## 6 Kristine Anigwe 76 200 2019 CON 22 17 0 ## # ℹ 25 more variables: avg_minutes_played \u0026lt;dbl\u0026gt;, avg_field_goals \u0026lt;dbl\u0026gt;, ## # avg_field_goal_attempts \u0026lt;dbl\u0026gt;, field_goal_pct \u0026lt;dbl\u0026gt;, ## # avg_three_pointers \u0026lt;dbl\u0026gt;, avg_three_pointer_attempts \u0026lt;dbl\u0026gt;, ## # three_pointer_pct \u0026lt;dbl\u0026gt;, avg_two_pointers \u0026lt;dbl\u0026gt;, ## # avg_two_pointer_attempts \u0026lt;dbl\u0026gt;, two_pointer_pct \u0026lt;dbl\u0026gt;, ## # avg_free_throws \u0026lt;dbl\u0026gt;, avg_free_throw_attempts \u0026lt;dbl\u0026gt;, free_throw_pct \u0026lt;dbl\u0026gt;, ## # avg_offensive_rb \u0026lt;dbl\u0026gt;, avg_defensive_rb \u0026lt;dbl\u0026gt;, avg_rb \u0026lt;dbl\u0026gt;, … lm(avg_points ~ total_minutes_100, data = basketball) |\u0026gt; coef() |\u0026gt; round(3) ## (Intercept) total_minutes_100 ## 1.136 1.014 coef( lm(avg_points ~ I(total_minutes - 100), data = basketball) ) |\u0026gt; round(3) ## (Intercept) I(total_minutes - 100) ## 2.15 0.01 gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;blue\u0026#39;, linewidth = 2) %\u0026gt;% gf_hline(yintercept = mean(basketball$avg_points), linewidth = 4) |\u0026gt; gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)  What about the error? So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?\nThe short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, \\(\\sigma^2\\). Sometimes this is written in math notation as:\n\\[ \\epsilon \\sim N(0, \\sigma^2) \\]\nFrom this notation, can you see why the error was disregarded earlier when generating predictions?\nIn short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.\nEstimating error in linear regression This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the sum of squares regression (SSR), the portion that is unexplained is referred to as the sum of squares error (SSE). This could be written in math notation as:\n\\[ \\sum (Y - \\bar{Y})^2 = \\sum (Y - \\hat{Y})^2 + \\sum (\\hat{Y} - \\bar{Y})^2 \\]\nLet’s try to visualize what this means.\nhead(basketball$avg_points) ## [1] 8.7 3.0 7.2 0.8 2.4 2.0 mean(basketball$avg_points) ## [1] 5.359589 8.7 - 5.4 ## [1] 3.3 3 - 5.4 ## [1] -2.4 7.2 - 5.4 ## [1] 1.8 0.8 - 5.4 ## [1] -4.6 with(basketball, sum((avg_points - mean(avg_points))^2) ) ## [1] 2003.752 sd(basketball$avg_points) ## [1] 3.717388 gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FF7F7F\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#65a765\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;) gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %\u0026gt;% gf_hline(yintercept = ~mean(avg_points), data = basketball) %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, linetype = 2, color = \u0026#39;lightblue\u0026#39;) %\u0026gt;% gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = \u0026#39;#FFD580\u0026#39;) %\u0026gt;% gf_labs(x = \u0026quot;Total Minutes Played\u0026quot;, y = \u0026quot;Average Points Scored\u0026quot;)   Another related measure of error Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.\n\\[ R^{2} = 1 - \\frac{SS_{res}}{SS_{total}} \\]\nor\n\\[ R^{2} = \\frac{SS_{reg}}{SS_{total}} \\]\nLet’s compute the sum of square and get a value for \\(R^2\\).\n\\[ \\epsilon \\sim N(0, \\sigma^2) \\]\n\\[ \\hat{\\sigma^2} = \\frac{\\sum (points - \\hat{points})^2}{( n - 2 )} \\]\n\\[ \\hat{\\sigma^2} = \\frac{SSE}{( n - 2 )} \\]\n\\[ \\hat{\\sigma} = \\sqrt{ \\frac{SSE}{( n - 2 )}} \\]\nbasketball %\u0026gt;% summarise(ss_total = sum((avg_points - mean(avg_points))^2), ss_error = sum((avg_points - fitted(wnba_reg))^2), ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %\u0026gt;% mutate(r_square = 1 - ss_error / ss_total, r_square2 = ss_reg / ss_total) ## # A tibble: 1 × 5 ## ss_total ss_error ss_reg r_square r_square2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2004. 564. 1440. 0.719 0.719 summary(wnba_reg)$r.square ## [1] 0.7185315 summary(wnba_reg)$sigma ## [1] 1.979045 sigma_hat_square \u0026lt;- 563.9929 / (nrow(basketball) - 2) sigma_hat \u0026lt;- sqrt(sigma_hat_square) sigma_hat_square ## [1] 3.916617 sigma_hat ## [1] 1.979045 sd(basketball$avg_points) ## [1] 3.717388   ","date":1692748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692748800,"objectID":"54d97236e4e003862f878309c214a01b","permalink":"https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-class/","publishdate":"2023-08-23T00:00:00Z","relpermalink":"/lectures/02-linear-regression-class/","section":"lectures","summary":"Introduction to Linear Regression - From class This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 continuous attributes.","tags":null,"title":"Introduction to Linear Regression -class","type":"book"},{"authors":null,"categories":null,"content":"  In Class Activity - February 1st, 2024 This activity is meant to give you some exploration of topics within class. I’ve created the code for you to run the activity and explore some data.\nThe goals of this activity are as follows:\nExplore what happens if the X and Y attributes are flipped in a linear regression.  What happens to the linear regression coefficient estimates (ie., the \\(\\hat{\\beta}\\))? What happens to the sigma and R-Square statistics?  Given what you find in #1, how do you decide which attribute should be an outcome (Y) vs a predictor (X)?  library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(palmerpenguins) library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 16)) # If you get errors, use this line of code too. # penguins \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv\u0026quot;) head(penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## # ℹ 2 more variables: sex \u0026lt;fct\u0026gt;, year \u0026lt;int\u0026gt; model_fit \u0026lt;- function(outcome, predictor, data = penguins) { formula \u0026lt;- as.formula(paste(outcome, predictor, sep = \u0026quot;~\u0026quot;)) model_out \u0026lt;- lm(formula, data = data) model_coef \u0026lt;- data.frame(matrix(c(coef(model_out)), ncol = 2)) names(model_coef) \u0026lt;- c(\u0026quot;Intercept\u0026quot;, \u0026quot;Slope\u0026quot;) data.frame(model_coef, Rsquare = summary(model_out)$r.square, sigma = summary(model_out)$sigma) } visualize_relationship \u0026lt;- function(outcome, predictor, data = penguins, add_regression_line = TRUE, add_smoother_line = FALSE) { formula \u0026lt;- as.formula(paste(outcome, predictor, sep = \u0026quot;~\u0026quot;)) if(add_regression_line \u0026amp; !add_smoother_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; print() } if(add_smoother_line \u0026amp; !add_regression_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5) |\u0026gt; print() } if(add_smoother_line \u0026amp; add_regression_line) { gf_point(gformula = formula, data = data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;, size = 1.5) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 1.5, linetype = 2) |\u0026gt; print() } }  Compute Correlation The following code chunk can help you compute correlations between attributes. To use the function, you can replace “outcome” with an attribute name from the data above and “predictor” with another attribute above.\nWhat happens when you flip the outcome / predictor outcomes when computing the correlation? Does the correlation change? Why or why not? Given the correlation computed, how is it interpreted? Given the correlation computed, what information would this tell us when we try estimate the regression coefficients below?  cor(outcome ~ predictor, data = penguins, use = \u0026#39;complete.obs\u0026#39;) |\u0026gt; round(3)  Visualize bivariate distribution The following code creates a scatter plot showing the bivariate association between the two attributes entered. Example code is shown below as an example. You can replace the “outcome” and “predictor” with the two continuous attributes that you are interested in exploring. These need to be entered in quotations, either single or double are fine. You can also add a regression line or smoother line by specifying those arguments as either TRUE (ie., Yes) or FALSE (ie., No).\nDoes the association between the two attributes appear to be linear? What happens to the association if you flip the predictor / outcome attributes? How would you summarize the association in a few sentences?  visualize_relationship(outcome = \u0026#39;body_mass_g\u0026#39;, predictor = \u0026#39;flipper_length_mm\u0026#39;, data = penguins, add_regression_line = TRUE, add_smoother_line = FALSE) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## Warning: Removed 2 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 2 rows containing missing values (`geom_point()`). visualize_relationship(outcome = \u0026#39;flipper_length_mm\u0026#39;, predictor = \u0026#39;body_mass_g\u0026#39;, data = penguins, add_regression_line = TRUE, add_smoother_line = FALSE) ## Warning: Removed 2 rows containing non-finite values (`stat_smooth()`). ## Warning: Removed 2 rows containing missing values (`geom_point()`).  Linear Regression Fitting Similar to the bivariate scatterplot, the following function was created to fit a linear regression and extract some information about the model. The output should include the Intercept, Slope, R-square, and sigma estimates. You can specify the outcome and predictor by replacing those as you did in the bivariate scatterplot above to reflect the attributes you are interested in exploring.\nHow are the 4 estimates interpreted, particularly in the context of the problem? What happens to the 4 estimates if you flip the outcome and predictor attributes?  Which one should truly be the outcome and what should guide this?   model_fit(outcome = \u0026#39;body_mass_g\u0026#39;, predictor = \u0026#39;flipper_length_mm\u0026#39;, data = penguins) ## Intercept Slope Rsquare sigma ## 1 -5780.831 49.68557 0.7589925 394.2782 \\[ body\\_mass = \\beta_{0} + \\beta_{1} flipper\\_length \\]\nmodel_fit(outcome = \u0026#39;flipper_length_mm\u0026#39;, predictor = \u0026#39;body_mass_g\u0026#39;, data = penguins) ## Intercept Slope Rsquare sigma ## 1 136.7296 0.01527592 0.7589925 6.913393 \\[ flipper\\_length = \\beta_{0} + \\beta_{1} body\\_mass \\]\n ","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"730e6344c2a80c627178c5d190900d5b","permalink":"https://psqf6243.brandonlebeau.org/lectures/02a-in-class-activity/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/lectures/02a-in-class-activity/","section":"lectures","summary":"In Class Activity - February 1st, 2024 This activity is meant to give you some exploration of topics within class. I’ve created the code for you to run the activity and explore some data.","tags":null,"title":"Linear Regression - Activity","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.\nObjectives  Recognize a linear regression formula Interpret regression parameters Specify a linear regression in statistical software  Activities  Read Chapter 1 of Applied Regression: An Introduction Engage with course notes. Optional, Read Chapter 7 of Statistical Reasoning through Computation and R Optional, Read 7.1 and 7.2 of Introduction to Modern Statistics  Assignments  Quiz 1 posted shortly (review materials) Practice Problem 1 posted soon (practice fitting linear regression)  ","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"a45a23bf966f2b7c885958051528505d","permalink":"https://psqf6243.brandonlebeau.org/content/03-week3/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/content/03-week3/","section":"content","summary":"TBD","tags":null,"title":"Week 3","type":"book"},{"authors":null,"categories":null,"content":"Introduction This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.\nObjectives  Define a residual or error from a regression model. Explore benefits of centering predictor attributes. Engage with linear regression parameter estimation.  Activities  Read Chapter 1 of Applied Regression: An Introduction Engage with course notes. Optional, Read Chapter 7 of Statistical Reasoning through Computation and R Optional, Read 7.2 of Introduction to Modern Statistics  Assignments  Quiz 1 to come soon Practice problem 1 posted soon. Practice problem 2 posted soon.  ","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662508800,"objectID":"e315f03cac4ec7010fcc961beef42232","permalink":"https://psqf6243.brandonlebeau.org/content/04-week4/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/content/04-week4/","section":"content","summary":"TBD","tags":null,"title":"Week 4","type":"book"},{"authors":null,"categories":null,"content":"  Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean. We will consider the estimation formulas in part of this to gain a sense of how these can be computed.\nNew Example Data The new data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\n  Bivariate Figure Note, below I do a bit of post-processing to combine data from different POC values within a single CBSA.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality %\u0026gt;% left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) %\u0026gt;% drop_na() ## Warning in left_join(., wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. head(airquality) ## # A tibble: 6 × 13 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 5 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt; dim(airquality) ## [1] 4821 13 gf_point(daily_aqi ~ avg_wind, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; cor(daily_aqi ~ avg_wind, data = airquality) |\u0026gt; round(3) ## [1] -0.292 air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) |\u0026gt; round(3) ## (Intercept) avg_wind ## 48.223 -2.212 summary(air_lm)$r.square |\u0026gt; round(3) ## [1] 0.085 summary(air_lm)$sigma |\u0026gt; round(3) ## [1] 18.055 Interpreting these estimates What do these parameter estimates mean in this context?\nIntercept: This is the model implied ______ when the ________ equals 0.\nSlope: For each 1 unit change in ____ there is a -2.2 unit decrease in .\nR-Square: The __ in ____ is explained by ____.\nSigma: The _____ distance each point is from ____.\n  Centering predictors There are times when centering of predictors can be helpful for interpretation of the model parameters. This can be helpful when 0 is not a practically useful characteristic of the attribute or for more specific tests of certain elements of the X attribute.\nMean Centering Mean centering is where the mean of the attribute is subtracted from each value. This is a linear transformation where each data point is subtracted by a constant, the mean. This means that the distance between points do not change.\nairquality \u0026lt;- airquality %\u0026gt;% mutate(avg_wind_mc = avg_wind - mean(avg_wind), avg_wind_maxc = avg_wind - max(avg_wind), avg_wind_10 = avg_wind - 10) gf_point(daily_aqi ~ avg_wind_mc, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; air_lm_mc \u0026lt;- lm(daily_aqi ~ avg_wind_mc, data = airquality) coef(air_lm_mc) ## (Intercept) avg_wind_mc ## 38.788011 -2.211798 summary(air_lm_mc)$r.square ## [1] 0.08528019 summary(air_lm_mc)$sigma ## [1] 18.05479 air_lm_maxc \u0026lt;- lm(daily_aqi ~ avg_wind_maxc, data = airquality) coef(air_lm_maxc) ## (Intercept) avg_wind_maxc ## 5.968391 -2.211798 summary(air_lm_maxc)$r.square ## [1] 0.08528019 summary(air_lm_maxc)$sigma ## [1] 18.05479 air_lm_10 \u0026lt;- lm(daily_aqi ~ avg_wind_10, data = airquality) coef(air_lm_10) ## (Intercept) avg_wind_10 ## 26.104968 -2.211798 summary(air_lm_10)$r.square ## [1] 0.08528019 summary(air_lm_10)$sigma ## [1] 18.05479   Standardized Regression Another type of regression that can be done is one in which the attributes are standardized prior to estimating the linear regression. What is meant by standardizing? This is converting the attributes into z-scores:\n\\[ Z_{api} = \\frac{(aqi - \\bar{aqi})}{s_{aqi}} \\]\nairquality \u0026lt;- airquality %\u0026gt;% mutate(z_aqi = scale(daily_aqi), z_aqi2 = (daily_aqi - mean(daily_aqi)) / sd(daily_aqi), z_wind = scale(avg_wind)) head(airquality) ## # A tibble: 6 × 19 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 11 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, avg_wind_mc \u0026lt;dbl\u0026gt;, ## # avg_wind_maxc \u0026lt;dbl\u0026gt;, avg_wind_10 \u0026lt;dbl\u0026gt;, z_aqi \u0026lt;dbl[,1]\u0026gt;, z_aqi2 \u0026lt;dbl\u0026gt;, ## # z_wind \u0026lt;dbl[,1]\u0026gt; air_lm_s \u0026lt;- lm(z_aqi ~ z_wind, data = airquality) coef(air_lm_s) ## (Intercept) z_wind ## -6.712023e-16 -2.920277e-01 summary(air_lm_s)$r.square ## [1] 0.08528019 summary(air_lm_s)$sigma ## [1] 0.9565091 We can also use this formula to convert any unstandardized regression coefficients into a standardized metric.\n\\[ b^{\u0026#39;}_{k} = b_{k} * \\frac{s_{x_{k}}}{s_{y}} \\]\n-2.211 * sd(airquality$avg_wind) / sd(airquality$daily_aqi) ## [1] -0.2919224 cor(daily_aqi ~ avg_wind, data = airquality) ## [1] -0.2920277  Parameter Estimation Now that we looked how the parameters are impacted by some changes in the model specification, how are these parameters actually estimated? I will show two ways, one is general, the other is specific to this simple case with a single predictor/covariate attribute. In general, linear regression (or more generally the general linear model) uses least square estimation. This means that the the parameters in the model minimize the squared distance between the observed and predicted values. That is, least squares estimates minimize this criterion:\n\\[ \\sum (Y - \\hat{Y})^2 \\]\nSpecific example Calculus can be used to show that these two equations can be solved simultanuously to get estimates for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimize the criterion above. These formulas are:\n\\[ b_{1} = \\frac{\\sum(X - \\bar{X})(Y - \\bar{Y})}{\\sum(X - \\bar{X})^2} \\] \\[ b_{0} = \\bar{Y} - b_{1}\\bar{X} \\]\nLet’s use R to get these quantities.\nb1 \u0026lt;- with(airquality, sum((avg_wind - mean(avg_wind)) * (daily_aqi - mean(daily_aqi))) / sum((avg_wind - mean(avg_wind))^2) ) b0 \u0026lt;- with(airquality, mean(daily_aqi) - b1 * mean(avg_wind) ) b0 ## [1] 48.22295 b1 ## [1] -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798  General Approach When there are more than one predictor, the number of equations gets a bit unyieldy, therefore, there is a general analytic approach that works for any set of predictor attributes. The general approach uses matrix algebra (anyone take linear algebra?), to achieve their estimates. This general form is:\n\\[ \\mathbf{b} = \\left( \\mathbf{X}^{`}\\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{`} \\mathbf{Y} \\right). \\] Where \\(\\mathbf{b}\\) is a vector of estimated regression coefficients, \\(\\mathbf{X}\\) is a matrix of covariate/predictor attributes (called the design matrix), and \\(\\mathbf{Y}\\) is a vector of the outcome attribute.\nBelow, I show what these would look like for the air quality example that has been used and solve for the regression coefficients.\nX \u0026lt;- model.matrix(air_lm) head(X) ## (Intercept) avg_wind ## 1 1 2.941667 ## 2 1 2.445833 ## 3 1 1.995833 ## 4 1 3.445833 ## 5 1 1.116667 ## 6 1 6.091667 Y \u0026lt;- as.matrix(airquality$daily_aqi) head(Y) ## [,1] ## [1,] 57 ## [2,] 54 ## [3,] 69 ## [4,] 56 ## [5,] 54 ## [6,] 22 X_X \u0026lt;- solve(t(X) %*% X) X_X ## (Intercept) avg_wind ## (Intercept) 0.0008152474 -1.424894e-04 ## avg_wind -0.0001424894 3.340328e-05 X_Y \u0026lt;- t(X) %*% Y X_Y ## [,1] ## (Intercept) 186997 ## avg_wind 731464 X_X %*% X_Y ## [,1] ## (Intercept) 48.222946 ## avg_wind -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798    ","date":1707350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707350400,"objectID":"4f3ecf68d62b085e025f260de84450f2","permalink":"https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/","publishdate":"2024-02-08T00:00:00Z","relpermalink":"/lectures/03-regression-estimates/","section":"lectures","summary":"Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean.","tags":null,"title":"Regression Estimates","type":"book"},{"authors":null,"categories":null,"content":"Introduction Digging deeper into linear regression. Time will be spent to show/prove via simulation the least square criterion that is minimized by the regression estimates. The foundation for inference using classical statistical methods will be explored.\nObjectives  Understand the least squares minimization for regression. Explore inference for regression parameters. Define what a standard error is. Define null hypothesis significance testing (NHST)  Activities  Read Chapter 2 of Applied Regression: An Introduction Read Chapter 3, section 1 of Introduction to Statistical Learning Engage with course notes. Optional, Read 24.4 of Introduction to Modern Statistics  Assignments  Quiz 1 Quiz 2 Quiz 3 Practice Problem 1 - Coming soon. Practice Problem 2 - coming soon.  ","date":1662508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662508800,"objectID":"7b26937bb8dacd1f2a59f61a2d255d6b","permalink":"https://psqf6243.brandonlebeau.org/content/05-week5/","publishdate":"2022-09-07T00:00:00Z","relpermalink":"/content/05-week5/","section":"content","summary":"TBD","tags":null,"title":"Week 5","type":"book"},{"authors":null,"categories":null,"content":"  Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean. We will consider the estimation formulas in part of this to gain a sense of how these can be computed.\nNew Example Data The new data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\n  Bivariate Figure Note, below I do a bit of post-processing to combine data from different POC values within a single CBSA.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality %\u0026gt;% left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) %\u0026gt;% drop_na() ## Warning in left_join(., wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. head(airquality) ## # A tibble: 6 × 13 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 5 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt; dim(airquality) ## [1] 4821 13 gf_density(~ daily_aqi, data = airquality) gf_point(daily_aqi ~ avg_wind, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; cor(daily_aqi ~ avg_wind, data = airquality) |\u0026gt; round(3) ## [1] -0.292 air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) |\u0026gt; round(3) ## (Intercept) avg_wind ## 48.223 -2.212 summary(air_lm)$r.square |\u0026gt; round(3) ## [1] 0.085 summary(air_lm)$sigma |\u0026gt; round(3) ## [1] 18.055 Interpreting these estimates What do these parameter estimates mean in this context?\nIntercept: This is the model implied ______ when the ________ equals 0.\nSlope: For each 1 unit change in ____ there is a -2.2 unit decrease in .\nR-Square: The __ in ____ is explained by ____.\nSigma: The _____ distance each point is from ____.\n  Centering predictors There are times when centering of predictors can be helpful for interpretation of the model parameters. This can be helpful when 0 is not a practically useful characteristic of the attribute or for more specific tests of certain elements of the X attribute.\nMean Centering Mean centering is where the mean of the attribute is subtracted from each value. This is a linear transformation where each data point is subtracted by a constant, the mean. This means that the distance between points do not change.\nairquality \u0026lt;- airquality %\u0026gt;% mutate(avg_wind_mc = avg_wind - mean(avg_wind), avg_wind_maxc = avg_wind - max(avg_wind), avg_wind_10 = avg_wind - 10) gf_point(daily_aqi ~ avg_wind_mc, data = airquality, size = 4, alpha = .15) %\u0026gt;% gf_labs(x = \u0026quot;Average daily wind speed (in knots)\u0026quot;, y = \u0026quot;Daily Air Quality\u0026quot;) %\u0026gt;% gf_smooth() %\u0026gt;% gf_smooth(method = \u0026#39;lm\u0026#39;, color = \u0026#39;lightblue\u0026#39;, linetype = 2) ## `geom_smooth()` using method = \u0026#39;gam\u0026#39; air_lm_mc \u0026lt;- lm(daily_aqi ~ avg_wind_mc, data = airquality) coef(air_lm_mc) ## (Intercept) avg_wind_mc ## 38.788011 -2.211798 summary(air_lm_mc)$r.square ## [1] 0.08528019 summary(air_lm_mc)$sigma ## [1] 18.05479 air_lm_maxc \u0026lt;- lm(daily_aqi ~ avg_wind_maxc, data = airquality) coef(air_lm_maxc) ## (Intercept) avg_wind_maxc ## 5.968391 -2.211798 summary(air_lm_maxc)$r.square ## [1] 0.08528019 summary(air_lm_maxc)$sigma ## [1] 18.05479 air_lm_10 \u0026lt;- lm(daily_aqi ~ avg_wind_10, data = airquality) coef(air_lm_10) ## (Intercept) avg_wind_10 ## 26.104968 -2.211798 summary(air_lm_10)$r.square ## [1] 0.08528019 summary(air_lm_10)$sigma ## [1] 18.05479   Standardized Regression Another type of regression that can be done is one in which the attributes are standardized prior to estimating the linear regression. What is meant by standardizing? This is converting the attributes into z-scores:\n\\[ Z_{api} = \\frac{(aqi - \\bar{aqi})}{s_{aqi}} \\]\nairquality \u0026lt;- airquality %\u0026gt;% mutate(z_aqi = scale(daily_aqi), z_aqi2 = (daily_aqi - mean(daily_aqi)) / sd(daily_aqi), z_wind = scale(avg_wind)) head(airquality) ## # A tibble: 6 × 19 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 11 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, avg_wind_mc \u0026lt;dbl\u0026gt;, ## # avg_wind_maxc \u0026lt;dbl\u0026gt;, avg_wind_10 \u0026lt;dbl\u0026gt;, z_aqi \u0026lt;dbl[,1]\u0026gt;, z_aqi2 \u0026lt;dbl\u0026gt;, ## # z_wind \u0026lt;dbl[,1]\u0026gt; air_lm_s \u0026lt;- lm(z_aqi ~ z_wind, data = airquality) coef(air_lm_s) ## (Intercept) z_wind ## -6.712023e-16 -2.920277e-01 summary(air_lm_s)$r.square ## [1] 0.08528019 summary(air_lm_s)$sigma ## [1] 0.9565091 We can also use this formula to convert any unstandardized regression coefficients into a standardized metric.\n\\[ b^{\u0026#39;}_{k} = b_{k} * \\frac{s_{x_{k}}}{s_{y}} \\]\n-2.211 * sd(airquality$avg_wind) / sd(airquality$daily_aqi) ## [1] -0.2919224 cor(daily_aqi ~ avg_wind, data = airquality) ## [1] -0.2920277  Parameter Estimation Now that we looked how the parameters are impacted by some changes in the model specification, how are these parameters actually estimated? I will show two ways, one is general, the other is specific to this simple case with a single predictor/covariate attribute. In general, linear regression (or more generally the general linear model) uses least square estimation. This means that the the parameters in the model minimize the squared distance between the observed and predicted values. That is, least squares estimates minimize this criterion:\n\\[ \\sum (Y - \\hat{Y})^2 \\]\nSpecific example Calculus can be used to show that these two equations can be solved simultanuously to get estimates for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimize the criterion above. These formulas are:\n\\[ b_{1} = \\frac{\\sum(X - \\bar{X})(Y - \\bar{Y})}{\\sum(X - \\bar{X})^2} \\] \\[ b_{0} = \\bar{Y} - b_{1}\\bar{X} \\]\nLet’s use R to get these quantities.\nb1 \u0026lt;- with(airquality, sum((avg_wind - mean(avg_wind)) * (daily_aqi - mean(daily_aqi))) / sum((avg_wind - mean(avg_wind))^2) ) b0 \u0026lt;- with(airquality, mean(daily_aqi) - b1 * mean(avg_wind) ) b0 ## [1] 48.22295 b1 ## [1] -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798  General Approach When there are more than one predictor, the number of equations gets a bit unyieldy, therefore, there is a general analytic approach that works for any set of predictor attributes. The general approach uses matrix algebra (anyone take linear algebra?), to achieve their estimates. This general form is:\n\\[ \\mathbf{b} = \\left( \\mathbf{X}^{`}\\mathbf{X} \\right)^{-1} \\left( \\mathbf{X}^{`} \\mathbf{Y} \\right). \\] Where \\(\\mathbf{b}\\) is a vector of estimated regression coefficients, \\(\\mathbf{X}\\) is a matrix of covariate/predictor attributes (called the design matrix), and \\(\\mathbf{Y}\\) is a vector of the outcome attribute.\nBelow, I show what these would look like for the air quality example that has been used and solve for the regression coefficients.\nX \u0026lt;- model.matrix(air_lm) head(X) ## (Intercept) avg_wind ## 1 1 2.941667 ## 2 1 2.445833 ## 3 1 1.995833 ## 4 1 3.445833 ## 5 1 1.116667 ## 6 1 6.091667 Y \u0026lt;- as.matrix(airquality$daily_aqi) head(Y) ## [,1] ## [1,] 57 ## [2,] 54 ## [3,] 69 ## [4,] 56 ## [5,] 54 ## [6,] 22 X_X \u0026lt;- solve(t(X) %*% X) X_X ## (Intercept) avg_wind ## (Intercept) 0.0008152474 -1.424894e-04 ## avg_wind -0.0001424894 3.340328e-05 X_Y \u0026lt;- t(X) %*% Y X_Y ## [,1] ## (Intercept) 186997 ## avg_wind 731464 X_X %*% X_Y ## [,1] ## (Intercept) 48.222946 ## avg_wind -2.211798 coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798    ","date":1707350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707350400,"objectID":"2539b33531a807d54d7eeb9b3fa747e4","permalink":"https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates-class/","publishdate":"2024-02-08T00:00:00Z","relpermalink":"/lectures/03-regression-estimates-class/","section":"lectures","summary":"Understanding Regression Parameters This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean.","tags":null,"title":"Regression Estimates - In class","type":"book"},{"authors":null,"categories":null,"content":"Introduction Digging deeper into linear regression inference using classical procedures. Further definitions of confidence intervals, sampling distribution, and interpretations of p-values. Regression conditions for valid inferences. Topics will explore evaluation of residuals for checking of conditions.\nObjectives  Construct a confidence interval Define the sampling distribution Perform significance testing Interpret p-value as a continuous probability Define what a residual is Define common statistical conditions for linear regression Interpret residual plots  Activities  Read Chapter 2 of Applied Regression: An Introduction Read Chapter 3, section 1 of Introduction to Statistical Learning Engage with course notes. Optional, Read 24.4 of Introduction to Modern Statistics Section 11.1 of Regression and Other Stories Regression and Other Stories. Optional, Read ASA statement on p-values  Assignments  Quiz 1 Quiz 2 Quiz 3 Quiz 4 - coming soon. Practice Problem 1 - Coming soon. Practice Problem 2 - coming soon. Practice Problem 3 - coming soon.  ","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695254400,"objectID":"a1ba50a25029964cc75a26ac15871b71","permalink":"https://psqf6243.brandonlebeau.org/content/06-week6/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/content/06-week6/","section":"content","summary":"TBD","tags":null,"title":"Week 6","type":"book"},{"authors":null,"categories":null,"content":"  Example to show least squares minimization This little example is meant as a way to show the least square really minimizes the criterion, $ ( Y - )^2 $.\nIn this example, we will generate some data so that we know what the truth is. Then, upon data generation, we will compute a bunch of different values for the linear slope and y-intercept. For each combination of the y-intercept and slope, I will compute the sum of squares error depicted above.\nSimulate some data The following example simulates data based on the following linear regression formula:\n\\[ Y = 5 + 0.5 X + \\epsilon \\]\nMore explicitly, the simulation allows us to specify what the intercept and slope is in the population. These are specified below in the reg_weights simulation argument.\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(simglm) theme_set(theme_bw(base_size = 18)) set.seed(2023) sim_arguments \u0026lt;- list( formula = y ~ x, fixed = list(x = list(var_type = \u0026#39;continuous\u0026#39;, mean = 100, sd = 20)), error = list(variance = 100), sample_size = 1000, reg_weights = c(5, .5) ) sim_data \u0026lt;- simulate_fixed(data = NULL, sim_arguments) |\u0026gt; simulate_error(sim_arguments) |\u0026gt; generate_response(sim_arguments) head(sim_data) ## X.Intercept. x level1_id error fixed_outcome random_effects ## 1 1 98.32431 1 -2.6862147 54.16216 0 ## 2 1 80.34113 2 -1.1886232 45.17056 0 ## 3 1 62.49865 3 0.6545456 36.24933 0 ## 4 1 96.27711 4 12.9091527 53.13855 0 ## 5 1 87.33029 5 -4.5144816 48.66514 0 ## 6 1 121.81595 6 19.5705749 65.90797 0 ## y ## 1 51.47594 ## 2 43.98194 ## 3 36.90387 ## 4 66.04771 ## 5 44.15066 ## 6 85.47855  Visualize the Simulated Data The following code visualizes the simulated data from above. What would you estimate the correlation to be?\nlibrary(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) gf_point(y ~ x, data = sim_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;)  Estimate Regression Coefficients Even though we know what truth is, there is error involved in the simulation process, therefore, the population values specified above will not equal the exact regression coefficients estimated. Below, we estimate what those regression coefficients are.\nsim_lm \u0026lt;- lm (y ~ x, data = sim_data) coef(sim_lm) ## (Intercept) x ## 5.1260093 0.5002076  Create different combinations of intercept and slope coefficients The following code generates a sequence of intercept and corresponding slope conditions. We will use these different values to estimate the sum of squares error shown at the top of the notes for each of these intercept and slope values to show that the regression estimates are optimal to minimize the sum of square error.\ny_intercept \u0026lt;- seq(0, 15, by = .25) slope \u0026lt;- seq(0, 1.5, by = .01) conditions \u0026lt;- rbind(expand.grid(y_intercept = y_intercept, slope = slope), coef(sim_lm)) tail(conditions) ## y_intercept slope ## 9207 14.000000 1.5000000 ## 9208 14.250000 1.5000000 ## 9209 14.500000 1.5000000 ## 9210 14.750000 1.5000000 ## 9211 15.000000 1.5000000 ## 9212 5.126009 0.5002076 dim(conditions) ## [1] 9212 2  Showing Two Combinations Here we visualize two possible slope conditions. Which one seems better for the data?\ngf_point(y ~ x, data = sim_data, size = 4) |\u0026gt; gf_smooth(method = \u0026#39;lm\u0026#39;) |\u0026gt; gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 1), linetype = 2, size = 2) |\u0026gt; gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 855), linetype = 2, color = \u0026#39;lightgreen\u0026#39;, size = 2) |\u0026gt; gf_refine(coord_cartesian(xlim = c(0, 160), ylim = c(0, 120))) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated.  Compute Sum of Squares Error The following code creates a new function that computes the sum of square error. The function takes two arguments, the combination of intercept and slope values and the simulated data. The output is the sigma or average error from the regression line. The first code chunk below performs the computation for a single condition. The second code chunk does it for all of the conditions.\nsum_square_error \u0026lt;- function(conditions, sim_data) { fitted \u0026lt;- conditions[[\u0026#39;y_intercept\u0026#39;]] + conditions[[\u0026#39;slope\u0026#39;]] * sim_data[[\u0026#39;x\u0026#39;]] deviation \u0026lt;- sim_data[[\u0026#39;y\u0026#39;]] - fitted sqrt((sum(deviation^2) / (nrow(sim_data) - 2))) } sum_square_error(conditions[1892, ], sim_data) ## [1] 26.25721 summary(sim_lm)$sigma ## [1] 9.738423 library(future) plan(multicore) ## Warning in supportsMulticoreAndRStudio(...): [ONE-TIME WARNING] Forked ## processing (\u0026#39;multicore\u0026#39;) is not supported when running R from RStudio because ## it is considered unstable. For more details, how to control forked processing ## or not, and how to silence this warning in future R sessions, see ## ?parallelly::supportsMulticore conditions$sse \u0026lt;- unlist(lapply(1:nrow(conditions), function(xx) sum_square_error(conditions[xx, ], sim_data))) head(conditions) ## y_intercept slope sse ## 1 0.00 0 56.72588 ## 2 0.25 0 56.48344 ## 3 0.50 0 56.24108 ## 4 0.75 0 55.99878 ## 5 1.00 0 55.75655 ## 6 1.25 0 55.51440   ","date":1707350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707350400,"objectID":"5207d81b053ace685d82a5546345fe22","permalink":"https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/","publishdate":"2024-02-08T00:00:00Z","relpermalink":"/lectures/04-least-squares-simulation/","section":"lectures","summary":"Example to show least squares minimization This little example is meant as a way to show the least square really minimizes the criterion, $ ( Y - )^2 $.","tags":null,"title":"Least Squares Simulation","type":"book"},{"authors":null,"categories":null,"content":"  Inference for regression parameters It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population. Therefore, a subset of the population (the sample) is used to estimate the population parameters that are of most interest. As such, our estimates come with error and this uncertainty we can quantify when making inference about the parameter estimates.\nIn this course, I plan to show you two ways to perform this inference. One of those frameworks will be the classical approach which uses classical statistical theory to estimate the amount of uncertainty in the parameter estimates. The second approach will use the bootstrap as a way to computationally estimate the uncertainty. The benefit of the bootstrap is that it comes with fewer assumptions than the classical approach. We will build up to these arguments.\nClassical Inferential Framework The classical inferential framework, sometimes referred to as the null hypothesis significance test (NHST) has been around for more than 100 years. This framework builds off of the idea of a null hypothesis.\nA null hypothesis is typically thought as a hypothesis that assumes there is no relationship or a null effect. Framing this in the regression concept that we have been working with, we could define the following null hypotheses.\n\\[ H_{0}: \\beta_{0} = 0.\\ The\\ population\\ yintercept\\ equals\\ 0. \\]\nor\n\\[ H_{0}: \\beta_{1} = 0.\\ The\\ population\\ slope\\ equals\\ 0. \\]\nIn the following two null hypotheses, represented with \\(H_{0}\\), the population parameters are being assumed to be 0 in the population. This is one definition of a null effect, but is not the only null effect we can define (more on this later). The value defined as a null effect is important as it centers the sampling distribution used for evaluating where the sample estimate falls in that distribution.\nAnother hypothesis is typically defined with the null hypothesis, called the alternative hypothesis. This hypothesis states that there is an effect. Within the linear regression framework, we could write the alternative hypotheses as:\n\\[ H_{A}: \\beta_{0} \\neq 0.\\ The\\ population\\ yintercept\\ is\\ not\\ equal\\ to\\ 0. \\]\nor\n\\[ H_{A}: \\beta_{1} \\neq 0.\\ The\\ population\\ slope\\ is\\ not\\ equal\\ to\\ 0. \\]\nIn the following two alternative hypotheses, represented with \\(H_{A}\\), the population parameters are assumed to be not equal to 0. These can also be one-sided, more on this with an example later.\nEstimating uncertainty in parameter estimates The standard error is used to estimate uncertainty or error in the parameter estimates due to having a sample from the population. More specifically, this means that the entire population is not used to estimate the parameter, therefore the estimate we have is very likely not equal exactly to the parameter. Instead, there is some sort of sampling error involved that we want to quantify. If the sample was collected well, ideally randomly, then the estimate should be unbiased. Unbiased here doesn’t mean that the estimate equals the population parameter, rather, that through repeated sampling, the average of our sample estimates would equal the population parameter.\nAs mentioned, standard errors are used to quantify this uncertainty. In the linear regression case we have explored so far, there are mathematical formula for the standard errors. These are shown below.\n\\[ SE\\left( \\hat{\\beta}_{0} \\right) = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum \\left( X - \\bar{X} \\right)^2} \\right)} \\]\nand\n\\[ SE\\left( \\hat{\\beta}_{1} \\right) = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum \\left( X - \\bar{X} \\right)^2}} \\]\nIn the equation above, \\(\\hat{\\sigma}^2\\), is equal to \\(\\sqrt{\\frac{SS_{error}}{n - 2}}\\), and \\(n\\) is the sample size (ie, number of rows of data in the model).\nMatrix Representation It is also possible, and more easily extendable, to write the standard error computations or the variance of the estimated parameters in matrix representation. This framework extends beyond the single predictor case (ie. one \\(X\\)), therefore is more readily used in practice.\n\\[ \\hat{var}\\left({\\hat{\\beta}}\\right) = \\hat{\\sigma}^2 \\left( \\mathbf{X}^{`}\\mathbf{X} \\right)^{-1} \\]\nIn the equation above, \\(\\hat{\\sigma}^2\\), is equal to \\(\\sqrt{\\frac{SS_{error}}{n - 2}}\\), and \\(\\mathbf{X}\\) is the design matrix from the regression analysis. Finally, to get the standard errors back, you take the square root of the diagonal elements.\n$$ SE( ) =   Data The data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality |\u0026gt; left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) |\u0026gt; drop_na() ## Warning in left_join(airquality, wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 summary(air_lm)$r.square ## [1] 0.08528019 summary(air_lm)$sigma ## [1] 18.05479 sum_x \u0026lt;- airquality |\u0026gt; summarise(mean_wind = mean(avg_wind), sum_dev_x_sq = sum( (avg_wind - mean_wind) ^ 2)) sum_x ## # A tibble: 1 × 2 ## mean_wind sum_dev_x_sq ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 4.27 29937. se_b0 \u0026lt;- sqrt(summary(air_lm)$sigma^2 * ((1 / nrow(airquality)) + ( sum_x[[\u0026#39;mean_wind\u0026#39;]]^2 / sum_x[[\u0026#39;sum_dev_x_sq\u0026#39;]]) )) se_b1 \u0026lt;- sqrt(summary(air_lm)$sigma^2 / sum_x[[\u0026#39;sum_dev_x_sq\u0026#39;]]) se_b0 ## [1] 0.51551 se_b1 ## [1] 0.1043487 X \u0026lt;- model.matrix(air_lm) var_b \u0026lt;- summary(air_lm)$sigma^2 * solve(t(X) %*% X) var_b  ## (Intercept) avg_wind ## (Intercept) 0.26575054 -0.04644803 ## avg_wind -0.04644803 0.01088865 sqrt(diag(var_b)) ## (Intercept) avg_wind ## 0.5155100 0.1043487 summary(air_lm)$coefficients[,2] ## (Intercept) avg_wind ## 0.5155100 0.1043487   Moving toward inference Now that there the parameters are estimated and the amount of uncertainty is quantified, inference is possible. There are two related pieces that can be computed now, a confidence interval and/or the test-statistic and p-value. Let’s go through both.\nFirst, a confidence interval can be computed. Confidence intervals take the following general form:\n\\[ \\hat{\\beta} \\pm C * SE \\]\nWhere, \\(\\hat{\\beta}\\) is the parameter estimate, \\(C\\) is the confidence level, and \\(SE\\) is the standard error. The parameter estimates and standard errors are what we have already established, the \\(C\\) is the confidence level. This indicates the percentage of times, over the long run/repeated sampling, that the interval will capture the population parameter. Historically, this value is often specified as 95%, but any value is theoretically possible.\nThe \\(C\\) value represents a quantile from a mathematical distribution that separates the middle percetage desired (ie, 95%) from the rest of the distribution. The mathematical distribution is most often the t-distribution, but the difference between a t-distribution and normal distribution are modest once the sample size is greater than 30 or so.\nThe figure below tries to highlight the \\(C\\) value.\nt_30 \u0026lt;- data.frame(value = seq(-5, 5, .01), density = dt(seq(-5, 5, .01), df = 30)) gf_line(density ~ value, data = t_30) |\u0026gt; gf_vline(xintercept = ~ qt(.025, df = 30), linetype = 2) |\u0026gt; gf_vline(xintercept = ~ qt(.975, df = 30), linetype = 2) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 summary(air_lm)$coefficients[,2] ## (Intercept) avg_wind ## 0.5155100 0.1043487 abs(qt(.025, df = nrow(airquality) -2)) ## [1] 1.960456 48.2 + c(-1, 1) * 1.96 * .5155 ## [1] 47.18962 49.21038 -2.211 + c(-1, 1) * 1.96 * .1043 ## [1] -2.415428 -2.006572  Inference with test statistics It is also possible to do inference with a test statistic and computation of a p-value. Inference in this framework can be summarized into the following steps:\nGenerate hypotheses, ie null and alternative hypotheses. Establish an \\(\\alpha\\) value Estimate parameters Compute test statistic Generate p-value  An \\(\\alpha\\) value is the level of significance and represents the probability of obtaining the results due to chance. This is a value that the researcher can select. For a 5% \\(\\alpha\\) value, this is what was used above to compute the confidence intervals.\nThe test statistic is computed as follows:\n\\[ test\\ stat = \\frac{\\hat{\\beta} - hypothesized\\ value}{SE(\\hat{\\beta})} \\]\nwhere \\(\\hat{\\beta}\\) is the estimated parameter, \\(SE(\\hat{\\beta})\\) is the standard error of the parameter estimate, and the \\(hypothesized\\ value\\) is the hypothesized value from the null hypothesis. This is often 0, but does not need to be zero.\nLet’s assume the following null/alternative hypotheses:\n\\[ H_{0}: \\beta_{1} = 0 \\\\ H_{A}: \\beta_{1} \\neq 0 \\]\nLet’s use R to compute this test statistic.\nt = (-2.211 - 0) / .1043 t |\u0026gt; round(3) ## [1] -21.198 pt(-21.198, df = nrow(airquality) -2, lower.tail = TRUE) |\u0026gt; round(3) ## [1] 0 t_dist \u0026lt;- data.frame(value = seq(-25, 25, .05), density = dt(seq(-25, 25, .05), df = (nrow(airquality) - 2))) gf_line(density ~ value, data = t_dist) |\u0026gt; gf_vline(xintercept = ~ qt(.025, df = (nrow(airquality) - 2)), linetype = 2) |\u0026gt; gf_vline(xintercept = ~ qt(.975, df = (nrow(airquality) - 2)), linetype = 2) |\u0026gt; gf_vline(xintercept = ~ -21.198, color = \u0026#39;red\u0026#39;) |\u0026gt; gf_vline(xintercept = ~ 21.198, color = \u0026#39;red\u0026#39;) summary(air_lm) ## ## Call: ## lm(formula = daily_aqi ~ avg_wind, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.71 -14.38 -0.73 12.43 86.84 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 48.2229 0.5155 93.54 \u0026lt;2e-16 *** ## avg_wind -2.2118 0.1043 -21.20 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 18.05 on 4819 degrees of freedom ## Multiple R-squared: 0.08528,\tAdjusted R-squared: 0.08509 ## F-statistic: 449.3 on 1 and 4819 DF, p-value: \u0026lt; 2.2e-16 confint(air_lm) ## 2.5 % 97.5 % ## (Intercept) 47.212311 49.233581 ## avg_wind -2.416369 -2.007227 confint(air_lm, level = .5) ## 25 % 75 % ## (Intercept) 47.875214 48.57068 ## avg_wind -2.282185 -2.14141    ","date":1707955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707955200,"objectID":"80c54d5cbf0059b702a774d2ae849d3c","permalink":"https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/","publishdate":"2024-02-15T00:00:00Z","relpermalink":"/lectures/05-regression-inference/","section":"lectures","summary":"Inference for regression parameters It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population.","tags":null,"title":"Linear Regression Inference","type":"book"},{"authors":null,"categories":null,"content":"  Conditions for Linear Regression The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:\n\\[ Y - \\hat{Y} \\]\nThese are the deviations in the observed scores from the predicted scores from the linear regression. Recall, through least square estimation that these residauls will sum to 0, therefore, their mean would also be equal to 0. However, there are certain conditions about these residuals that are made for the linear regression model to have the inferences be appropriate. We’ll talk more about what the implications for violating these conditions will have on the linear regression model, but first, the conditions.\nApproximately Normally distributed residuals Homogeneity of variance Uncorrelated residuals Error term is uncorrelated with the predictor attribute Linearity and additivity of the model.  Each of these will be discussed in turn.\nData conditions that are not directly testable:\nValidity Representativeness  Data The data for this section of notes will explore data from the Environmental Protection Agency on Air Quality collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.\n    Variable Description    date Date of observation  id Site ID  poc Parameter Occurrence Code (POC)  pm2.5 Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)  daily_aqi Average air quality index  site_name Site Name  aqs_parameter_desc Text Description of Observation  cbsa_code Core Based Statistical Area (CBSA) ID  cbsa_name CBSA Name  county County in Iowa  avg_wind Average daily wind speed (in knots)  max_wind Maximum daily wind speed (in knots)  max_wind_hours Time of maximum daily wind speed    Guiding Question How is average daily wind speed related to the daily air quality index?\nlibrary(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.2 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.3 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.2 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (\u0026lt;http://conflicted.r-lib.org/\u0026gt;) to force all conflicts to become errors library(ggformula) ## Loading required package: ggstance ## ## Attaching package: \u0026#39;ggstance\u0026#39; ## ## The following objects are masked from \u0026#39;package:ggplot2\u0026#39;: ## ## geom_errorbarh, GeomErrorbarh ## ## Loading required package: scales ## ## Attaching package: \u0026#39;scales\u0026#39; ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## discard ## ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## col_factor ## ## Loading required package: ggridges ## ## New to ggformula? Try the tutorials: ## learnr::run_tutorial(\u0026quot;introduction\u0026quot;, package = \u0026quot;ggformula\u0026quot;) ## learnr::run_tutorial(\u0026quot;refining\u0026quot;, package = \u0026quot;ggformula\u0026quot;) library(mosaic) ## Registered S3 method overwritten by \u0026#39;mosaic\u0026#39;: ## method from ## fortify.SpatialPolygonsDataFrame ggplot2 ## ## The \u0026#39;mosaic\u0026#39; package masks several functions from core packages in order to add ## additional features. The original behavior of these functions should not be affected by this. ## ## Attaching package: \u0026#39;mosaic\u0026#39; ## ## The following object is masked from \u0026#39;package:Matrix\u0026#39;: ## ## mean ## ## The following object is masked from \u0026#39;package:scales\u0026#39;: ## ## rescale ## ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## count, do, tally ## ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## cross ## ## The following object is masked from \u0026#39;package:ggplot2\u0026#39;: ## ## stat ## ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test, ## quantile, sd, t.test, var ## ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## max, mean, min, prod, range, sample, sum theme_set(theme_bw(base_size = 18)) airquality \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv\u0026quot;) ## Rows: 6917 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county ## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. wind \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv\u0026quot;) ## Rows: 1537 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: \u0026quot;,\u0026quot; ## chr (2): date, cbsa_name ## dbl (3): avg_wind, max_wind, max_wind_hours ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. airquality \u0026lt;- airquality |\u0026gt; left_join(wind, by = c(\u0026#39;cbsa_name\u0026#39;, \u0026#39;date\u0026#39;)) |\u0026gt; drop_na() ## Warning in left_join(airquality, wind, by = c(\u0026quot;cbsa_name\u0026quot;, \u0026quot;date\u0026quot;)): Detected an unexpected many-to-many relationship between `x` and `y`. ## ℹ Row 21 of `x` matches multiple rows in `y`. ## ℹ Row 1 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, set `relationship = ## \u0026quot;many-to-many\u0026quot;` to silence this warning. air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) coef(air_lm) ## (Intercept) avg_wind ## 48.222946 -2.211798 The residuals can be saved with the resid() function. These can also be added to the original data, which are particularly helpful.\nhead(resid(air_lm)) ## 1 2 3 4 5 6 ## 15.283427 11.186742 25.191433 15.398540 8.246896 -12.749410 airquality$residuals \u0026lt;- resid(air_lm) head(airquality) ## # A tibble: 6 × 14 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 6 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, residuals \u0026lt;dbl\u0026gt;   Approximately Normally distributed residuals The first assumption is that the residuals are at least approximately Normally distributed. This assumption is really only much of a concern when the sample size is small. If the sample size is larger, the Central Limit Thereom (CLT) states that the distribution of the statstics will be approximately normally distributed. The threshold for the CLT to be properly invoked is about 30. Larger then this, the residuals do not need to be approximately normally distributed. Even still, exploring the distribution of the residuals can still be helpful and can also be helpful to identify potential extreme values.\nThis example will make use of the air quality data one more time.\ngf_density(~ residuals, data = airquality) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) ggplot(airquality, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) Different QQ plots by distribution type This next section will generate some residuals that would fit certain types you may see in practice. These are meant as a way to give you some data from residuals that would be common in practice.\nSymmetric / Normally Distributed Residuals # install.packages(\u0026#39;patchwork\u0026#39;) library(patchwork) sim_resid \u0026lt;- data.frame(residuals = rnorm(500, 0, sd = 1)) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Skewed Residuals sim_resid \u0026lt;- data.frame(residuals = rchisq(500, df = 1) - 1) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Negatively Skewed sim_resid \u0026lt;- data.frame(residuals = (rchisq(500, df = 1) - 1) * -1) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Heavy Tails sim_resid \u0026lt;- data.frame(residuals = (rt(500, df = 4))) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Uniform Distribution sim_resid \u0026lt;- data.frame(residuals = (runif(500, min = -1, max = 1))) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Floor Effect sim_resid \u0026lt;- data.frame(residuals = (rnorm(500, mean = 0, sd = 1))) |\u0026gt; mutate(residuals = ifelse(residuals \u0026lt; -1.5, -1.5, residuals)) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot  Ceiling Effect sim_resid \u0026lt;- data.frame(residuals = (rnorm(500, mean = 0, sd = 1))) |\u0026gt; mutate(residuals = ifelse(residuals \u0026gt; 1.5, 1.5, residuals)) dens \u0026lt;- gf_density(~ residuals, data = sim_resid) |\u0026gt; gf_labs(x = \u0026quot;Residuals\u0026quot;) qq_plot \u0026lt;- ggplot(sim_resid, aes(sample = residuals)) + stat_qq(size = 5) + stat_qq_line(linewidth = 2) dens + qq_plot   Standardized Residuals Standardized residuals can be another way to explore the residuals. These will now be standardized to have a variance of 1, similar to that of a Z-score. These can be computed as:\n\\[ standardized\\ residuals = \\frac{\\epsilon_{i}}{SD_{\\epsilon}} \\]\nWithin R, these can be computed using the function rstandard(). Furthermore, these can be computed from another package called broom with the augment() function.\nhead(rstandard(air_lm)) ## 1 2 3 4 5 6 ## 0.8466153 0.6196983 1.3955421 0.8529766 0.4568937 -0.7062638 library(broom) resid_diagnostics \u0026lt;- augment(air_lm) head(resid_diagnostics) ## # A tibble: 6 × 8 ## daily_aqi avg_wind .fitted .resid .hat .sigma .cooksd .std.resid ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 57 2.94 41.7 15.3 0.000266 18.1 0.0000953 0.847 ## 2 54 2.45 42.8 11.2 0.000318 18.1 0.0000611 0.620 ## 3 69 2.00 43.8 25.2 0.000380 18.1 0.000370 1.40 ## 4 56 3.45 40.6 15.4 0.000230 18.1 0.0000836 0.853 ## 5 54 1.12 45.8 8.25 0.000539 18.1 0.0000563 0.457 ## 6 22 6.09 34.7 -12.7 0.000319 18.1 0.0000795 -0.706 gf_density(~ .std.resid, data = resid_diagnostics) |\u0026gt; gf_labs(x = \u0026quot;Standardized Residuals\u0026quot;) ggplot(resid_diagnostics, aes(sample = .std.resid)) + stat_qq(size = 5) + stat_qq_line(size = 2) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated.   Homogeneity of variance Homoegeneity of variance is an assumption that is of larger concern compared to normality of the residuals. Homoogeneity of variance is an assumption that states that the variance of the residuals are similar across the predicted or fitted values from the regression line. This assumption can be explored by looking at the residuals (standardized or raw residuals), by the fitted or predicted values. Within this plot, the range of residuals should be similar across the range of fitted or predicted values.\ngf_point(.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) |\u0026gt; gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Residuals\u0026#39;) gf_point(.std.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) |\u0026gt; gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Standardized Residuals\u0026#39;) Another figure that can also be helpful for the homogeneity of variance assumption is one that rescales the residuals on the y-axis. The rescaling makes all the standardized residuals positive (takes the absolute value) and then takes the square root of this.\nresid_diagnostics |\u0026gt; mutate(sqrt_abs_sresid = sqrt(abs(.std.resid))) |\u0026gt; gf_point(sqrt_abs_sresid ~ .fitted, size = 5, alpha = .15) |\u0026gt; gf_smooth(method = \u0026#39;loess\u0026#39;, size = 2) |\u0026gt; gf_labs(x = \u0026#39;Fitted Values\u0026#39;, y = \u0026#39;Sqrt Abs Standardized Residuals\u0026#39;) Statistical test for homogeneity of variance There are also statistical tests that can aid in the assessing if the data are constant or not. The one that has been shown to perform relatively well is the Breusch-Pagan test. The details of the statistical test are not going to be discussed here, but we will focus on interpreting the output.\n# install.packages(\u0026quot;car\u0026quot;) library(car) ## Loading required package: carData ## ## Attaching package: \u0026#39;car\u0026#39; ## The following objects are masked from \u0026#39;package:mosaic\u0026#39;: ## ## deltaMethod, logit ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## recode ## The following object is masked from \u0026#39;package:purrr\u0026#39;: ## ## some air_lm \u0026lt;- lm(daily_aqi ~ avg_wind, data = airquality) ncvTest(air_lm) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 0.007804568, Df = 1, p = 0.9296   Data with high leverage Data with high leverage are extreme values that may significantly impact the regression estimates. These statistics include extreme values for the outcome or predictor attributes. Cook’s distance is one statistic that can help to identify points with high impact/leverage for the regression estimates. Cook’s distance is a statistic that represents how much change there would be in the fitted values if the point was removed when estimating the regression coefficients. There is some disagreement between what type of thresholds to use for Cook’s distance, but one rule of thumb is Cook’s distance greater than 1. There has also been some research showing that Cook’s distance follows an F-distribution, so a more specific value could be computed. The rule of thumb for greater than 1 comes from the F distribution for large samples.\nresid_diagnostics |\u0026gt; mutate(obs_num = 1:n()) |\u0026gt; gf_col(.cooksd ~ obs_num, fill = \u0026#39;black\u0026#39;, color = \u0026#39;black\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Cook\u0026#39;s Distance\u0026quot;) Another leave one out statistic is the studentized deleted residuals. These are computed by removing a data point, refitting the regression model, then generate a predicted value for the X value for the data point removed. Then the residual is computed the same as before and is standardized like the standardized residuals above. The function in R to compute these is rstudent().\nhead(rstudent(air_lm)) ## 1 2 3 4 5 6 ## 0.8465904 0.6196587 1.3956793 0.8529524 0.4568561 -0.7062271 airquality$student_residuals \u0026lt;- rstudent(air_lm) head(airquality) ## # A tibble: 6 × 15 ## date id poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1/1/21 190130009 1 15.1 57 Water To… PM2.5 - Local Con… 47940 ## 2 1/4/21 190130009 1 13.3 54 Water To… PM2.5 - Local Con… 47940 ## 3 1/7/21 190130009 1 20.5 69 Water To… PM2.5 - Local Con… 47940 ## 4 1/10/21 190130009 1 14.3 56 Water To… PM2.5 - Local Con… 47940 ## 5 1/13/21 190130009 1 13.7 54 Water To… PM2.5 - Local Con… 47940 ## 6 1/16/21 190130009 1 5.3 22 Water To… PM2.5 - Local Con… 47940 ## # ℹ 7 more variables: cbsa_name \u0026lt;chr\u0026gt;, county \u0026lt;chr\u0026gt;, avg_wind \u0026lt;dbl\u0026gt;, ## # max_wind \u0026lt;dbl\u0026gt;, max_wind_hours \u0026lt;dbl\u0026gt;, residuals \u0026lt;dbl\u0026gt;, ## # student_residuals \u0026lt;dbl\u0026gt; airquality |\u0026gt; mutate(obs_num = 1:n()) |\u0026gt; gf_point(student_residuals ~ obs_num, size = 5, alpha = .15) |\u0026gt; gf_hline(yintercept = ~ 3, color = \u0026#39;blue\u0026#39;, size = 2) |\u0026gt; gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Studentized Residuals\u0026quot;) Leverage can be another measure to help detect outliers in X values. The hat values that were computed from the augment() function above and can be interpreted as the distance the X scores are from the center of all X predictors. In the case of a single predictor, the hat values are the distance the X score is from the mean of X. The hat values will also sum up to the number of predictors and will always range between 0 and 1.\nresid_diagnostics |\u0026gt; mutate(obs_num = 1:n()) |\u0026gt; gf_col(.hat ~ obs_num, fill = \u0026#39;black\u0026#39;, color = \u0026#39;black\u0026#39;) |\u0026gt; gf_labs(x = \u0026quot;Observation Number\u0026quot;, y = \u0026quot;Hat Values (leverage)\u0026quot;) plot(air_lm, which = 1:5)  Solutions to overcome data conditions not being met There are some strategies that can be done when a variety of data conditions for linear regression have not been met. In general, the strategies stem around generalizing the model and adding some complexity.\nThe following are options that would be possible:\nAdd interactions or other non-linear terms Use more complicated model  weighted least squares for homogeneity of variance concerns models that include measurement error mixed models for correlated data  Transform the data.    ","date":1707955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707955200,"objectID":"c603bd50d5b7676892b81eba2ae9ddf7","permalink":"https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions/","publishdate":"2024-02-15T00:00:00Z","relpermalink":"/lectures/06-regression-conditions/","section":"lectures","summary":"Conditions for Linear Regression The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:\n\\[ Y - \\hat{Y} \\]\nThese are the deviations in the observed scores from the predicted scores from the linear regression.","tags":null,"title":"Linear Regression Data Conditions","type":"book"},{"authors":null,"categories":null,"content":"Quiz 1 can be taken on ICON, due February 12th, 2023. The quiz covers content from Week 1.\nQuiz 1 Link\n","date":1694044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694044800,"objectID":"ac83706d5690d70e2da1268514b156a4","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz1/","publishdate":"2023-09-07T00:00:00Z","relpermalink":"/assignments/quizzes/quiz1/","section":"assignments","summary":"Quiz 1 can be taken on ICON, due February 12th, 2023. The quiz covers content from Week 1.\nQuiz 1 Link","tags":null,"title":"Quiz 1","type":"book"},{"authors":null,"categories":null,"content":"Quiz 2 can be taken on ICON, due February 19th, 2022. The quiz covers content from Week 2 or section 2 of the notes.\nQuiz 2 Link\n","date":1662595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662595200,"objectID":"89e911fba74fe6a1dc69de07ae2db619","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz2/","publishdate":"2022-09-08T00:00:00Z","relpermalink":"/assignments/quizzes/quiz2/","section":"assignments","summary":"Quiz 2 can be taken on ICON, due February 19th, 2022. The quiz covers content from Week 2 or section 2 of the notes.\nQuiz 2 Link","tags":null,"title":"Quiz 2","type":"book"},{"authors":null,"categories":null,"content":"Quiz 3 can be taken on ICON, due February 26th, 2024. The quiz covers content from Week 3.\nQuiz 3 Link\n","date":1694736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694736000,"objectID":"ffe02f7f4b85dd056013551821b59d03","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz3/","publishdate":"2023-09-15T00:00:00Z","relpermalink":"/assignments/quizzes/quiz3/","section":"assignments","summary":"Quiz 3 can be taken on ICON, due February 26th, 2024. The quiz covers content from Week 3.\nQuiz 3 Link","tags":null,"title":"Quiz 3","type":"book"},{"authors":null,"categories":null,"content":"Quiz 4 can be taken on ICON, due March 4th, 2024. The quiz covers content from Week 4 and Week 5.\nQuiz 4 Link\n","date":1708473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708473600,"objectID":"c160bc7b80914ff7c52bbe5d6dff1a09","permalink":"https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz4/","publishdate":"2024-02-21T00:00:00Z","relpermalink":"/assignments/quizzes/quiz4/","section":"assignments","summary":"Quiz 4 can be taken on ICON, due March 4th, 2024. The quiz covers content from Week 4 and Week 5.\nQuiz 4 Link","tags":null,"title":"Quiz 4","type":"book"},{"authors":null,"categories":null,"content":"The following practice problem is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this practice problem. If you work in groups, please have every member of the group complete the ICON survey.\nInstructions What to turn in Please turn in a document that includes any relevant statistics/figures created. You will be asked to complete a graded survey on ICON as part of this practice problem.\nFinally, upload the final document to ICON and complete the graded survey.\nDue Date Due around February 26th, 2023. No penalty for late submissions as long as it is submitted by May 9th.\nData The data for this activity comes from the Kaggle. The data contain 104 rows and 14 columns about possums collected from Australia. A data description for each column in the data is shown below.\nThe data can be obtained in csv format. A short description for each attribute is as follows. These data are also found within the \u0026ldquo;data\u0026rdquo; folder inside the IDAS.\n   variable class description     case integer Observation number.   site integer site.   Pop character Population, either Vic (Victoria) or other (New South Wales or Queensland)..   sex character Sex of possum, either m (male) or f (female)..   age integer Age.   hdlngth integer Head length, in mm.   skullw integer Skull width, in mm.   totlngth integer Total length, in cm.   taill integer Tail length, in cm   footlgth integer foot length, in mm.   earconch integer ear conch length, in mm.   eye integer distance from medial canthus to lateral canthus of right eye, in mm.   chest integer chest girth, in cm.   belly double belly girth, in cm.    Questions   Explore the distribution of the totlngth attribute. Summarize key elements of the distribution, for instance discussing elements related to shape, center, variation, and/or extreme values.\n  Explore the bivariate association between tail length (taill attribute) and the total length from question 1 visually. From the figure, estimate what is the bivariate association. Does the association appear to be linear?\n  Compute the bivariate association (i.e., the correlation) between tail length and total length. What is the best interpretation for the correlation?\n  ","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707868800,"objectID":"d751f87057c06ead38780cde79f63ad0","permalink":"https://psqf6243.brandonlebeau.org/assignments/practice/practice1/","publishdate":"2024-02-14T00:00:00Z","relpermalink":"/assignments/practice/practice1/","section":"assignments","summary":"The following practice problem is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this practice problem.","tags":null,"title":"Practice Problem 1","type":"book"},{"authors":null,"categories":null,"content":"The following practice problem is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this practice problem. If you work in groups, please have every member of the group complete the ICON survey.\nInstructions What to turn in Please turn in a document that includes any relevant statistics/figures created. You will be asked to complete a graded survey on ICON as part of this practice problem.\nFinally, upload the final document to ICON and complete the graded survey.\nDue Date Due around March 4th, 2023. No penalty for late submissions as long as it is submitted by May 9th.\nData The data for this activity comes from the Kaggle. The data contain 104 rows and 14 columns about possums collected from Australia. A data description for each column in the data is shown below.\nThe data can be obtained in csv format. A short description for each attribute is as follows. These data are also found within the \u0026ldquo;data\u0026rdquo; folder inside the IDAS.\n   variable class description     case integer Observation number.   site integer site.   Pop character Population, either Vic (Victoria) or other (New South Wales or Queensland)..   sex character Sex of possum, either m (male) or f (female)..   age integer Age.   hdlngth integer Head length, in mm.   skullw integer Skull width, in mm.   totlngth integer Total length, in cm.   taill integer Tail length, in cm   footlgth integer foot length, in mm.   earconch integer ear conch length, in mm.   eye integer distance from medial canthus to lateral canthus of right eye, in mm.   chest integer chest girth, in cm.   belly double belly girth, in cm.    Guiding Question Does the tail length (taill attribute) explain variation in the total length (totlngth attribute) of the possum?\nQuestions   Fit a linear regression to answer the research question highlighted above. Interpret the intercept and slope of the linear regression. That is, what do these terms mean?\n  What are the r-squared and sigma estimates from the linear regression? Interpret these two values in the context of the problem. That is, what do these two terms mean in the context of the data and this problem?\n  Finally, in a couple of sentences, provide a summary of the overall model. Does the model appear to be useful to predict or explain variation in the total length of the possum with the tail length? Use statistics from the analysis steps above to support your answer.\n  ","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707868800,"objectID":"16c4d06460ecf4bcc268c6a12eb4e234","permalink":"https://psqf6243.brandonlebeau.org/assignments/practice/practice2/","publishdate":"2024-02-14T00:00:00Z","relpermalink":"/assignments/practice/practice2/","section":"assignments","summary":"The following practice problem is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this practice problem.","tags":null,"title":"Practice Problem 2","type":"book"}]