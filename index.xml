<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PSQF 6243</title>
    <link>https://psqf6243.brandonlebeau.org/</link>
      <atom:link href="https://psqf6243.brandonlebeau.org/index.xml" rel="self" type="application/rss+xml" />
    <description>PSQF 6243</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 28 Sep 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://psqf6243.brandonlebeau.org/media/blue-balloon.jpg</url>
      <title>PSQF 6243</title>
      <link>https://psqf6243.brandonlebeau.org/</link>
    </image>
    
    <item>
      <title>Review</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/01-review/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/01-review/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;review-for-psqf-6243&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Review for PSQF 6243&lt;/h1&gt;
&lt;p&gt;This serves as a non-exhaustive review for the course. These are elements that I assume you have knowledge of prior to starting the course.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variable vs constant attributes&lt;/li&gt;
&lt;li&gt;Types of variables (ie., nominal, ordinal, integer, ratio)&lt;/li&gt;
&lt;li&gt;Descriptive Statistics (eg., mean, median, standard deviation, variance, percentiles)&lt;/li&gt;
&lt;li&gt;Higher order moments (eg., skewness and kurtosis)&lt;/li&gt;
&lt;li&gt;Exploring/summarizing univariate distributions (eg., histogram or density figure)&lt;/li&gt;
&lt;li&gt;What is a statistical model? Why do we use them?&lt;/li&gt;
&lt;li&gt;Population vs Sample&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;Mario Kart 64 world record data:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;class&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;track&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Track name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;type&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Single or three lap record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;shortcut&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Shortcut or non-shortcut record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;player&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Player’s name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;system_played&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Used system (NTSC or PAL)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;World record date&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_period&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;period&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Time as &lt;code&gt;hms&lt;/code&gt; period&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;double&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Time in seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;record_duration&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;double&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Record duration in days&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load some libraries
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)

theme_set(theme_bw(base_size = 18))

# load in some data
mariokart &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-05-25/records.csv&amp;#39;) %&amp;gt;%
    mutate(year = year(date),
           month = month(date),
           day = month(date))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 2334 Columns: 9
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): track, type, shortcut, player, system_played, time_period
## dbl  (2): time, record_duration
## date (1): date
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mariokart)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 12
##   track         type  shortcut player system_played date       time_period  time
##   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;         &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Luigi Raceway Thre… No       Salam  NTSC          1997-02-15 2M 12.99S    133.
## 2 Luigi Raceway Thre… No       Booth  NTSC          1997-02-16 2M 9.99S     130.
## 3 Luigi Raceway Thre… No       Salam  NTSC          1997-02-16 2M 8.99S     129.
## 4 Luigi Raceway Thre… No       Salam  NTSC          1997-02-28 2M 6.99S     127.
## 5 Luigi Raceway Thre… No       Gregg… NTSC          1997-03-07 2M 4.51S     125.
## 6 Luigi Raceway Thre… No       Rocky… NTSC          1997-04-30 2M 2.89S     123.
## # ℹ 4 more variables: record_duration &amp;lt;dbl&amp;gt;, year &amp;lt;dbl&amp;gt;, month &amp;lt;dbl&amp;gt;, day &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# univariate distribution of time
gf_histogram(~ time, data = mariokart, bins = 30) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ time, data = mariokart) %&amp;gt;% 
   gf_labs(x = &amp;quot;Time (in seconds)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(~ time, data = mariokart, mean, median, sd, skewness, kurtosis, quantile(probs = c(0.1, 0.5, 0.9)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response     mean median      sd skewness kurtosis   10%   50%     90%
## 1     time 90.62383  86.19 66.6721 1.771732 3.844745 31.31 86.19 171.961&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-association&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bivariate Association&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(time ~ record_duration, data = mariokart)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.06736739&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(time ~ record_duration, data = mariokart) %&amp;gt;%
  gf_labs(x = &amp;quot;How long the record was held&amp;quot;,
          y = &amp;quot;Time (in seconds)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/01-review_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Questions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is problematic about the analyses above? Why?&lt;/li&gt;
&lt;li&gt;What could be done to improve the analyses above?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>https://psqf6243.brandonlebeau.org/content/00-getting-started/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/00-getting-started/</guid>
      <description>&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Review the syllabus&lt;/li&gt;
&lt;li&gt;Review the schedule&lt;/li&gt;
&lt;li&gt;Review, accessing the IDAS page
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://notebooks.hpc.uiowa.edu/fall2023-psqf-6243-0002/hub/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jupyter Notebooks&lt;/a&gt; version of IDAS&lt;/li&gt;
&lt;li&gt;RStudio Server, to access you can switch via the &amp;ldquo;New&amp;rdquo; button on Jupyter Notebooks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optionally, but strongly encouraged, complete the &lt;a href=&#34;https://forms.office.com/r/k5mWc5JbZg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course survey&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>IDAS Introduction</title>
      <link>https://psqf6243.brandonlebeau.org/content/00a-idas/</link>
      <pubDate>Fri, 18 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/00a-idas/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This page is meant as a way to give some introduction and how to access the IDAS used for the course.&lt;/p&gt;
&lt;h2 id=&#34;access&#34;&gt;Access&lt;/h2&gt;
&lt;p&gt;The IDAS server can be accessed via the following link directly: &lt;a href=&#34;https://notebooks.hpc.uiowa.edu/fall2023-psqf-6243-0002/hub/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Access IDAS&lt;/a&gt;. Upon going here, you will be prompted to log in with your HawkID. Finally, to use the IDAS, you will need to have DUO authentication activated.&lt;/p&gt;
&lt;p&gt;Here is a good link if you have not set up DUO authentication before from ITS. It may take a few hours or up to a day to be granted access to the IDAS after setting up DUO authentication for the first time, &lt;a href=&#34;https://its.uiowa.edu/duo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DUO Authentication Docs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;idas-specifics&#34;&gt;IDAS Specifics&lt;/h2&gt;
&lt;p&gt;The IDAS by default will launch in Jupyter Lab mode and will sync all course materials that I have posted for the course. The syncing of remote materials will never overwrite anything that you have manually edited by you, the student. For this reason, if I notice a breaking change, I usually post a new updated file with a new name.&lt;/p&gt;
&lt;p&gt;You can also access RStudio server within the IDAS for the course as well. This can be accessed by clicking on &amp;ldquo;RStudio&amp;rdquo; from the launcher page. You should still have access to all of the course files that are automatically synced for you within the RStudio environment.&lt;/p&gt;
&lt;h2 id=&#34;shutting-down-idas&#34;&gt;Shutting down IDAS&lt;/h2&gt;
&lt;p&gt;When you are finished working in the IDAS, you should go to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;File &amp;gt; Hub Control Panel &amp;gt; Stop My Server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This disconnects/stops the server instance so that the resources can be allocated to others wishing to use the service.&lt;/p&gt;
&lt;h2 id=&#34;common-issues&#34;&gt;Common issues&lt;/h2&gt;
&lt;p&gt;ITS has established a &lt;a href=&#34;https://wiki.uiowa.edu/display/hpcdocs/Troubleshooting&amp;#43;Common&amp;#43;Issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wiki page&lt;/a&gt; for common problems that occur and other documentation related to the IDAS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Linear Regression</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/</link>
      <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;introduction-to-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 &lt;strong&gt;continuous&lt;/strong&gt; attributes. One of those attributes will be the &lt;em&gt;outcome&lt;/em&gt; or &lt;em&gt;attribute of interest&lt;/em&gt; whereas the other will used as a &lt;em&gt;predictor&lt;/em&gt;. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable &lt;strong&gt;explains variation&lt;/strong&gt; in the dependent variable (more on this later).&lt;/p&gt;
&lt;p&gt;Of note, variable is a typical word used in statistics, I’ve come to like the word &lt;strong&gt;attribute&lt;/strong&gt; instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.&lt;/p&gt;
&lt;p&gt;We may write this general model as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_{0} + \beta_{1} X + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the outcome attribute. It is also known as the dependent variable. The &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; term is the predictor/covariate attribute. It is also known as the independent variable. The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a random error term, more on this later. Finally, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; are unknown population coefficients that we are interested in estimating. More on this later too.&lt;/p&gt;
&lt;div id=&#34;specific-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specific example&lt;/h2&gt;
&lt;p&gt;The data used for this section of the course is from the 2019 WNBA season. These data are part of the &lt;a href=&#34;https://www.bayesrulesbook.com/&#34;&gt;&lt;em&gt;bayesrules&lt;/em&gt; package/book&lt;/a&gt;. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)

basketball &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 146 Columns: 32
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (2): player_name, team
## dbl (29): height, weight, year, age, games_played, games_started, avg_minute...
## lgl  (1): starter
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(basketball)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 32
##   player_name     height weight  year team    age games_played games_started
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 Natalie Achonwa     75    190  2019 IND      26           30            18
## 2 Kayla Alexander     76    195  2019 CHI      28            3             0
## 3 Rebecca Allen       74    162  2019 NYL      26           24             2
## 4 Jillian Alleyne     74    193  2019 MIN      24            5             0
## 5 Kristine Anigwe     76    200  2019 TOT      22           27             0
## 6 Kristine Anigwe     76    200  2019 CON      22           17             0
## # ℹ 24 more variables: avg_minutes_played &amp;lt;dbl&amp;gt;, avg_field_goals &amp;lt;dbl&amp;gt;,
## #   avg_field_goal_attempts &amp;lt;dbl&amp;gt;, field_goal_pct &amp;lt;dbl&amp;gt;,
## #   avg_three_pointers &amp;lt;dbl&amp;gt;, avg_three_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   three_pointer_pct &amp;lt;dbl&amp;gt;, avg_two_pointers &amp;lt;dbl&amp;gt;,
## #   avg_two_pointer_attempts &amp;lt;dbl&amp;gt;, two_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_free_throws &amp;lt;dbl&amp;gt;, avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.&lt;/p&gt;
&lt;p&gt;One first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.&lt;/p&gt;
&lt;p&gt;The next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;questions-to-consider&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Questions to consider&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What can be noticed about the relationship between these two attributes?&lt;/li&gt;
&lt;li&gt;Does there appear to be a relationship between the two?&lt;/li&gt;
&lt;li&gt;Is this relationship perfect?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-smoother-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding a smoother line&lt;/h2&gt;
&lt;p&gt;Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.&lt;/p&gt;
&lt;p&gt;I’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.&lt;/p&gt;
&lt;p&gt;Does there appear to be much difference in the relationship across the two lines?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-linear-regression-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating linear regression coefficients&lt;/h2&gt;
&lt;p&gt;The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is &lt;code&gt;lm()&lt;/code&gt; to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.&lt;/p&gt;
&lt;p&gt;This equation could be written more directly for our specific problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Avg\_points = \beta_{0} + \beta_{1} Minutes\_Played + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.&lt;/p&gt;
&lt;p&gt;For the R formula, instead of an $ = $, you could insert a ~.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wnba_reg &amp;lt;- lm(avg_points ~ total_minutes, data = basketball)
coef(wnba_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) total_minutes 
##    1.13562456    0.01014207&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretting-linear-regression-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretting linear regression terms&lt;/h2&gt;
&lt;p&gt;Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{avg\_points} = 1.1356 + .0101 min\_played
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where instead of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;, the estimated values from this single season were inserted. Note the &lt;span class=&#34;math inline&#34;&gt;\(\hat{avg\_points}\)&lt;/span&gt;, which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1457&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.1456&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * mean(basketball$avg_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.189732&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 5000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.6356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * -50&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6306&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.&lt;/p&gt;
&lt;p&gt;First, for the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.&lt;/p&gt;
&lt;p&gt;The slope, (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.&lt;/p&gt;
&lt;p&gt;The predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-the-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about the error?&lt;/h2&gt;
&lt;p&gt;So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?&lt;/p&gt;
&lt;p&gt;The short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Sometimes this is written in math notation as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\epsilon \sim N(0, \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From this notation, can you see why the error was disregarded earlier when generating predictions?&lt;/p&gt;
&lt;p&gt;In short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.&lt;/p&gt;
&lt;div id=&#34;estimating-error-in-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating error in linear regression&lt;/h3&gt;
&lt;p&gt;This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the &lt;em&gt;sum of squares regression&lt;/em&gt; (SSR), the portion that is unexplained is referred to as the &lt;em&gt;sum of squares error&lt;/em&gt; (SSE). This could be written in math notation as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum (Y - \bar{Y})^2 = \sum (Y - \hat{Y})^2 + \sum (\hat{Y} - \bar{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s try to visualize what this means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#FF7F7F&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#65a765&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#FFD580&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;another-related-measure-of-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another related measure of error&lt;/h2&gt;
&lt;p&gt;Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R^{2} = 1 - \frac{SS_{res}}{SS_{total}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R^{2} = \frac{SS_{reg}}{SS_{total}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s compute the sum of square and get a value for &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basketball %&amp;gt;%
summarise(ss_total = sum((avg_points - mean(avg_points))^2),
          ss_error = sum((avg_points - fitted(wnba_reg))^2),
          ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %&amp;gt;%
mutate(r_square = 1 - ss_error / ss_total,
       r_square2 = ss_reg / ss_total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   ss_total ss_error ss_reg r_square r_square2
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    2004.     564.  1440.    0.719     0.719&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(wnba_reg)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7185315&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(wnba_reg)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma_hat_square &amp;lt;- 563.9929 / (nrow(basketball) - 2)
sigma_hat &amp;lt;- sqrt(sigma_hat_square)

sigma_hat_square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916617&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma_hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>https://psqf6243.brandonlebeau.org/content/01-week1/</link>
      <pubDate>Tue, 22 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/01-week1/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This week is an introduction to the course. Primary content will be a review to set the basis for the remainder of the semester.&lt;/p&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read syllabus, ask any questions&lt;/li&gt;
&lt;li&gt;Review &lt;a href=&#34;../../lectures/01-review/&#34;&gt;introductory statistics content&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Explore R code from introductory slides&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Review introductory statistics content
&lt;ul&gt;
&lt;li&gt;Chapters 2 - 4 of &lt;a href=&#34;https://lebebr01.github.io/stat_thinking/visualization.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Reasoning through computation and R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optional, complete &lt;a href=&#34;https://forms.office.com/r/k5mWc5JbZg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course survey&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;p&gt;None this week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>https://psqf6243.brandonlebeau.org/content/02-week2/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/02-week2/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.&lt;/p&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Recognize a linear regression formula&lt;/li&gt;
&lt;li&gt;Interpret regression parameters&lt;/li&gt;
&lt;li&gt;Specify a linear regression in statistical software&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Chapter 1 of &lt;a href=&#34;https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Regression: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Engage with &lt;a href=&#34;../../lectures/&#34;&gt;course notes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Optional, Read Chapter 7 of &lt;a href=&#34;https://lebebr01.github.io/stat_thinking/linear-regression.html#simple-regression-continuous-predictor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Reasoning through Computation and R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optional, Read 7.1 and 7.2 of &lt;a href=&#34;https://openintro-ims.netlify.app/model-slr.html#model-slr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Modern Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;p&gt;TBA&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression - In Class</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/02a-in-class-activity/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/02a-in-class-activity/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;in-class-activity---august-29-2023&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In Class Activity - August 29, 2023&lt;/h2&gt;
&lt;p&gt;This activity is meant to give you some exploration of topics within class. I’ve created the code for you to run the activity and explore some data.&lt;/p&gt;
&lt;p&gt;The goals of this activity are as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Explore what happens if the X and Y attributes are flipped in a linear regression.
&lt;ul&gt;
&lt;li&gt;What happens to the linear regression coefficient estimates (ie., the &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;)?&lt;/li&gt;
&lt;li&gt;What happens to the sigma and R-Square statistics?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Given what you find in #1, how do you decide which attribute should be an outcome (Y) vs a predictor (X)?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(palmerpenguins)
library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 16))

# If you get errors, use this line of code too.
# penguins &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/main/inst/extdata/penguins.csv&amp;quot;)

head(penguins)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 8
##   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;              &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;             &amp;lt;int&amp;gt;       &amp;lt;int&amp;gt;
## 1 Adelie  Torgersen           39.1          18.7               181        3750
## 2 Adelie  Torgersen           39.5          17.4               186        3800
## 3 Adelie  Torgersen           40.3          18                 195        3250
## 4 Adelie  Torgersen           NA            NA                  NA          NA
## 5 Adelie  Torgersen           36.7          19.3               193        3450
## 6 Adelie  Torgersen           39.3          20.6               190        3650
## # ℹ 2 more variables: sex &amp;lt;fct&amp;gt;, year &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_fit &amp;lt;- function(outcome, predictor, data = penguins) {
    formula &amp;lt;- as.formula(paste(outcome, predictor, sep = &amp;quot;~&amp;quot;))
    model_out &amp;lt;- lm(formula, data = data)

    model_coef &amp;lt;- data.frame(matrix(c(coef(model_out)), ncol = 2))
    names(model_coef) &amp;lt;- c(&amp;quot;Intercept&amp;quot;, &amp;quot;Slope&amp;quot;)

    data.frame(model_coef, 
    Rsquare = summary(model_out)$r.square,
    sigma = summary(model_out)$sigma)
}

visualize_relationship &amp;lt;- function(outcome, predictor, data = penguins, 
     add_regression_line = TRUE, add_smoother_line = FALSE) {
    formula &amp;lt;- as.formula(paste(outcome, predictor, sep = &amp;quot;~&amp;quot;))

    if(add_regression_line &amp;amp; !add_smoother_line) {
        gf_point(gformula = formula, data = data, size = 4) |&amp;gt;
          gf_smooth(method = &amp;#39;lm&amp;#39;, size = 1.5) |&amp;gt; print()
    }
    if(add_smoother_line &amp;amp; !add_regression_line) {
        gf_point(gformula = formula, data = data, size = 4) |&amp;gt;
          gf_smooth(method = &amp;#39;loess&amp;#39;, size = 1.5) |&amp;gt; print()
    }
    if(add_smoother_line &amp;amp; add_regression_line) {
        gf_point(gformula = formula, data = data, size = 4) |&amp;gt;
          gf_smooth(method = &amp;#39;lm&amp;#39;, size = 1.5) |&amp;gt;
          gf_smooth(method = &amp;#39;loess&amp;#39;, size = 1.5, linetype = 2) |&amp;gt; print()
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compute Correlation&lt;/h2&gt;
&lt;p&gt;The following code chunk can help you compute correlations between attributes. To use the function, you can replace “outcome” with an attribute name from the data above and “predictor” with another attribute above.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What happens when you flip the outcome / predictor outcomes when computing the correlation? Does the correlation change? Why or why not?&lt;/li&gt;
&lt;li&gt;Given the correlation computed, how is it interpreted?&lt;/li&gt;
&lt;li&gt;Given the correlation computed, what information would this tell us when we try estimate the regression coefficients below?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(outcome ~ predictor, data = penguins, use = &amp;#39;complete.obs&amp;#39;) |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-bivariate-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize bivariate distribution&lt;/h2&gt;
&lt;p&gt;The following code creates a scatter plot showing the bivariate association between the two attributes entered. Example code is shown below as an example. You can replace the “outcome” and “predictor” with the two continuous attributes that you are interested in exploring. These need to be entered in quotations, either single or double are fine. You can also add a regression line or smoother line by specifying those arguments as either TRUE (ie., Yes) or FALSE (ie., No).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Does the association between the two attributes appear to be linear?&lt;/li&gt;
&lt;li&gt;What happens to the association if you flip the predictor / outcome attributes?&lt;/li&gt;
&lt;li&gt;How would you summarize the association in a few sentences?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;visualize_relationship(outcome = &amp;#39;bill_length_mm&amp;#39;,
          predictor = &amp;#39;flipper_length_mm&amp;#39;,
          data = penguins,
          add_regression_line = TRUE,
          add_smoother_line = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing non-finite values (`stat_smooth()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02a-in-class-activity_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression-fitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression Fitting&lt;/h2&gt;
&lt;p&gt;Similar to the bivariate scatterplot, the following function was created to fit a linear regression and extract some information about the model. The output should include the Intercept, Slope, R-square, and sigma estimates. You can specify the outcome and predictor by replacing those as you did in the bivariate scatterplot above to reflect the attributes you are interested in exploring.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How are the 4 estimates interpreted, particularly in the context of the problem?&lt;/li&gt;
&lt;li&gt;What happens to the 4 estimates if you flip the outcome and predictor attributes?
&lt;ul&gt;
&lt;li&gt;Which one should truly be the outcome and what should guide this?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_fit(outcome = &amp;#39;bill_length_mm&amp;#39;,
          predictor = &amp;#39;flipper_length_mm&amp;#39;,
          data = penguins)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Intercept     Slope  Rsquare    sigma
## 1 -7.264868 0.2547682 0.430574 4.125874&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression Estimation</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/</link>
      <pubDate>Tue, 29 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;understanding-regression-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Understanding Regression Parameters&lt;/h1&gt;
&lt;p&gt;This section of notes aims to dig a bit more into what the simple linear regression (ie., regression with a single continuous covariate/attribute) parameter estimates mean. We will consider the estimation formulas in part of this to gain a sense of how these can be computed.&lt;/p&gt;
&lt;div id=&#34;new-example-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New Example Data&lt;/h2&gt;
&lt;p&gt;The new data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;43%&#34; /&gt;
&lt;col width=&#34;56%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;id&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;poc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;site_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;county&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-figure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Figure&lt;/h2&gt;
&lt;p&gt;Note, below I do a bit of post-processing to combine data from different POC values within a single CBSA.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&amp;#39;cbsa_name&amp;#39;, &amp;#39;date&amp;#39;)) %&amp;gt;% 
   drop_na()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in left_join(., wind, by = c(&amp;quot;cbsa_name&amp;quot;, &amp;quot;date&amp;quot;)): Detected an unexpected many-to-many relationship between `x` and `y`.
## ℹ Row 21 of `x` matches multiple rows in `y`.
## ℹ Row 1 of `y` matches multiple rows in `x`.
## ℹ If a many-to-many relationship is expected, set `relationship =
##   &amp;quot;many-to-many&amp;quot;` to silence this warning.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 13
##   date           id   poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 1/1/21  190130009     1  15.1        57 Water To… PM2.5 - Local Con…     47940
## 2 1/4/21  190130009     1  13.3        54 Water To… PM2.5 - Local Con…     47940
## 3 1/7/21  190130009     1  20.5        69 Water To… PM2.5 - Local Con…     47940
## 4 1/10/21 190130009     1  14.3        56 Water To… PM2.5 - Local Con…     47940
## 5 1/13/21 190130009     1  13.7        54 Water To… PM2.5 - Local Con…     47940
## 6 1/16/21 190130009     1   5.3        22 Water To… PM2.5 - Local Con…     47940
## # ℹ 5 more variables: cbsa_name &amp;lt;chr&amp;gt;, county &amp;lt;chr&amp;gt;, avg_wind &amp;lt;dbl&amp;gt;,
## #   max_wind &amp;lt;dbl&amp;gt;, max_wind_hours &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4821   13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(daily_aqi ~ avg_wind, data = airquality, size = 4, alpha = .15) %&amp;gt;%
  gf_labs(x = &amp;quot;Average daily wind speed (in knots)&amp;quot;,
          y = &amp;quot;Daily Air Quality&amp;quot;) %&amp;gt;%
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, color = &amp;#39;lightblue&amp;#39;, linetype = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(daily_aqi ~ avg_wind, data = airquality) |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm) |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##      48.223      -2.212&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$r.square |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.085&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$sigma |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.055&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;interpreting-these-estimates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting these estimates&lt;/h3&gt;
&lt;p&gt;What do these parameter estimates mean in this context?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; This is the model implied ______ when the ________ equals 0.&lt;br /&gt;
&lt;strong&gt;Slope:&lt;/strong&gt; For each 1 unit change in ____ there is a -2.2 unit decrease in &lt;strong&gt;&lt;em&gt;.&lt;br /&gt;
&lt;strong&gt;R-Square:&lt;/strong&gt; The &lt;/em&gt;&lt;/strong&gt;__ in ____ is explained by ____.&lt;br /&gt;
&lt;strong&gt;Sigma:&lt;/strong&gt; The _____ distance each point is from ____.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;centering-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Centering predictors&lt;/h2&gt;
&lt;p&gt;There are times when centering of predictors can be helpful for interpretation of the model parameters. This can be helpful when 0 is not a practically useful characteristic of the attribute or for more specific tests of certain elements of the X attribute.&lt;/p&gt;
&lt;div id=&#34;mean-centering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mean Centering&lt;/h3&gt;
&lt;p&gt;Mean centering is where the mean of the attribute is subtracted from each value. This is a linear transformation where each data point is subtracted by a constant, the mean. This means that the distance between points do not change.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality &amp;lt;- airquality %&amp;gt;%
  mutate(avg_wind_mc = avg_wind - mean(avg_wind),
         avg_wind_maxc = avg_wind - max(avg_wind),
         avg_wind_10 = avg_wind - 10)

gf_point(daily_aqi ~ avg_wind_mc, data = airquality, size = 4, alpha = .15) %&amp;gt;%
  gf_labs(x = &amp;quot;Average daily wind speed (in knots)&amp;quot;,
          y = &amp;quot;Daily Air Quality&amp;quot;) %&amp;gt;%
  gf_smooth() %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, color = &amp;#39;lightblue&amp;#39;, linetype = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/03-regression-estimates_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm_mc &amp;lt;- lm(daily_aqi ~ avg_wind_mc, data = airquality)
coef(air_lm_mc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) avg_wind_mc 
##   38.788011   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_mc)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_mc)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm_maxc &amp;lt;- lm(daily_aqi ~ avg_wind_maxc, data = airquality)
coef(air_lm_maxc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) avg_wind_maxc 
##      5.968391     -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_maxc)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_maxc)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm_10 &amp;lt;- lm(daily_aqi ~ avg_wind_10, data = airquality)
coef(air_lm_10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) avg_wind_10 
##   26.104968   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_10)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_10)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standardized Regression&lt;/h2&gt;
&lt;p&gt;Another type of regression that can be done is one in which the attributes are standardized prior to estimating the linear regression. What is meant by standardizing? This is converting the attributes into z-scores:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Z_{api} = \frac{(aqi - \bar{aqi})}{s_{aqi}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality &amp;lt;- airquality %&amp;gt;%
  mutate(z_aqi = scale(daily_aqi),
         z_aqi2 = (daily_aqi - mean(daily_aqi)) / sd(daily_aqi),
         z_wind = scale(avg_wind))

head(airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 19
##   date           id   poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 1/1/21  190130009     1  15.1        57 Water To… PM2.5 - Local Con…     47940
## 2 1/4/21  190130009     1  13.3        54 Water To… PM2.5 - Local Con…     47940
## 3 1/7/21  190130009     1  20.5        69 Water To… PM2.5 - Local Con…     47940
## 4 1/10/21 190130009     1  14.3        56 Water To… PM2.5 - Local Con…     47940
## 5 1/13/21 190130009     1  13.7        54 Water To… PM2.5 - Local Con…     47940
## 6 1/16/21 190130009     1   5.3        22 Water To… PM2.5 - Local Con…     47940
## # ℹ 11 more variables: cbsa_name &amp;lt;chr&amp;gt;, county &amp;lt;chr&amp;gt;, avg_wind &amp;lt;dbl&amp;gt;,
## #   max_wind &amp;lt;dbl&amp;gt;, max_wind_hours &amp;lt;dbl&amp;gt;, avg_wind_mc &amp;lt;dbl&amp;gt;,
## #   avg_wind_maxc &amp;lt;dbl&amp;gt;, avg_wind_10 &amp;lt;dbl&amp;gt;, z_aqi &amp;lt;dbl[,1]&amp;gt;, z_aqi2 &amp;lt;dbl&amp;gt;,
## #   z_wind &amp;lt;dbl[,1]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm_s &amp;lt;- lm(z_aqi ~ z_wind, data = airquality)
coef(air_lm_s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)        z_wind 
## -6.712023e-16 -2.920277e-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_s)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm_s)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9565091&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use this formula to convert any unstandardized regression coefficients into a standardized metric.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
b^{&amp;#39;}_{k} = b_{k} * \frac{s_{x_{k}}}{s_{y}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-2.211 * sd(airquality$avg_wind) / sd(airquality$daily_aqi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2919224&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(daily_aqi ~ avg_wind, data = airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.2920277&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameter Estimation&lt;/h2&gt;
&lt;p&gt;Now that we looked how the parameters are impacted by some changes in the model specification, how are these parameters actually estimated? I will show two ways, one is general, the other is specific to this simple case with a single predictor/covariate attribute. In general, linear regression (or more generally the general linear model) uses least square estimation. This means that the the parameters in the model minimize the squared distance between the observed and predicted values. That is, least squares estimates minimize this criterion:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum (Y - \hat{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;specific-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specific example&lt;/h3&gt;
&lt;p&gt;Calculus can be used to show that these two equations can be solved simultanuously to get estimates for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; that minimize the criterion above. These formulas are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
b_{1} = \frac{\sum(X - \bar{X})(Y - \bar{Y})}{\sum(X - \bar{X})^2}
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
b_{0} = \bar{Y} - b_{1}\bar{X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s use R to get these quantities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1 &amp;lt;- with(airquality, 
      sum((avg_wind - mean(avg_wind)) * (daily_aqi - mean(daily_aqi))) / sum((avg_wind - mean(avg_wind))^2)
)
b0 &amp;lt;- with(airquality, 
      mean(daily_aqi) - b1 * mean(avg_wind)
)
b0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 48.22295&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;general-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;General Approach&lt;/h3&gt;
&lt;p&gt;When there are more than one predictor, the number of equations gets a bit unyieldy, therefore, there is a general analytic approach that works for any set of predictor attributes. The general approach uses matrix algebra (anyone take linear algebra?), to achieve their estimates. This general form is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{b} = \left( \mathbf{X}^{`}\mathbf{X} \right)^{-1} \left( \mathbf{X}^{`} \mathbf{Y} \right).
\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{b}\)&lt;/span&gt; is a vector of estimated regression coefficients, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; is a matrix of covariate/predictor attributes (called the design matrix), and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}\)&lt;/span&gt; is a vector of the outcome attribute.&lt;/p&gt;
&lt;p&gt;Below, I show what these would look like for the air quality example that has been used and solve for the regression coefficients.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(air_lm)
head(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) avg_wind
## 1           1 2.941667
## 2           1 2.445833
## 3           1 1.995833
## 4           1 3.445833
## 5           1 1.116667
## 6           1 6.091667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Y &amp;lt;- as.matrix(airquality$daily_aqi)
head(Y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,]   57
## [2,]   54
## [3,]   69
## [4,]   56
## [5,]   54
## [6,]   22&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_X &amp;lt;- solve(t(X) %*% X)
X_X&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               (Intercept)      avg_wind
## (Intercept)  0.0008152474 -1.424894e-04
## avg_wind    -0.0001424894  3.340328e-05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_Y &amp;lt;- t(X) %*% Y
X_Y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               [,1]
## (Intercept) 186997
## avg_wind    731464&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_X %*% X_Y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  [,1]
## (Intercept) 48.222946
## avg_wind    -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>https://psqf6243.brandonlebeau.org/content/03-week3/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/03-week3/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This week is an introduction to linear regression. The primary goal of this week is to introduce the model and get familiar with the basic interpretation of parameters.&lt;/p&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Define a residual or error from a regression model.&lt;/li&gt;
&lt;li&gt;Explore benefits of centering predictor attributes.&lt;/li&gt;
&lt;li&gt;Engage with linear regression parameter estimation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Chapter 1 of &lt;a href=&#34;https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Regression: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Engage with &lt;a href=&#34;../../lectures/&#34;&gt;course notes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Optional, Read Chapter 7 of &lt;a href=&#34;https://lebebr01.github.io/stat_thinking/linear-regression.html#simple-regression-continuous-predictor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Reasoning through Computation and R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optional, Read 7.2 of &lt;a href=&#34;https://openintro-ims.netlify.app/model-slr.html#least-squares-regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Modern Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quiz 1 to come soon&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/actitivy/activity1/&#34;&gt;Activity 1&lt;/a&gt; - Due Around September 18th&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/actitivy/activity2/&#34;&gt;Activity 2&lt;/a&gt; - Due Around October 2nd&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>https://psqf6243.brandonlebeau.org/content/04-week4/</link>
      <pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/04-week4/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Digging deeper into linear regression. Time will be spent to show/prove via simulation the least square criterion that is minimized by the regression estimates. The foundation for inference using classical statistical methods will be explored.&lt;/p&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Understand the least squares minimization for regression.&lt;/li&gt;
&lt;li&gt;Explore inference for regression parameters.&lt;/li&gt;
&lt;li&gt;Define what a standard error is.&lt;/li&gt;
&lt;li&gt;Define null hypothesis significance testing (NHST)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Chapter 2 of &lt;a href=&#34;https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Regression: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read Chapter 3, section 1 of &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Engage with &lt;a href=&#34;../../lectures/&#34;&gt;course notes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Optional, Read 24.4 of &lt;a href=&#34;https://openintro-ims.netlify.app/inf-model-slr.html#mathslope&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Modern Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Quiz 2 &amp;ndash; To come &amp;hellip;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/actitivy/activity1/&#34;&gt;Activity 1&lt;/a&gt; - Due Around September 18th&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/actitivy/activity2/&#34;&gt;Activity 2&lt;/a&gt; - Due Around October 2nd&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Week 5</title>
      <link>https://psqf6243.brandonlebeau.org/content/05-week5/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/05-week5/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Digging deeper into linear regression inference using classical procedures. Further definitions of confidence intervals, sampling distribution, and interpretations of p-values. Regression conditions for valid inferences. Topics will explore evaluation of residuals for checking of conditions.&lt;/p&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Construct a confidence interval&lt;/li&gt;
&lt;li&gt;Define the sampling distribution&lt;/li&gt;
&lt;li&gt;Perform significance testing&lt;/li&gt;
&lt;li&gt;Interpret p-value as a continuous probability&lt;/li&gt;
&lt;li&gt;Define what a residual is&lt;/li&gt;
&lt;li&gt;Define common statistical conditions for linear regression&lt;/li&gt;
&lt;li&gt;Interpret residual plots&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Chapter 2 of &lt;a href=&#34;https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Regression: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read Chapter 3, section 1 of &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Engage with &lt;a href=&#34;../../lectures/&#34;&gt;course notes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Optional, Read 24.4 of &lt;a href=&#34;https://openintro-ims.netlify.app/inf-model-slr.html#mathslope&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Modern Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Section 11.1 of Regression and Other Stories &lt;a href=&#34;https://users.aalto.fi/~ave/ROS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression and Other Stories&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Optional, Read &lt;a href=&#34;https://doi.org/10.1080/00031305.2016.1154108&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASA statement on p-values&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/activity/activity1/&#34;&gt;Activity 1&lt;/a&gt; &amp;ndash; Due around September 18th&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/activity/activity2/&#34;&gt;Activity 2&lt;/a&gt; &amp;ndash; Due around October 2nd&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/quizzes/quiz2/&#34;&gt;Quiz 2&lt;/a&gt; &amp;ndash; Due September 25th&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Least Squares Minimization</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;example-to-show-least-squares-minimization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example to show least squares minimization&lt;/h2&gt;
&lt;p&gt;This little example is meant as a way to show the least square really minimizes the criterion, $ ( Y - )^2 $.&lt;/p&gt;
&lt;p&gt;In this example, we will generate some data so that we know what the truth is. Then, upon data generation, we will compute a bunch of different values for the linear slope and y-intercept. For each combination of the y-intercept and slope, I will compute the sum of squares error depicted above.&lt;/p&gt;
&lt;div id=&#34;simulate-some-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulate some data&lt;/h3&gt;
&lt;p&gt;The following example simulates data based on the following linear regression formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = 5 + 0.5 X + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;More explicitly, the simulation allows us to specify what the intercept and slope is in the population. These are specified below in the &lt;code&gt;reg_weights&lt;/code&gt; simulation argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(simglm)

theme_set(theme_bw(base_size = 18))

set.seed(2023)

sim_arguments &amp;lt;- list(
    formula = y ~ x,
    fixed = list(x = list(var_type = &amp;#39;continuous&amp;#39;, mean = 100, sd = 20)),
    error = list(variance = 100),
    sample_size = 1000,
    reg_weights = c(5, .5)
)

sim_data &amp;lt;- simulate_fixed(data = NULL, sim_arguments) |&amp;gt;
  simulate_error(sim_arguments) |&amp;gt;
  generate_response(sim_arguments)

head(sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   X.Intercept.         x level1_id      error fixed_outcome random_effects
## 1            1  98.32431         1 -2.6862147      54.16216              0
## 2            1  80.34113         2 -1.1886232      45.17056              0
## 3            1  62.49865         3  0.6545456      36.24933              0
## 4            1  96.27711         4 12.9091527      53.13855              0
## 5            1  87.33029         5 -4.5144816      48.66514              0
## 6            1 121.81595         6 19.5705749      65.90797              0
##          y
## 1 51.47594
## 2 43.98194
## 3 36.90387
## 4 66.04771
## 5 44.15066
## 6 85.47855&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-the-simulated-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize the Simulated Data&lt;/h3&gt;
&lt;p&gt;The following code visualizes the simulated data from above. What would you estimate the correlation to be?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;ggstance&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: scales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;scales&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggridges&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(y ~ x, data = sim_data, size = 4) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimate-regression-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimate Regression Coefficients&lt;/h3&gt;
&lt;p&gt;Even though we know what truth is, there is error involved in the simulation process, therefore, the population values specified above will not equal the exact regression coefficients estimated. Below, we estimate what those regression coefficients are.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_lm &amp;lt;- lm (y ~ x, data = sim_data)
coef(sim_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           x 
##   5.1260093   0.5002076&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-different-combinations-of-intercept-and-slope-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create different combinations of intercept and slope coefficients&lt;/h3&gt;
&lt;p&gt;The following code generates a sequence of intercept and corresponding slope conditions. We will use these different values to estimate the sum of squares error shown at the top of the notes for each of these intercept and slope values to show that the regression estimates are optimal to minimize the sum of square error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_intercept &amp;lt;- seq(0, 15, by = .25)
slope &amp;lt;- seq(0, 1.5, by = .01)

conditions &amp;lt;- rbind(expand.grid(y_intercept = y_intercept, 
                          slope = slope),
                          coef(sim_lm))

tail(conditions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      y_intercept     slope
## 9207   14.000000 1.5000000
## 9208   14.250000 1.5000000
## 9209   14.500000 1.5000000
## 9210   14.750000 1.5000000
## 9211   15.000000 1.5000000
## 9212    5.126009 0.5002076&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(conditions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9212    2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;showing-two-combinations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Showing Two Combinations&lt;/h3&gt;
&lt;p&gt;Here we visualize two possible slope conditions. Which one seems better for the data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(y ~ x, data = sim_data, size = 4) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;) |&amp;gt;
  gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 1), linetype = 2, size = 2) |&amp;gt;
  gf_abline(slope = ~slope, intercept = ~y_intercept, data = slice(conditions, 855), linetype = 2, color = &amp;#39;lightgreen&amp;#39;, size = 2) |&amp;gt;
  gf_refine(coord_cartesian(xlim = c(0, 160), ylim = c(0, 120)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/04-least-squares-simulation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-sum-of-squares-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute Sum of Squares Error&lt;/h3&gt;
&lt;p&gt;The following code creates a new function that computes the sum of square error. The function takes two arguments, the combination of intercept and slope values and the simulated data. The output is the sigma or average error from the regression line. The first code chunk below performs the computation for a single condition. The second code chunk does it for all of the conditions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_square_error &amp;lt;- function(conditions, sim_data) {
    fitted &amp;lt;- conditions[[&amp;#39;y_intercept&amp;#39;]] + conditions[[&amp;#39;slope&amp;#39;]] * sim_data[[&amp;#39;x&amp;#39;]]

    deviation &amp;lt;- sim_data[[&amp;#39;y&amp;#39;]] - fitted

    sqrt((sum(deviation^2) / (nrow(sim_data) - 2)))
}

sum_square_error(conditions[1892, ], sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 26.25721&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sim_lm)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9.738423&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(future)

plan(multicore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in supportsMulticoreAndRStudio(...): [ONE-TIME WARNING] Forked
## processing (&amp;#39;multicore&amp;#39;) is not supported when running R from RStudio because
## it is considered unstable. For more details, how to control forked processing
## or not, and how to silence this warning in future R sessions, see
## ?parallelly::supportsMulticore&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conditions$sse &amp;lt;- unlist(lapply(1:nrow(conditions), function(xx) sum_square_error(conditions[xx, ], sim_data)))

head(conditions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   y_intercept slope      sse
## 1        0.00     0 56.72588
## 2        0.25     0 56.48344
## 3        0.50     0 56.24108
## 4        0.75     0 55.99878
## 5        1.00     0 55.75655
## 6        1.25     0 55.51440&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 6</title>
      <link>https://psqf6243.brandonlebeau.org/content/06-week6/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/content/06-week6/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Exploration of utilizing a bootstrap for a robust alternative to inference for regression parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;objectives&#34;&gt;Objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Define the bootstrap procedure&lt;/li&gt;
&lt;li&gt;Define and explain the steps to conduct a bootstrap&lt;/li&gt;
&lt;li&gt;Interpret bootstrap results&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Chapter 2 of &lt;a href=&#34;https://dx-doi-org.proxy.lib.uiowa.edu/10.4135/9781483396774.n2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Regression: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read Chapter 3, section 1 of &lt;a href=&#34;https://web.stanford.edu/~hastie/ISLR2/ISLRv2_website.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Section 11.1 of Regression and Other Stories &lt;a href=&#34;https://users.aalto.fi/~ave/ROS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regression and Other Stories&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Engage with &lt;a href=&#34;../../lectures/&#34;&gt;course notes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;assignments&#34;&gt;Assignments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/activity/activity2/&#34;&gt;Activity 2&lt;/a&gt; &amp;ndash; Due around October 2nd&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/quizzes/quiz3/&#34;&gt;Quiz 3&lt;/a&gt; - Due October 2nd&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/assignment/assignment1/&#34;&gt;Assignment 1&lt;/a&gt; - Due around October 9th&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../assignments/assignment/assignment1/&#34;&gt;Assignment 2&lt;/a&gt; - Due around October 23rd&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Inference for regression parameters</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/05-regression-inference/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;inference-for-regression-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inference for regression parameters&lt;/h1&gt;
&lt;p&gt;It is often of interest to perform inference about the regression parameters that we have estimated thus far. The reason inference is useful is based on the idea that for most problems the sample is used to approximate the population. Therefore, a subset of the population (the sample) is used to estimate the population parameters that are of most interest. As such, our estimates come with error and this uncertainty we can quantify when making inference about the parameter estimates.&lt;/p&gt;
&lt;p&gt;In this course, I plan to show you two ways to perform this inference. One of those frameworks will be the classical approach which uses classical statistical theory to estimate the amount of uncertainty in the parameter estimates. The second approach will use the bootstrap as a way to computationally estimate the uncertainty. The benefit of the bootstrap is that it comes with fewer assumptions than the classical approach. We will build up to these arguments.&lt;/p&gt;
&lt;div id=&#34;classical-inferential-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classical Inferential Framework&lt;/h2&gt;
&lt;p&gt;The classical inferential framework, sometimes referred to as the null hypothesis significance test (NHST) has been around for more than 100 years. This framework builds off of the idea of a null hypothesis.&lt;/p&gt;
&lt;p&gt;A null hypothesis is typically thought as a hypothesis that assumes there is no relationship or a null effect. Framing this in the regression concept that we have been working with, we could define the following null hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \beta_{0} = 0.\ The\  population\  yintercept\  equals\  0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \beta_{1} = 0.\ The\  population\  slope\  equals\  0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the following two null hypotheses, represented with &lt;span class=&#34;math inline&#34;&gt;\(H_{0}\)&lt;/span&gt;, the population parameters are being assumed to be 0 in the population. This is one definition of a null effect, but is not the only null effect we can define (more on this later). The value defined as a null effect is important as it centers the sampling distribution used for evaluating where the sample estimate falls in that distribution.&lt;/p&gt;
&lt;p&gt;Another hypothesis is typically defined with the null hypothesis, called the alternative hypothesis. This hypothesis states that there is an effect. Within the linear regression framework, we could write the alternative hypotheses as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{A}: \beta_{0} \neq 0.\ The\  population\  yintercept\  is\  not\  equal\  to\  0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{A}: \beta_{1} \neq 0.\ The\  population\  slope\  is\  not\  equal\  to\  0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the following two alternative hypotheses, represented with &lt;span class=&#34;math inline&#34;&gt;\(H_{A}\)&lt;/span&gt;, the population parameters are assumed to be not equal to 0. These can also be one-sided, more on this with an example later.&lt;/p&gt;
&lt;div id=&#34;estimating-uncertainty-in-parameter-estimates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating uncertainty in parameter estimates&lt;/h3&gt;
&lt;p&gt;The standard error is used to estimate uncertainty or error in the parameter estimates due to having a sample from the population. More specifically, this means that the entire population is not used to estimate the parameter, therefore the estimate we have is very likely not equal exactly to the parameter. Instead, there is some sort of sampling error involved that we want to quantify. If the sample was collected well, ideally randomly, then the estimate should be unbiased. Unbiased here doesn’t mean that the estimate equals the population parameter, rather, that through repeated sampling, the average of our sample estimates would equal the population parameter.&lt;/p&gt;
&lt;p&gt;As mentioned, standard errors are used to quantify this uncertainty. In the linear regression case we have explored so far, there are mathematical formula for the standard errors. These are shown below.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SE\left( \hat{\beta}_{0} \right) = \sqrt{\hat{\sigma}^2 \left( \frac{1}{n} + \frac{\bar{X}^2}{\sum \left( X - \bar{X} \right)^2} \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SE\left( \hat{\beta}_{1} \right) = \sqrt{\frac{\hat{\sigma}^2}{\sum \left( X - \bar{X} \right)^2}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the equation above, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;, is equal to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{SS_{error}}{n - 2}}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size (ie, number of rows of data in the model).&lt;/p&gt;
&lt;div id=&#34;matrix-representation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Matrix Representation&lt;/h4&gt;
&lt;p&gt;It is also possible, and more easily extendable, to write the standard error computations or the variance of the estimated parameters in matrix representation. This framework extends beyond the single predictor case (ie. one &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;), therefore is more readily used in practice.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{var}\left({\hat{\beta}}\right) = \hat{\sigma}^2 \left( \mathbf{X}^{`}\mathbf{X} \right)^{-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the equation above, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;, is equal to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{SS_{error}}{n - 2}}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; is the design matrix from the regression analysis. Finally, to get the standard errors back, you take the square root of the diagonal elements.&lt;/p&gt;
&lt;p&gt;$$
SE( ) = &lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;43%&#34; /&gt;
&lt;col width=&#34;56%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;id&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;poc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;site_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;county&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Guiding Question&lt;/h4&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&amp;#39;cbsa_name&amp;#39;, &amp;#39;date&amp;#39;)) %&amp;gt;% 
   drop_na()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in left_join(., wind, by = c(&amp;quot;cbsa_name&amp;quot;, &amp;quot;date&amp;quot;)): Detected an unexpected many-to-many relationship between `x` and `y`.
## ℹ Row 21 of `x` matches multiple rows in `y`.
## ℹ Row 1 of `y` matches multiple rows in `x`.
## ℹ If a many-to-many relationship is expected, set `relationship =
##   &amp;quot;many-to-many&amp;quot;` to silence this warning.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08528019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18.05479&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum_x &amp;lt;- airquality %&amp;gt;%
    summarise(mean_wind = mean(avg_wind),
              sum_dev_x_sq = sum( (avg_wind - mean_wind) ^ 2))
sum_x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 2
##   mean_wind sum_dev_x_sq
##       &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1      4.27       29937.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se_b0 &amp;lt;- sqrt(summary(air_lm)$sigma^2 * ((1 / nrow(airquality)) + ( sum_x[[&amp;#39;mean_wind&amp;#39;]]^2 / sum_x[[&amp;#39;sum_dev_x_sq&amp;#39;]]) ))
se_b1 &amp;lt;- sqrt(summary(air_lm)$sigma^2 / sum_x[[&amp;#39;sum_dev_x_sq&amp;#39;]])

se_b0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.51551&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;se_b1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1043487&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(air_lm)

var_b &amp;lt;- summary(air_lm)$sigma^2 * solve(t(X) %*% X)
var_b &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             (Intercept)    avg_wind
## (Intercept)  0.26575054 -0.04644803
## avg_wind    -0.04644803  0.01088865&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(diag(var_b))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$coefficients[,2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;moving-toward-inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Moving toward inference&lt;/h3&gt;
&lt;p&gt;Now that there the parameters are estimated and the amount of uncertainty is quantified, inference is possible. There are two related pieces that can be computed now, a confidence interval and/or the test-statistic and p-value. Let’s go through both.&lt;/p&gt;
&lt;p&gt;First, a confidence interval can be computed. Confidence intervals take the following general form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta} \pm C * SE
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is the parameter estimate, &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the confidence level, and &lt;span class=&#34;math inline&#34;&gt;\(SE\)&lt;/span&gt; is the standard error. The parameter estimates and standard errors are what we have already established, the &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is the confidence level. This indicates the percentage of times, over the long run/repeated sampling, that the interval will capture the population parameter. Historically, this value is often specified as 95%, but any value is theoretically possible.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; value represents a quantile from a mathematical distribution that separates the middle percetage desired (ie, 95%) from the rest of the distribution. The mathematical distribution is most often the t-distribution, but the difference between a t-distribution and normal distribution are modest once the sample size is greater than 30 or so.&lt;/p&gt;
&lt;p&gt;The figure below tries to highlight the &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_30 &amp;lt;- data.frame(value = seq(-5, 5, .01), density = dt(seq(-5, 5, .01), df = 30))
gf_line(density ~ value, data = t_30) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.025, df = 30), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.975, df = 30), linetype = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05-regression-inference_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)$coefficients[,2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   0.5155100   0.1043487&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(qt(.025, df = nrow(airquality) -2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.960456&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;48.2 + c(-1, 1) * 1.96 * .5155&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 47.18962 49.21038&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;-2.211 + c(-1, 1) * 1.96 * .1043&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.415428 -2.006572&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inference-with-test-statistics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference with test statistics&lt;/h3&gt;
&lt;p&gt;It is also possible to do inference with a test statistic and computation of a p-value. Inference in this framework can be summarized into the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate hypotheses, ie null and alternative hypotheses.&lt;/li&gt;
&lt;li&gt;Establish an &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; value&lt;/li&gt;
&lt;li&gt;Estimate parameters&lt;/li&gt;
&lt;li&gt;Compute test statistic&lt;/li&gt;
&lt;li&gt;Generate p-value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; value is the &lt;strong&gt;level of significance&lt;/strong&gt; and &lt;em&gt;represents the probability of obtaining the results due to chance&lt;/em&gt;. This is a value that the researcher can select. For a 5% &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; value, this is what was used above to compute the confidence intervals.&lt;/p&gt;
&lt;p&gt;The test statistic is computed as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
test\ stat = \frac{\hat{\beta} - hypothesized\ value}{SE(\hat{\beta})}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is the estimated parameter, &lt;span class=&#34;math inline&#34;&gt;\(SE(\hat{\beta})\)&lt;/span&gt; is the standard error of the parameter estimate, and the &lt;span class=&#34;math inline&#34;&gt;\(hypothesized\ value\)&lt;/span&gt; is the hypothesized value from the null hypothesis. This is often 0, but does not need to be zero.&lt;/p&gt;
&lt;p&gt;Let’s assume the following null/alternative hypotheses:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \beta_{1} = 0  \\
H_{A}: \beta_{1} \neq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s use R to compute this test statistic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t = (-2.211 - 0) / .1043
t |&amp;gt; round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -21.198&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pt(-21.198, df = nrow(airquality) -2, lower.tail = TRUE) |&amp;gt; 
  round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_dist &amp;lt;- data.frame(value = seq(-25, 25, .05), density = dt(seq(-25, 25, .05), df = (nrow(airquality) - 2)))
gf_line(density ~ value, data = t_dist) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.025, df = (nrow(airquality) - 2)), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ qt(.975, df = (nrow(airquality) - 2)), linetype = 2) %&amp;gt;%
  gf_vline(xintercept = ~ -21.198, color = &amp;#39;red&amp;#39;) %&amp;gt;%
  gf_vline(xintercept = ~ 21.198, color = &amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05-regression-inference_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = daily_aqi ~ avg_wind, data = airquality)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -41.71 -14.38  -0.73  12.43  86.84 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  48.2229     0.5155   93.54   &amp;lt;2e-16 ***
## avg_wind     -2.2118     0.1043  -21.20   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 18.05 on 4819 degrees of freedom
## Multiple R-squared:  0.08528,	Adjusted R-squared:  0.08509 
## F-statistic: 449.3 on 1 and 4819 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 2.5 %    97.5 %
## (Intercept) 47.212311 49.233581
## avg_wind    -2.416369 -2.007227&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(air_lm, level = .5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  25 %     75 %
## (Intercept) 47.875214 48.57068
## avg_wind    -2.282185 -2.14141&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression in class</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;in-class-activity---september-13-2023&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In class activity - September 13, 2023&lt;/h1&gt;
&lt;p&gt;This activity is intended to give you some practice interpreting regression coefficients and also conducting inference.&lt;/p&gt;
&lt;p&gt;The data used for this activity comes from medical insurance claims.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Attribute&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;age&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Age of individual&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sex&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Sex of individual at birth&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bmi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Body mass index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;children&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Number of children covered on insurance&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;smoker&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Whether individuals smokes or not&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;region&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Region of the United States&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;charges&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;integer&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Medical charges&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following code chunk reads in the data and shows the first 6 rows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;charges &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1338 Columns: 7
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (3): sex, smoker, region
## dbl (4): age, bmi, children, charges
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(charges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
##     age sex      bmi children smoker region    charges
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1    19 female  27.9        0 yes    southwest  16885.
## 2    18 male    33.8        1 no     southeast   1726.
## 3    28 male    33          3 no     southeast   4449.
## 4    33 male    22.7        0 no     northwest  21984.
## 5    32 male    28.9        0 no     northwest   3867.
## 6    31 female  25.7        0 no     southeast   3757.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;descriptive-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Descriptive analysis&lt;/h2&gt;
&lt;p&gt;Pick 2 integer attributes and explore those two attributes descriptively. Think about key characteristics about these two attributes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the linear model&lt;/h2&gt;
&lt;p&gt;Fit the linear model to estimate the regression parameters. Then do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Extract the linear model estimates.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Example text&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Interpret the slope and intercept.&lt;/li&gt;
&lt;li&gt;Interpret the standard errors&lt;/li&gt;
&lt;li&gt;Interpret overall model fit.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;conduct-inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conduct Inference&lt;/h2&gt;
&lt;p&gt;Suppose we wished to use this data to make broader claims to a population of interest. Let us perform classical inference using the NHST framework. Do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Establish the statistical hypotheses.&lt;/li&gt;
&lt;li&gt;Extract and interpret the test-statistics and p-values&lt;/li&gt;
&lt;li&gt;What do the test-statistics and p-values tell us about the statistical hypotheses?&lt;/li&gt;
&lt;li&gt;What do the test-statistics and p-values tell us about the magnitude (i.e., size) of effect?&lt;/li&gt;
&lt;li&gt;What do the test-statistics and p-values tell us about the practical implications of the effect?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-confidence-interval&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compute confidence interval&lt;/h2&gt;
&lt;p&gt;Compute a confidence interval in the following steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Establish the confidence level&lt;/li&gt;
&lt;li&gt;Compute the confidence interval.&lt;/li&gt;
&lt;li&gt;Interpret the confidence interval.&lt;/li&gt;
&lt;li&gt;What does the confidence interval tell us about magnitude or practical implications of the effect?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ charges, data = charges)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(~ charges, data = charges, mean, median, sd, IQR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response     mean   median       sd      IQR
## 1  charges 13270.42 9382.033 12110.01 11899.63&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(charges ~ age, data = charges, size = 5, alpha = .25) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(charges ~ age, data = charges, color = ~ sex, size = 5, alpha = .25) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(charges ~ age, data = charges, color = ~ smoker, size = 5, alpha = .25) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(charges ~ age, data = charges, color = ~ region, size = 5, alpha = .25) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(charges ~ age, data = charges, color = ~ smoker, size = 5, alpha = .25) |&amp;gt;
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 3) |&amp;gt; 
  gf_facet_wrap(~ sex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/05a-regression-in-class_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(charges ~ age, data = charges,  mean, median, sd, min, max)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    response age      mean    median        sd       min      max
## 1   charges  18  7086.218  2198.190 10198.460  1121.874 38792.69
## 2   charges  19  9747.909  2135.892 12469.537  1241.565 39722.75
## 3   charges  20 10159.698  2459.720 12049.625  1391.529 38344.57
## 4   charges  21  4730.464  2254.424  6168.059  1515.345 26018.95
## 5   charges  22 10012.933  2641.156 14653.364  1665.000 44501.40
## 6   charges  23 12419.820  3594.538 13421.332  1815.876 40904.20
## 7   charges  24 10648.016  3045.138 12203.651  1969.614 38126.25
## 8   charges  25  9838.365  3750.149 11551.289  2137.654 42112.24
## 9   charges  26  6133.825  3388.882  7765.729  2302.300 36085.22
## 10  charges  27 12184.702  4544.324 11941.822  2483.736 39611.76
## 11  charges  28  9069.188  4344.951 11428.968  2689.495 51194.56
## 12  charges  29 10430.159  4906.410 10680.198  2866.091 44585.46
## 13  charges  30 12719.110  4837.582 12515.212  3554.203 40932.43
## 14  charges  31 10196.981  4738.268 13646.443  3260.199 58571.07
## 15  charges  32  9220.300  4672.016  9346.679  3866.855 37607.53
## 16  charges  33 12351.533  6210.083 12877.616  3704.354 55135.40
## 17  charges  34 11613.528  5490.092 11735.073  3935.180 43943.88
## 18  charges  35 11307.182  5836.520 10699.982  4746.344 39983.43
## 19  charges  36 12204.476  5478.037 12884.342  4399.731 43753.34
## 20  charges  37 18019.912  6985.507 15125.287  4646.759 46113.51
## 21  charges  38  8102.734  6455.863  7328.907  5383.536 41949.24
## 22  charges  39 11778.243  7512.267  8633.210  5649.715 40103.89
## 23  charges  40 11772.251  7077.189  9908.913  5415.661 40003.33
## 24  charges  41  9653.746  6875.961  8859.592  5699.837 40273.65
## 25  charges  42 13061.039  7443.643 10707.617  5966.887 43896.38
## 26  charges  43 19267.279 18767.738 13529.394  6250.435 45863.21
## 27  charges  44 15859.397  8023.135 14394.949  6948.701 48885.14
## 28  charges  45 14830.200  8603.823 13722.542  7222.786 62592.87
## 29  charges  46 14342.591  8825.086 12159.483  7147.105 46151.12
## 30  charges  47 17654.000  9715.841 12208.024  8062.764 44202.65
## 31  charges  48 14632.500  9447.382 10163.267  7789.635 45702.02
## 32  charges  49 12696.006  9681.120  7531.013  8116.680 39727.61
## 33  charges  50 15663.003 10107.221 10939.124  8442.667 42856.84
## 34  charges  51 15682.256  9875.680 12707.113  8782.469 47462.89
## 35  charges  52 18256.270 11396.900 12408.353  9140.951 60021.40
## 36  charges  53 16020.931 11157.174 10096.072  9487.644 46661.44
## 37  charges  54 18758.546 11816.449 14216.087  9850.432 63770.43
## 38  charges  55 16164.545 11880.231 10107.690 10214.636 44423.80
## 39  charges  56 15025.516 11658.247  9204.315 10577.087 43921.18
## 40  charges  57 16447.185 11893.878  9987.941 10959.330 48675.52
## 41  charges  58 13878.928 11931.125  7437.417 11345.519 47496.49
## 42  charges  59 18895.870 12928.791 11170.560 11743.299 48970.25
## 43  charges  60 21979.419 13204.286 14923.698 12142.579 52590.83
## 44  charges  61 22024.458 13635.638 12525.159 12557.605 48517.56
## 45  charges  62 19163.857 13844.797 10320.270 12957.118 46718.16
## 46  charges  63 19884.998 14349.854 11850.935 13390.559 48824.45
## 47  charges  64 23275.531 15528.758 13029.842 13822.803 49577.66&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(charges ~ age, data = charges) |&amp;gt; 
round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.299&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Type text &lt;strong&gt;bold&lt;/strong&gt; &lt;em&gt;italic&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;charge_mod &amp;lt;- lm(charges ~ age, data = charges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;charge_mod_c &amp;lt;- lm(charges ~ I(age - min(age)), data = charges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(charge_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = charges ~ age, data = charges)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -8059  -6671  -5939   5440  47829 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   3165.9      937.1   3.378 0.000751 ***
## age            257.7       22.5  11.453  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 11560 on 1336 degrees of freedom
## Multiple R-squared:  0.08941,	Adjusted R-squared:  0.08872 
## F-statistic: 131.2 on 1 and 1336 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(charge_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    3166.     937.       3.38 7.51e- 4
## 2 age             258.      22.5     11.5  4.89e-29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::tidy(charge_mod_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term              estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)          7805.     572.       13.6 9.77e-40
## 2 I(age - min(age))     258.      22.5      11.5 4.89e-29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;broom::glance(charge_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squared  sigma statistic  p.value    df  logLik    AIC    BIC
##       &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1    0.0894        0.0887 11560.      131. 4.89e-29     1 -14415. 28836. 28852.
## # ℹ 3 more variables: deviance &amp;lt;dbl&amp;gt;, df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(charge_mod, level = 0.99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                0.5 %    99.5 %
## (Intercept) 748.4946 5583.2755
## age         199.6774  315.7679&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Assignment 1</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/assignment/assignment1/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/assignment/assignment1/</guid>
      <description>&lt;p&gt;The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed assignment per group on ICON. &lt;em&gt;Please make sure to add everyone&amp;rsquo;s name to the submission&lt;/em&gt;, this can be a comment on ICON or on the document itself.&lt;/p&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-turn-in&#34;&gt;What to turn in&lt;/h3&gt;
&lt;p&gt;Please turn in a document that contains the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answers to the questions below&lt;/li&gt;
&lt;li&gt;include any relevant statistics/figures that support your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally,upload the final document to &lt;a href=&#34;https://uiowa.instructure.com/courses/214414/assignments/2006011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICON&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;including-statistical-evidence&#34;&gt;Including Statistical Evidence&lt;/h3&gt;
&lt;p&gt;This assignment is worth a total of 10 points spread over 9 questions. Please make sure to include any statistical evidence to support your statements, this could include graphics or statistics. This assignment also gives you a choice about what question you are interested in exploring within these data, therefore, including the statistical evidence is extremely important. &lt;em&gt;Failure to include statistical evidence to support claims will result in a&lt;/em&gt; &lt;strong&gt;1pt deduction&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;due-date&#34;&gt;Due Date&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Due around&lt;/em&gt; &lt;strong&gt;October 9th, 2023&lt;/strong&gt;. No penalty for late submissions as long as it is submitted by December 11th.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this assignment is tuition and salary data for higher education institutions. The data originally come from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-10/readme.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy Tuesday&lt;/a&gt;. I have done some processing to join two tables together with fuzzy string matching for you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data to use:&lt;/strong&gt; The data can be obtained in &lt;a href=&#34;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/salary_tuition.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csv format&lt;/a&gt; from GitHub. The data are also posted to the data folder within the IDAS. A short description for each attribute is as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Name of school&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;state_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;state name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;early_career_pay&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated early career pay in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mid_career_pay&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated mid career pay in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;make_world_better_percent&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Percent of alumni who think they are making the world a better place&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stem_percent&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Percent of student body in STEM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;type&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Type: Public, private, for-profit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;degree_length&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4 year or 2 year degree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;room_and_board&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Room and board in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;in_state_tuition&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tuition for in-state residents in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;in_state_total&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total cost for in-state residents in USD (sum of room &amp;amp; board + in state tuition)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;out_of_state_tuition&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tuition for out-of-state residents in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;out_of_state_total&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total cost for in-state residents in USD (sum of room &amp;amp; board + out of state tuition)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Each question is worth 1 pt unless otherwise specified.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Identify a research question of interest that can use linear regression to answer the question.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore the research question you identified in #1 descriptively. In a few sentences, summarize any potential relationship, including statistical evidence (i.e., figures or statistics) to support your statements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit the linear regression to answer your question from #1. Interpret the regression coefficients in the context of the problem at hand. That is, what do the coefficients mean for the attributes included in the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Is the model intercept as interpretable as it could be? What could be done to enhance the interpretation of the intercept? Summarize in a few sentences any theory or data elements that may help to decide how to improve the interpretation of the intercept.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interpret the model estimates that show how well the model is performing. That is, what model statistics help to understand how well the model representing the outcome attribute? What do these statistics mean in the context of the problem?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore and evaluate the model assumptions/data conditions regarding the residual/error term. Summarize how well the model is meeting the assumptions, citing specific statistics or visualizations to justify your conclusions. &lt;strong&gt;2 pts&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Write out the null and alternative hypotheses based on your research question from #1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Summarize the statistical evidence surrounding the null or alternative hypotheses from question 7. In a few sentences, describe if the evidence provides support for or against the null hypothesis. Provide specific statistical evidence to support your justification.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on the justification in #8, what practical implications does this result have? That is, are the statistical results that you have been describing/summarizing throughout this assignment practically relevant? Be as specific as you can in your discussion about why you believe the results are practically useful or not.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/assignment/assignment2/</link>
      <pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/assignment/assignment2/</guid>
      <description>&lt;p&gt;The following assignment is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed assignment per group on ICON. &lt;em&gt;Please make sure to add everyone&amp;rsquo;s name to the submission&lt;/em&gt;, this can be a comment on ICON or on the document itself.&lt;/p&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-turn-in&#34;&gt;What to turn in&lt;/h3&gt;
&lt;p&gt;Please turn in a document that contains the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answers to the questions below&lt;/li&gt;
&lt;li&gt;include any relevant statistics/figures that support your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally,upload the final document to &lt;a href=&#34;https://uiowa.instructure.com/courses/214414/assignments/2006012&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICON&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;including-statistical-evidence&#34;&gt;Including Statistical Evidence&lt;/h3&gt;
&lt;p&gt;This assignment is worth a total of 10 points spread over 7 questions. Please make sure to include any statistical evidence to support your statements, this could include graphics or statistics. This assignment also gives you a choice about what question you are interested in exploring within these data, therefore, including the statistical evidence is extremely important. &lt;em&gt;Failure to include statistical evidence to support claims will result in a&lt;/em&gt; &lt;strong&gt;1pt deduction&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;due-date&#34;&gt;Due Date&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Due around&lt;/em&gt; &lt;strong&gt;October 23rd, 2023&lt;/strong&gt;. No penalty for late submissions as long as it is submitted by December 11th.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this assignment is tuition and salary data for higher education institutions. The data originally come from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-10/readme.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy Tuesday&lt;/a&gt;. I have done some processing to join two tables together with fuzzy string matching for you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data to use:&lt;/strong&gt; The data can be obtained in &lt;a href=&#34;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/salary_tuition.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csv format&lt;/a&gt; from GitHub. The data are also posted to the data folder within the IDAS. A short description for each attribute is as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Name of school&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;state_name&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;state name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;early_career_pay&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated early career pay in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mid_career_pay&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated mid career pay in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;make_world_better_percent&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Percent of alumni who think they are making the world a better place&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stem_percent&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Percent of student body in STEM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;type&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Type: Public, private, for-profit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;degree_length&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;4 year or 2 year degree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;room_and_board&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Room and board in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;in_state_tuition&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tuition for in-state residents in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;in_state_total&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total cost for in-state residents in USD (sum of room &amp;amp; board + in state tuition)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;out_of_state_tuition&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tuition for out-of-state residents in USD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;out_of_state_total&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Total cost for in-state residents in USD (sum of room &amp;amp; board + out of state tuition)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;note&#34;&gt;Note&lt;/h2&gt;
&lt;p&gt;This assignment builds off of &lt;a href=&#34;assignment1/&#34;&gt;assignment 1&lt;/a&gt;, therefore, please complete assignment 1 first. Review assignment 1 briefly prior to starting this assignment.&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Each question is worth 1 pt unless otherwise specified.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Based on your first assignment, are there attributes that could have been missing from the linear regression fitted in assignment 1? That is, is the assumption of all attributes being included in the model appropriate? Why or why not? If not, what other attributes may be of interest?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add one or more of the attributes identified in question 1 to create a multiple regression model. That is, add one or more of the attributes from question 1 while keeping the attribute from assignment 1 into the regression model. Summarize the interpretation of the regression coefficient estimates for this multiple regression model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Estimate model fit indices to compare the model from assignment 1 to the multiple regression fitted in question 2. Does the model improve model fit based on that from assignment 1? Why or why not?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore and evaluate the model assumptions regarding the residual/error term. Summarize how well the model is meeting the assumptions, citing specific statistics or visualizations to justify your conclusions. In addition, do the model assumptions seem better met compared to those from the regression in assignment 1? &lt;strong&gt;2 pts&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Summarize the statistical evidence surrounding the null or alternative hypotheses that are being explored for the coefficients entered into the model. Note, I did not explicitly ask you for null/alternative hypotheses, but you may want to write those down for your reference. In a few sentences, describe if the evidence provides support for or against the null hypothesis. Provide specific statistical evidence to support your justification. &lt;strong&gt;2 pts&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create confidence intervals for the coefficients in the multiple regression model. Justify your confidence level and interpret the confidence intervals in the context of the data. What do these confidence intervals suggest about the magnitude of effect? &lt;strong&gt;2 pts&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on the justification in #5 and #6, what practical implications does this result have? That is, are the statistical results that you have been describing/summarizing throughout this assignment practically relevant? Be as specific as you can in your discussion about why you believe the results are practically useful or not.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Conditions for Linear Regression</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;conditions-for-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conditions for Linear Regression&lt;/h1&gt;
&lt;p&gt;The conditions surrounding linear regression typically surround the residuals. Residuals are defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y - \hat{Y}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These are the deviations in the observed scores from the predicted scores from the linear regression. Recall, through least square estimation that these residauls will sum to 0, therefore, their mean would also be equal to 0. However, there are certain conditions about these residuals that are made for the linear regression model to have the inferences be appropriate. We’ll talk more about what the implications for violating these conditions will have on the linear regression model, but first, the conditions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Approximately Normally distributed residuals&lt;/li&gt;
&lt;li&gt;Homogeneity of variance&lt;/li&gt;
&lt;li&gt;Uncorrelated residuals&lt;/li&gt;
&lt;li&gt;Error term is uncorrelated with the predictor attribute&lt;/li&gt;
&lt;li&gt;Linearity and additivity of the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these will be discussed in turn.&lt;/p&gt;
&lt;p&gt;Data conditions that are not directly testable:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Validity&lt;/li&gt;
&lt;li&gt;Representativeness&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;approximately-normally-distributed-residuals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approximately Normally distributed residuals&lt;/h2&gt;
&lt;p&gt;The first assumption is that the residuals are at least approximately Normally distributed. This assumption is really only much of a concern when the sample size is small. If the sample size is larger, the Central Limit Thereom (CLT) states that the distribution of the statstics will be approximately normally distributed. The threshold for the CLT to be properly invoked is about 30. Larger then this, the residuals do not need to be approximately normally distributed. Even still, exploring the distribution of the residuals can still be helpful and can also be helpful to identify potential extreme values.&lt;/p&gt;
&lt;p&gt;This example will make use of the air quality data one more time.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The data for this section of notes will explore data from the &lt;a href=&#34;https://www.epa.gov/outdoor-air-quality-data&#34;&gt;Environmental Protection Agency on Air Quality&lt;/a&gt; collected for the state of Iowa in 2021. The data are daily values for PM 2.5 particulates. The attributes included in the data are shown below with a short description.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;43%&#34; /&gt;
&lt;col width=&#34;56%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Variable&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Date of observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;id&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;poc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Parameter Occurrence Code (POC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;pm2.5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily pm 2.5 particulate value, in (ug/m3; micrograms per meter cubed)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;daily_aqi&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average air quality index&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;site_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Site Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;aqs_parameter_desc&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Text Description of Observation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_code&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Core Based Statistical Area (CBSA) ID&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;cbsa_name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CBSA Name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;county&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;County in Iowa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;avg_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Average daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Maximum daily wind speed (in knots)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;max_wind_hours&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Time of maximum daily wind speed&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Guiding Question&lt;/h4&gt;
&lt;p&gt;How is average daily wind speed related to the daily air quality index?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following object is masked from &amp;#39;package:scales&amp;#39;:
## 
##     rescale
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

airquality &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/iowa_air_quality_2021.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 6917 Columns: 10
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (5): date, site_name, aqs_parameter_desc, cbsa_name, county
## dbl (5): id, poc, pm2.5, daily_aqi, cbsa_code
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wind &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/daily_WIND_2021-iowa.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 1537 Columns: 5
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr (2): date, cbsa_name
## dbl (3): avg_wind, max_wind, max_wind_hours
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality &amp;lt;- airquality %&amp;gt;%
   left_join(wind, by = c(&amp;#39;cbsa_name&amp;#39;, &amp;#39;date&amp;#39;)) %&amp;gt;% 
   drop_na()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in left_join(., wind, by = c(&amp;quot;cbsa_name&amp;quot;, &amp;quot;date&amp;quot;)): Detected an unexpected many-to-many relationship between `x` and `y`.
## ℹ Row 21 of `x` matches multiple rows in `y`.
## ℹ Row 1 of `y` matches multiple rows in `x`.
## ℹ If a many-to-many relationship is expected, set `relationship =
##   &amp;quot;many-to-many&amp;quot;` to silence this warning.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;air_lm &amp;lt;- lm(daily_aqi ~ avg_wind, data = airquality)
coef(air_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)    avg_wind 
##   48.222946   -2.211798&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The residuals can be saved with the &lt;code&gt;resid()&lt;/code&gt; function. These can also be added to the original data, which are particularly helpful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(resid(air_lm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  15.283427  11.186742  25.191433  15.398540   8.246896 -12.749410&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality$residuals &amp;lt;- resid(air_lm)
head(airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 14
##   date           id   poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 1/1/21  190130009     1  15.1        57 Water To… PM2.5 - Local Con…     47940
## 2 1/4/21  190130009     1  13.3        54 Water To… PM2.5 - Local Con…     47940
## 3 1/7/21  190130009     1  20.5        69 Water To… PM2.5 - Local Con…     47940
## 4 1/10/21 190130009     1  14.3        56 Water To… PM2.5 - Local Con…     47940
## 5 1/13/21 190130009     1  13.7        54 Water To… PM2.5 - Local Con…     47940
## 6 1/16/21 190130009     1   5.3        22 Water To… PM2.5 - Local Con…     47940
## # ℹ 6 more variables: cbsa_name &amp;lt;chr&amp;gt;, county &amp;lt;chr&amp;gt;, avg_wind &amp;lt;dbl&amp;gt;,
## #   max_wind &amp;lt;dbl&amp;gt;, max_wind_hours &amp;lt;dbl&amp;gt;, residuals &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ residuals, data = airquality) %&amp;gt;%
  gf_labs(x = &amp;quot;Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(airquality, aes(sample = residuals)) + 
  stat_qq(size = 5) + 
  stat_qq_line(size = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;standardized-residuals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standardized Residuals&lt;/h3&gt;
&lt;p&gt;Standardized residuals can be another way to explore the residuals. These will now be standardized to have a variance of 1, similar to that of a Z-score. These can be computed as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
standardized\ residuals = \frac{\epsilon_{i}}{SD_{\epsilon}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Within R, these can be computed using the function &lt;code&gt;rstandard()&lt;/code&gt;. Furthermore, these can be computed from another package called broom with the &lt;code&gt;augment()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(rstandard(air_lm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  0.8466153  0.6196983  1.3955421  0.8529766  0.4568937 -0.7062638&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)

resid_diagnostics &amp;lt;- augment(air_lm)
head(resid_diagnostics)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 8
##   daily_aqi avg_wind .fitted .resid     .hat .sigma   .cooksd .std.resid
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1        57     2.94    41.7  15.3  0.000266   18.1 0.0000953      0.847
## 2        54     2.45    42.8  11.2  0.000318   18.1 0.0000611      0.620
## 3        69     2.00    43.8  25.2  0.000380   18.1 0.000370       1.40 
## 4        56     3.45    40.6  15.4  0.000230   18.1 0.0000836      0.853
## 5        54     1.12    45.8   8.25 0.000539   18.1 0.0000563      0.457
## 6        22     6.09    34.7 -12.7  0.000319   18.1 0.0000795     -0.706&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ .std.resid, data = resid_diagnostics) %&amp;gt;%
  gf_labs(x = &amp;quot;Standardized Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(resid_diagnostics, aes(sample = .std.resid)) + 
  stat_qq(size = 5) + 
  stat_qq_line(size = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;homogeneity-of-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Homogeneity of variance&lt;/h2&gt;
&lt;p&gt;Homoegeneity of variance is an assumption that is of larger concern compared to normality of the residuals. Homoogeneity of variance is an assumption that states that the variance of the residuals are similar across the predicted or fitted values from the regression line. This assumption can be explored by looking at the residuals (standardized or raw residuals), by the fitted or predicted values. Within this plot, the range of residuals should be similar across the range of fitted or predicted values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &amp;#39;loess&amp;#39;, size = 2) %&amp;gt;%
  gf_labs(x = &amp;#39;Fitted Values&amp;#39;,
          y = &amp;#39;Residuals&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(.std.resid ~ .fitted, data = resid_diagnostics, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &amp;#39;loess&amp;#39;, size = 2) %&amp;gt;%
  gf_labs(x = &amp;#39;Fitted Values&amp;#39;,
          y = &amp;#39;Standardized Residuals&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another figure that can also be helpful for the homogeneity of variance assumption is one that rescales the residuals on the y-axis. The rescaling makes all the standardized residuals positive (takes the absolute value) and then takes the square root of this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resid_diagnostics %&amp;gt;%
  mutate(sqrt_abs_sresid = sqrt(abs(.std.resid))) %&amp;gt;%
  gf_point(sqrt_abs_sresid ~ .fitted, size = 5, alpha = .15) %&amp;gt;%
  gf_smooth(method = &amp;#39;loess&amp;#39;, size = 2) %&amp;gt;%
  gf_labs(x = &amp;#39;Fitted Values&amp;#39;,
          y = &amp;#39;Sqrt Abs Standardized Residuals&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-with-high-leverage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data with high leverage&lt;/h2&gt;
&lt;p&gt;Data with high leverage are extreme values that may significantly impact the regression estimates. These statistics include extreme values for the outcome or predictor attributes. Cook’s distance is one statistic that can help to identify points with high impact/leverage for the regression estimates. Cook’s distance is a statistic that represents how much change there would be in the fitted values if the point was removed when estimating the regression coefficients. There is some disagreement between what type of thresholds to use for Cook’s distance, but one rule of thumb is Cook’s distance greater than 1. There has also been some research showing that Cook’s distance follows an F-distribution, so a more specific value could be computed. The rule of thumb for greater than 1 comes from the F distribution for large samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resid_diagnostics %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_col(.cooksd ~ obs_num, fill = &amp;#39;black&amp;#39;, color = &amp;#39;black&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Cook&amp;#39;s Distance&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another leave one out statistic is the studentized deleted residuals. These are computed by removing a data point, refitting the regression model, then generate a predicted value for the X value for the data point removed. Then the residual is computed the same as before and is standardized like the standardized residuals above. The function in R to compute these is &lt;code&gt;rstudent()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(rstudent(air_lm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1          2          3          4          5          6 
##  0.8465904  0.6196587  1.3956793  0.8529524  0.4568561 -0.7062271&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality$student_residuals &amp;lt;- rstudent(air_lm)
head(airquality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 15
##   date           id   poc pm2.5 daily_aqi site_name aqs_parameter_desc cbsa_code
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 1/1/21  190130009     1  15.1        57 Water To… PM2.5 - Local Con…     47940
## 2 1/4/21  190130009     1  13.3        54 Water To… PM2.5 - Local Con…     47940
## 3 1/7/21  190130009     1  20.5        69 Water To… PM2.5 - Local Con…     47940
## 4 1/10/21 190130009     1  14.3        56 Water To… PM2.5 - Local Con…     47940
## 5 1/13/21 190130009     1  13.7        54 Water To… PM2.5 - Local Con…     47940
## 6 1/16/21 190130009     1   5.3        22 Water To… PM2.5 - Local Con…     47940
## # ℹ 7 more variables: cbsa_name &amp;lt;chr&amp;gt;, county &amp;lt;chr&amp;gt;, avg_wind &amp;lt;dbl&amp;gt;,
## #   max_wind &amp;lt;dbl&amp;gt;, max_wind_hours &amp;lt;dbl&amp;gt;, residuals &amp;lt;dbl&amp;gt;,
## #   student_residuals &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airquality %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_point(student_residuals ~ obs_num, size = 5, alpha = .15) %&amp;gt;%
  gf_hline(yintercept = ~ 3, color = &amp;#39;blue&amp;#39;, size = 2) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Studentized Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Leverage can be another measure to help detect outliers in X values. The hat values that were computed from the &lt;code&gt;augment()&lt;/code&gt; function above and can be interpreted as the distance the X scores are from the center of all X predictors. In the case of a single predictor, the hat values are the distance the X score is from the mean of X. The hat values will also sum up to the number of predictors and will always range between 0 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resid_diagnostics %&amp;gt;%
  mutate(obs_num = 1:n()) %&amp;gt;%
  gf_col(.hat ~ obs_num, fill = &amp;#39;black&amp;#39;, color = &amp;#39;black&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Observation Number&amp;quot;,
          y = &amp;quot;Hat Values (leverage)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(air_lm, which = 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-17-3.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-17-4.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/06-regression-conditions_files/figure-html/unnamed-chunk-17-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions-to-overcome-data-conditions-not-being-met&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions to overcome data conditions not being met&lt;/h2&gt;
&lt;p&gt;There are some strategies that can be done when a variety of data conditions for linear regression have not been met. In general, the strategies stem around generalizing the model and adding some complexity.&lt;/p&gt;
&lt;p&gt;The following are options that would be possible:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Add interactions or other non-linear terms&lt;/li&gt;
&lt;li&gt;Use more complicated model
&lt;ul&gt;
&lt;li&gt;weighted least squares for homogeneity of variance concerns&lt;/li&gt;
&lt;li&gt;models that include measurement error&lt;/li&gt;
&lt;li&gt;mixed models for correlated data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Transform the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrap</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/07-bootstrap/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/07-bootstrap/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;bootstrap&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bootstrap&lt;/h1&gt;
&lt;p&gt;The bootstrap is an alternative to the NHST framework already discussed. The primary benefit of the bootstrap is that it comes with fewer assumptions then the NHST framework. The only real assumption when doing a bootstrap approach is that the sample is obtained randomly from the population, an assumption already made in the NHST framework. The primary drawback of the bootstrap approach is that it is computationally expensive, therefore, it can take time to peform the procedure.&lt;/p&gt;
&lt;div id=&#34;bootstrapping-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrapping Steps&lt;/h2&gt;
&lt;p&gt;The following are the steps when performing a bootstrap.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Treat the sample data as the population.&lt;/li&gt;
&lt;li&gt;Resample, with replacement, from the sample data, ensuring the new sample is the same size as the original.&lt;/li&gt;
&lt;li&gt;Estimate the model using the resampled data from step 2.&lt;/li&gt;
&lt;li&gt;Repeat steps 2 and 3 many many times (eg, 10,000 or more).&lt;/li&gt;
&lt;li&gt;Visualize distribution of effect of interest&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;resampling-with-replacement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resampling with replacement&lt;/h2&gt;
&lt;p&gt;What is meant by sampling with replacement? Let’s do an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fruit &amp;lt;- data.frame(name = c(&amp;#39;watermelon&amp;#39;, &amp;#39;apple&amp;#39;, &amp;#39;orange&amp;#39;, &amp;#39;kumquat&amp;#39;, &amp;#39;grapes&amp;#39;, &amp;#39;canteloupe&amp;#39;, &amp;#39;kiwi&amp;#39;, &amp;#39;banana&amp;#39;)) |&amp;gt;
    mutate(obs_num = 1:n())

fruit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1 watermelon       1
## 2      apple       2
## 3     orange       3
## 4    kumquat       4
## 5     grapes       5
## 6 canteloupe       6
## 7       kiwi       7
## 8     banana       8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;slice_sample(fruit, n = nrow(fruit), replace = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1 watermelon       1
## 2     banana       8
## 3      apple       2
## 4 canteloupe       6
## 5    kumquat       4
## 6     orange       3
## 7       kiwi       7
## 8     grapes       5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;slice_sample(fruit, n = nrow(fruit), replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         name obs_num
## 1     banana       8
## 2      apple       2
## 3 watermelon       1
## 4     grapes       5
## 5       kiwi       7
## 6       kiwi       7
## 7     grapes       5
## 8     orange       3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;more-practical-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More practical example&lt;/h2&gt;
&lt;p&gt;Let’s load some data to do a more practical example. The following data come from a &lt;a href=&#34;https://nces.ed.gov/timss/&#34;&gt;TIMSS&lt;/a&gt; on science achievement. The data provided is a subset of the available data and is not intended to be representative. Below is a short description of the data. Please don’t hesitate to send any data related questions, happy to provide additional help on interpreting the data appropriately.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IDSTUD&lt;/strong&gt;: A unique student ID&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITBIRTHM&lt;/strong&gt;: The birth month of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITBIRTHY&lt;/strong&gt;: The birth year of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ITSEX&lt;/strong&gt;: The birth sex of the student&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASDAGE&lt;/strong&gt;: The age of the student (at time of test)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSSCI01&lt;/strong&gt;: Overall science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSEAR01&lt;/strong&gt;: Earth science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSLIF01&lt;/strong&gt;: Life science scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSPHY01&lt;/strong&gt;: Physics scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSKNO01&lt;/strong&gt;: Science knowing scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSAPP01&lt;/strong&gt;: Science applying scale score&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ASSREA01&lt;/strong&gt;: Science reasoning scale score&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

timss &amp;lt;- readr::read_csv(&amp;#39;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/timss_grade4_science.csv&amp;#39;) |&amp;gt; 
   filter(ASDAGE &amp;lt; 15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 10029 Columns: 12
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## dbl (12): IDSTUD, ITBIRTHM, ITBIRTHY, ITSEX, ASDAGE, ASSSCI01, ASSEAR01, ASS...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(timss)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 12
##   IDSTUD ITBIRTHM ITBIRTHY ITSEX ASDAGE ASSSCI01 ASSEAR01 ASSLIF01 ASSPHY01
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1  10101        6     2005     2   9.92     505.     445.     408.     458.
## 2  10102       10     2005     2   9.58     401.     291.     323.     297.
## 3  10103        1     2005     1  10.3      542.     440.     529.     529.
## 4  10104        6     2005     2   9.92     544.     534.     552.     557.
## 5  10105        6     2005     2   9.92     588.     555.     500.     559.
## 6  10106       12     2005     1   9.42     583.     556.     585.     562.
## # ℹ 3 more variables: ASSKNO01 &amp;lt;dbl&amp;gt;, ASSAPP01 &amp;lt;dbl&amp;gt;, ASSREA01 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;two-guiding-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two Guiding Questions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Is the age of a student related to their overall science scale score?&lt;/li&gt;
&lt;li&gt;Is the life science scale score related to the overall science scale score?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate exploration&lt;/h2&gt;
&lt;p&gt;It is important to explore relationships bivariately before going to the model phase. To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”. You may also want to fill in an appropriate axis labels in place of “@@” below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Summarize the bivariate association&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)

theme_set(theme_bw(base_size = 18))

gf_point(%% ~ ^^, data = timss, size = 4) |&amp;gt; 
  gf_smooth(method = &amp;#39;lm&amp;#39;, size = 1.5) |&amp;gt;
  gf_smooth(method = &amp;#39;loess&amp;#39;, size = 1.5, color = &amp;#39;green&amp;#39;) |&amp;gt;
  gf_labs(x = &amp;quot;@@&amp;quot;,
          y = &amp;quot;@@&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimate-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimate Parameters&lt;/h2&gt;
&lt;p&gt;To do this, fill in the outcome of interest in place of “%%” below and fill in the appropriate predictor in place of “^^”.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interpret the following parameter estimates in the context of the current problem. (ie., what do these parameter estimates mean?)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;timss_model &amp;lt;- lm(%% ~ ^^, data = timss)

summary(timss_model)
confint(timss_model)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;resample-these-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resample these data&lt;/h2&gt;
&lt;p&gt;For the astronauts data, to resample, a similar idea can be made. Essentially, we are treating these data as a random sample of some population of space missions. We again, would resample, with replacement, which means that multiple missions would likely show up in the resampling procedure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;slice_sample(timss, n = nrow(timss), replace = TRUE) |&amp;gt; 
  count(IDSTUD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resamp_lm &amp;lt;- slice_sample(timss, n = nrow(timss), replace = TRUE) |&amp;gt; 
  lm(%% ~ ^^, data = .)

summary(resamp_lm)
confint(resamp_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;questions-to-consider&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Questions to consider&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Do the parameter estimates differ from before? Why or why not?&lt;/li&gt;
&lt;li&gt;Would you come to substantially different conclusions from the original analysis? Why or why not?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s now repeat this a bunch of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2022)

resample_timss &amp;lt;- function(data, model_equation) {
  timss_resample &amp;lt;- slice_sample(data, n = nrow(data), replace = TRUE)

  
  lm(model_equation, data = timss_resample) |&amp;gt;
    coef()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run this function a bunch of times manually, what happens to the estimates? Why is this happening?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resample_timss(data = timss, model_equation = %% ~ ^^)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;make-sure-future.apply-is-installed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Make sure future.apply is installed&lt;/h2&gt;
&lt;p&gt;The following code chunk makes sure the future.apply function is installed for parallel processing in R. If you get an error, you can uncomment (delete the &lt;code&gt;#&lt;/code&gt; symbol) in the first line of code to (hopefully) install the package yourself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;future.apply&amp;quot;)
library(future.apply)

plan(multisession)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;replicate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replicate&lt;/h2&gt;
&lt;p&gt;The following code replicates the analysis many times. Pick an initial value for N and fill in the model equation to match your code above. I will ask you to change this N value later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;timss_coef &amp;lt;- future.apply::future_replicate(N, resample_timss(data = timss, 
                        model_equation = %% ~ ^^), simplify = FALSE) |&amp;gt;
                        dplyr::bind_rows() |&amp;gt; 
                        tidyr::pivot_longer(cols = everything())

head(timss_coef)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize results&lt;/h2&gt;
&lt;p&gt;The following code visualizes the results of the analysis above. Explore the following questions.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What does this distribution show/represent?&lt;/li&gt;
&lt;li&gt;What are key features of this distribution?&lt;/li&gt;
&lt;li&gt;How do these values compare to the original linear regression results?
&lt;ul&gt;
&lt;li&gt;Are there comparable statistics here compared to those?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ value, data = timss_coef) |&amp;gt;
  gf_facet_wrap(~ name, scales = &amp;#39;free&amp;#39;) |&amp;gt;
  gf_labs(x = &amp;quot;Regression Estimates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;change-the-n-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Change the N value&lt;/h2&gt;
&lt;p&gt;Now, change the N value for the replicate step (you can either add a new cell to copy/paste the code or just change it in the code above).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens to the resulting figure when the N increases?&lt;/li&gt;
&lt;li&gt;What value for N seems to be reasonable? Why?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Regression</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;multiple-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple Regression&lt;/h1&gt;
&lt;p&gt;So far, the course has considered a single continuous predictor attribute. That is, in the following equation, we have only considered a single &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; attribute.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_{0} + \beta_{1} X + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, there are many situations where we would want more than one predictor attribute in the data. We can simply add another predictor attribute and this would look like the following regression equation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{2}\)&lt;/span&gt; are two different predictors attributes. Let’s dive right into some data to explore this a bit more.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;For this portion, using data from the college scorecard representing information about higher education institutions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

college &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7058 Columns: 16
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): instnm, city, stabbr, preddeg, region, locale
## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(college)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 16
##   instnm     city  stabbr preddeg region locale adm_rate actcmmid  ugds costt4_a
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 Alabama A… Norm… AL     Bachel… South… City:…    0.903       18  4824    22886
## 2 Universit… Birm… AL     Bachel… South… City:…    0.918       25 12866    24129
## 3 Amridge U… Mont… AL     Bachel… South… City:…   NA           NA   322    15080
## 4 Universit… Hunt… AL     Bachel… South… City:…    0.812       28  6917    22108
## 5 Alabama S… Mont… AL     Bachel… South… City:…    0.979       18  4189    19413
## 6 The Unive… Tusc… AL     Bachel… South… City:…    0.533       28 32387    28836
## # ℹ 6 more variables: costt4_p &amp;lt;dbl&amp;gt;, tuitionfee_in &amp;lt;dbl&amp;gt;,
## #   tuitionfee_out &amp;lt;dbl&amp;gt;, debt_mdn &amp;lt;dbl&amp;gt;, grad_debt_mdn &amp;lt;dbl&amp;gt;, female &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question&lt;/h2&gt;
&lt;p&gt;Suppose we were interested in exploring admission rates and which attributes helped to explain variation in admission rates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ adm_rate, data = college) %&amp;gt;%
  gf_labs(x = &amp;quot;Admission Rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5039 rows containing non-finite values (`stat_density()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(adm_rate ~ actcmmid, data = college) %&amp;gt;%
  gf_smooth(method = &amp;#39;loess&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Median ACT Score&amp;quot;,
          y = &amp;quot;Admission Rate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing non-finite values (`stat_smooth()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;GGally&amp;#39;:
##   method from   
##   +.gg   ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(college[c(&amp;#39;adm_rate&amp;#39;, &amp;#39;actcmmid&amp;#39;, &amp;#39;costt4_a&amp;#39;)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5039 rows containing non-finite values (`stat_density()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5769 rows containing missing values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5236 rows containing missing values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5769 rows containing non-finite values (`stat_density()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :
## Removed 5777 rows containing missing values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5236 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 5777 rows containing missing values (`geom_point()`).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 3486 rows containing non-finite values (`stat_density()`).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/08-multiple-regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s now fit a multiple regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adm_mult_reg &amp;lt;- lm(adm_rate ~ actcmmid + costt4_a, data = college)

summary(adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ actcmmid + costt4_a, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.135e+00  3.326e-02  34.130  &amp;lt; 2e-16 ***
## actcmmid    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## costt4_a    -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
##   (5777 observations deleted due to missingness)
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model is now the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Admission\ Rate = 1.1 + -0.017 ACT + -0.000002 cost + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Why are these parameter estimates so small?&lt;/li&gt;
&lt;li&gt;How well is the overall model doing?&lt;/li&gt;
&lt;li&gt;Are both terms important in understanding variation in admission rates? How can you tell?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance decomposition&lt;/h2&gt;
&lt;p&gt;Ultimately, multiple regression is a decomposition of variance. Recall,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum (Y - \bar{Y})^2 =  \sum (\hat{Y} - \bar{Y})^2 + \sum (Y - \hat{Y})^2  \\[10pt]
SS_{Total} = SS_{Regression} + SS_{Error}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Multiple regression still does this, but now, there are two attributes going into helping explain variation in the regression portion of the variance decomposition. How is this variance decomposed by default? For linear regression, the variance decomposition is commonly done using type I sum of square decomposition. What does this mean? Essentially, this means that additional terms are added to determine if they help explain variation over and above the other terms in the model. This is a conditional variance added. For example, given the model above, the variance decomposition could be broken down into the following.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SS_{Total} = SS_{Regression} + SS_{Error}   \\[10pt]
SS_{Total} = SS_{X_{1}} + SS_{X_{2} | X_{1}} + SS_{Error}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;act_lm &amp;lt;- lm(adm_rate ~ actcmmid, data = college)

summary(act_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ actcmmid, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.71615 -0.12521  0.02024  0.14490  0.37314 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.180914   0.032988   35.80   &amp;lt;2e-16 ***
## actcmmid    -0.022162   0.001391  -15.93   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1844 on 1287 degrees of freedom
##   (5769 observations deleted due to missingness)
## Multiple R-squared:  0.1647,	Adjusted R-squared:  0.1641 
## F-statistic: 253.8 on 1 and 1287 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(act_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## actcmmid     1  8.635  8.6351  253.84 &amp;lt; 2.2e-16 ***
## Residuals 1287 43.781  0.0340                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## actcmmid     1  8.386  8.3861 255.092 &amp;lt; 2.2e-16 ***
## costt4_a     1  1.065  1.0655  32.411 1.546e-08 ***
## Residuals 1278 42.014  0.0329                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_lm &amp;lt;- lm(adm_rate ~ costt4_a, data = college)

summary(cost_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ costt4_a, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.72108 -0.12656  0.02474  0.14459  0.38059 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  8.233e-01  1.151e-02   71.52   &amp;lt;2e-16 ***
## costt4_a    -4.147e-06  3.022e-07  -13.73   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1965 on 1820 degrees of freedom
##   (5236 observations deleted due to missingness)
## Multiple R-squared:  0.0938,	Adjusted R-squared:  0.09331 
## F-statistic: 188.4 on 1 and 1820 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(cost_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## costt4_a     1  7.278  7.2781   188.4 &amp;lt; 2.2e-16 ***
## Residuals 1820 70.309  0.0386                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Comparisons</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/09-model-comparison/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/09-model-comparison/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;model-comparison&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Comparison&lt;/h1&gt;
&lt;p&gt;This section of multiple regression is going to explore model comparison, to guide the selection of the best fitting model from a set of competing models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

college &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/statthink/main/data-raw/College-scorecard-4143.csv&amp;quot;) %&amp;gt;%
  mutate(act_mean = actcmmid - mean(actcmmid, na.rm = TRUE),
         cost_mean = costt4_a - mean(costt4_a, na.rm = TRUE)) %&amp;gt;%
  drop_na(act_mean, cost_mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 7058 Columns: 16
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (6): instnm, city, stabbr, preddeg, region, locale
## dbl (10): adm_rate, actcmmid, ugds, costt4_a, costt4_p, tuitionfee_in, tuiti...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(college)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 18
##   instnm     city  stabbr preddeg region locale adm_rate actcmmid  ugds costt4_a
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 Alabama A… Norm… AL     Bachel… South… City:…    0.903       18  4824    22886
## 2 Universit… Birm… AL     Bachel… South… City:…    0.918       25 12866    24129
## 3 Universit… Hunt… AL     Bachel… South… City:…    0.812       28  6917    22108
## 4 Alabama S… Mont… AL     Bachel… South… City:…    0.979       18  4189    19413
## 5 The Unive… Tusc… AL     Bachel… South… City:…    0.533       28 32387    28836
## 6 Auburn Un… Mont… AL     Bachel… South… City:…    0.825       22  4211    19892
## # ℹ 8 more variables: costt4_p &amp;lt;dbl&amp;gt;, tuitionfee_in &amp;lt;dbl&amp;gt;,
## #   tuitionfee_out &amp;lt;dbl&amp;gt;, debt_mdn &amp;lt;dbl&amp;gt;, grad_debt_mdn &amp;lt;dbl&amp;gt;, female &amp;lt;dbl&amp;gt;,
## #   act_mean &amp;lt;dbl&amp;gt;, cost_mean &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(college)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1281   18&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adm_mult_reg &amp;lt;- lm(adm_rate ~ act_mean + cost_mean, data = college)

summary(adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean + cost_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  6.834e-01  6.346e-03 107.681  &amp;lt; 2e-16 ***
## act_mean    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## cost_mean   -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;omnibus-hypothesis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Omnibus Hypothesis&lt;/h2&gt;
&lt;p&gt;In linear regression, I omitted a portion of the output from above that typically comes with most statistical output. That is, there is a test statistic that aims to test if the model is explaining variation over and above a simple mean. More specifically, this omnibus hypothesis tests the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: All\ \beta = 0 \\[10pt]
H_{A}: Any\ \beta \neq 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This hypothesis can be formally tested with an F-statistic which is distributed as an F distribution with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; predictor attributes and &lt;span class=&#34;math inline&#34;&gt;\(n - p - 1\)&lt;/span&gt; degrees of freedom.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_data &amp;lt;- data.frame(value = seq(0, 5, by = .01)) %&amp;gt;%
   mutate(dens = df(value, 2, 1278))

gf_line(dens ~ value, data = f_data, size = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/09-model-comparison_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f_data &amp;lt;- data.frame(value = seq(0, 5, by = .01)) %&amp;gt;%
   mutate(dens = df(value, 5, 50))

gf_line(dens ~ value, data = f_data, size = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/09-model-comparison_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusted-r-squared&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusted R-squared&lt;/h2&gt;
&lt;p&gt;The adjusted R-squared is typically used when comparing models. This statistic is commonly used as R-square represents the ratio between explained and total variance, therefore, this will always increase, even if the new attribute entered is not helpful. The adjusted R-squared tries to adjust for model complexity. There are many ways to do this, but the most common will be defined here.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{R}^2 = 1 - (1 - R^2) \frac{n - 1}{n - p - 1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{R}^2 = 1 - \frac{SS_{res} / df_{e}}{SS_{tot} / df_{t}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of predictors (excluding the intercept), &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size, &lt;span class=&#34;math inline&#34;&gt;\(SS_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(SS_{tot}\)&lt;/span&gt; are sum of square residual and total respectively, and &lt;span class=&#34;math inline&#34;&gt;\(df_{e}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(df_{t}\)&lt;/span&gt; are degrees of freedom for the error (&lt;span class=&#34;math inline&#34;&gt;\(n - p - 1\)&lt;/span&gt;) and total (&lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;) respectively.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean + cost_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.65484 -0.12230  0.02291  0.13863  0.37054 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  6.834e-01  6.346e-03 107.681  &amp;lt; 2e-16 ***
## act_mean    -1.669e-02  1.650e-03 -10.114  &amp;lt; 2e-16 ***
## cost_mean   -2.304e-06  4.047e-07  -5.693 1.55e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1813 on 1278 degrees of freedom
## Multiple R-squared:  0.1836,	Adjusted R-squared:  0.1824 
## F-statistic: 143.8 on 2 and 1278 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - (1 - .1836) * (1281 - 1) / (1281 - 2 - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1823224&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: adm_rate
##             Df Sum Sq Mean Sq F value    Pr(&amp;gt;F)    
## act_mean     1  8.386  8.3861 255.092 &amp;lt; 2.2e-16 ***
## cost_mean    1  1.065  1.0655  32.411 1.546e-08 ***
## Residuals 1278 42.014  0.0329                      
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - (42.01 / 1278) / ((8.39 + 1.066 + 42.014) / 1280)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1825191&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Comparison&lt;/h2&gt;
&lt;p&gt;There are a variety of statistics used to provide statistical evidence for competing models. If the models are nested, then the variance decomposition can be used to determine if the added predictors helped to explain significant variation over and above the simpler model.&lt;/p&gt;
&lt;p&gt;In this situation, another F statistic can be derived where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F = \frac{SS_{res}^{R} - SS_{res}^{F} / \Delta p}{SS_{res}^{F} / df_{F}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;act_lm &amp;lt;- lm(adm_rate ~ act_mean, data = college)

summary(act_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = adm_rate ~ act_mean, data = college)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.71229 -0.12483  0.01999  0.14317  0.37289 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.661593   0.005128  129.02   &amp;lt;2e-16 ***
## act_mean    -0.021905   0.001388  -15.78   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1835 on 1279 degrees of freedom
## Multiple R-squared:  0.1629,	Adjusted R-squared:  0.1623 
## F-statistic:   249 on 1 and 1279 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(act_lm, adm_mult_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: adm_rate ~ act_mean
## Model 2: adm_rate ~ act_mean + cost_mean
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1   1279 43.079                                  
## 2   1278 42.014  1    1.0655 32.411 1.546e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;((43.08 - 42.01) / 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.07 / (42.01 / 1278)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 32.55082&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;non-nested-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-nested models&lt;/h3&gt;
&lt;p&gt;For non-nested models, the F-statistic defined above will not work. Instead other statistics are needed to evaluate which model is the best. The one that I prefer for this is the AIC (Akaike information criteria) or the related small sample form, AICc. The equations for these aren’t all that useful, utilizing software is the best way to compute these statistics. In general, smaller AIC values indicate a better fitting model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(AICcmodavg)

cost_lm &amp;lt;- lm(adm_rate ~ cost_mean, data = college)

aictab(list(cost_lm, act_lm), 
       modnames = c(&amp;#39;cost&amp;#39;, &amp;#39;act&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model selection based on AICc:
## 
##      K    AICc Delta_AICc AICcWt Cum.Wt     LL
## act  3 -704.26       0.00      1      1 355.14
## cost 3 -637.70      66.56      0      1 321.86&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Regression with Categorical Predictors</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;regression-with-categorical-predictors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Regression with Categorical Predictors&lt;/h1&gt;
&lt;p&gt;This set of notes will explore using linear regression for a single predictor attribute that is categorical instead of continuous. To explore this first, let’s explore some data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
library(ggformula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggstance
## 
## Attaching package: &amp;#39;ggstance&amp;#39;
## 
## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     geom_errorbarh, GeomErrorbarh
## 
## Loading required package: scales
## 
## Attaching package: &amp;#39;scales&amp;#39;
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     discard
## 
## The following object is masked from &amp;#39;package:readr&amp;#39;:
## 
##     col_factor
## 
## Loading required package: ggridges
## 
## New to ggformula?  Try the tutorials: 
## 	learnr::run_tutorial(&amp;quot;introduction&amp;quot;, package = &amp;quot;ggformula&amp;quot;)
## 	learnr::run_tutorial(&amp;quot;refining&amp;quot;, package = &amp;quot;ggformula&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  dplyr::select(-playerID)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## ℹ Please use `tibble::as_tibble()` instead.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(career)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   name               lgID      H    AB average
##   &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 Jeff Abbott        AL      127   459   0.277
## 2 Kurt Abbott        AL       33   123   0.268
## 3 Kurt Abbott        NL      455  1780   0.256
## 4 Reggie Abercrombie NL       54   255   0.212
## 5 Brent Abernathy    AL      194   767   0.253
## 6 Shawn Abner        AL       81   309   0.262&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in the batting average of baseball players since 1990, that is, the average is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
average = \frac{number\ of\ hits}{number\ of\ atbats}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s first visualize this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_density(~ average, data = career) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What if we hypothesized that the batting average will differ based on the league that players played in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_violin(lgID ~ average, data = career, fill = &amp;#39;gray80&amp;#39;, draw_quantiles = c(&amp;#39;0.1&amp;#39;, &amp;#39;0.5&amp;#39;, &amp;#39;0.9&amp;#39;)) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;,
          y = &amp;quot;League&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The distributions seem similar, but what if we wanted to go a step further and estimate a model to explore if there are really differences or not. For example, suppose we were interested in:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \mu_{NL} = \mu_{AL}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What type of model could we use? What about linear regression?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression-with-categorical-attributes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression with Categorical Attributes&lt;/h2&gt;
&lt;p&gt;Since these notes are happening, you can assume it is possible. But how can a categorical attribute with categories rather than numbers be included in the linear regression model?&lt;/p&gt;
&lt;p&gt;The answer is that they can’t. We need a new representation of the categorical attribute, enter dummy or indicator coding.&lt;/p&gt;
&lt;div id=&#34;dummyindicator-coding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dummy/Indicator Coding&lt;/h3&gt;
&lt;p&gt;Suppose we use the following logic:&lt;/p&gt;
&lt;p&gt;If NL, then give a value of 1, else give a value of 0.&lt;/p&gt;
&lt;p&gt;Does this give the same information as before?&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;League ID&lt;/th&gt;
&lt;th&gt;Dummy League ID&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;AL&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;NL&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What would this look like for the actual data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &amp;#39;NL&amp;#39;, 1, 0))

head(career, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 6
##    name               lgID      H    AB average league_dummy
##    &amp;lt;chr&amp;gt;              &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Jeff Abbott        AL      127   459   0.277            0
##  2 Kurt Abbott        AL       33   123   0.268            0
##  3 Kurt Abbott        NL      455  1780   0.256            1
##  4 Reggie Abercrombie NL       54   255   0.212            1
##  5 Brent Abernathy    AL      194   767   0.253            0
##  6 Shawn Abner        AL       81   309   0.262            0
##  7 Shawn Abner        NL       19   115   0.165            1
##  8 CJ Abrams          NL       70   284   0.246            1
##  9 Bobby Abreu        AL      858  3061   0.280            0
## 10 Bobby Abreu        NL     1602  5373   0.298            1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that there is a numeric attribute, these can be added into the linear regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;average_lm &amp;lt;- lm(average ~ league_dummy, data = career)

broom::tidy(average_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term         estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.252     0.000758   333.      0    
## 2 league_dummy 0.000558  0.00107      0.522   0.602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How are these terms interpreted now?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(average ~ league_dummy, data = career, mean, sd, length)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response league_dummy      mean         sd length
## 1  average            0 0.2518959 0.02895094   1461
## 2  average            1 0.2524535 0.02896021   1477&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;average_lm2 &amp;lt;- lm(average ~ lgID, data = career)

broom::tidy(average_lm2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept) 0.252     0.000758   333.      0    
## 2 lgIDNL      0.000558  0.00107      0.522   0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(average ~ lgID, data = career, var.equal = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 	Two Sample t-test
## 
## data:  average by lgID
## t = -0.52189, df = 2936, p-value = 0.6018
## alternative hypothesis: true difference in means between group AL and group NL is not equal to 0
## 95 percent confidence interval:
##  -0.002652537  0.001537330
## sample estimates:
## mean in group AL mean in group NL 
##        0.2518959        0.2524535&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;values-other-than-01&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Values other than 0/1&lt;/h2&gt;
&lt;p&gt;First, I want to build off of the first part of the notes on regression with categorical predictors. Before generalizing to more than two groups, let’s first explore what happens when values other than 0/1 are used for the categorical attribute. The following three dummy/indicator attributes will be used:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;1 = NL, 0 = AL&lt;/li&gt;
&lt;li&gt;1 = NL, 2 = AL&lt;/li&gt;
&lt;li&gt;100 = NL, 0 = AL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Make some predictions about what you think will happen in the three separate regressions?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Lahman)
library(ggformula)

theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  dplyr::select(-playerID)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## ℹ Please use `tibble::as_tibble()` instead.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &amp;#39;NL&amp;#39;, 1, 0),
         league_dummy_12 = ifelse(lgID == &amp;#39;NL&amp;#39;, 1, 2),
         league_dummy_100 = ifelse(lgID == &amp;#39;NL&amp;#39;, 100, 0))

head(career, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 8
##    name  lgID      H    AB average league_dummy league_dummy_12 league_dummy_100
##    &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 Jeff… AL      127   459   0.277            0               2                0
##  2 Kurt… AL       33   123   0.268            0               2                0
##  3 Kurt… NL      455  1780   0.256            1               1              100
##  4 Regg… NL       54   255   0.212            1               1              100
##  5 Bren… AL      194   767   0.253            0               2                0
##  6 Shaw… AL       81   309   0.262            0               2                0
##  7 Shaw… NL       19   115   0.165            1               1              100
##  8 CJ A… NL       70   284   0.246            1               1              100
##  9 Bobb… AL      858  3061   0.280            0               2                0
## 10 Bobb… NL     1602  5373   0.298            1               1              100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;average_lm &amp;lt;- lm(average ~ league_dummy, data = career)

broom::tidy(average_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term         estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.252     0.000758   333.      0    
## 2 league_dummy 0.000558  0.00107      0.522   0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;average_lm_12 &amp;lt;- lm(average ~ league_dummy_12, data = career)

broom::tidy(average_lm_12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term             estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)      0.253      0.00169   150.      0    
## 2 league_dummy_12 -0.000558   0.00107    -0.522   0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;average_lm_100 &amp;lt;- lm(average ~ league_dummy_100, data = career)

broom::tidy(average_lm_100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term               estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)      0.252      0.000758    333.      0    
## 2 league_dummy_100 0.00000558 0.0000107     0.522   0.602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before moving to more than 2 groups, any thoughts on how we could run a one-sample t-test using a linear regression? For example, suppose this null hypothesis wanted to be explored.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{0}: \mu_{BA} = .2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_{A}: \mu_{BA} \neq .2
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generalize-to-more-than-2-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalize to more than 2 groups&lt;/h2&gt;
&lt;p&gt;The ability to use regression with categorical attributes of more than 2 groups is possible and an extension of the 2 groups model shown above. First, let’s think about how we could represent three categories as numeric attributes. Suppose we had the following 4 categories of baseball players.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Position&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Outfield&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Infield&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Catcher&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Designated Hitter&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GeomMLBStadiums)

ggplot() + 
  geom_mlb_stadium(stadium_segments = &amp;quot;all&amp;quot;) + 
  facet_wrap(~team) + 
  coord_fixed() + 
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Lahman)
library(ggformula)

theme_set(theme_bw(base_size = 18))

career &amp;lt;- Batting %&amp;gt;%
  filter(AB &amp;gt; 100) %&amp;gt;%
  anti_join(Pitching, by = &amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(H = sum(H), AB = sum(AB)) %&amp;gt;%
  mutate(average = H / AB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- Appearances %&amp;gt;%
  filter(yearID &amp;gt; 1990) %&amp;gt;%
  select(-GS, -G_ph, -G_pr, -G_batting, -G_defense, -G_p, -G_lf, -G_cf, -G_rf) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(g_inf = sum(c_across(G_1b:G_ss))) %&amp;gt;%
  select(-G_1b, -G_2b, -G_3b, -G_ss) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  summarise(catcher = sum(G_c),
            outfield = sum(G_of),
            dh = sum(G_dh),
            infield = sum(g_inf),
            total_games = sum(G_all)) %&amp;gt;%
  pivot_longer(catcher:infield,
               names_to = &amp;quot;position&amp;quot;) %&amp;gt;%
  filter(value &amp;gt; 0) %&amp;gt;%
  group_by(playerID, lgID) %&amp;gt;%
  slice_max(value) %&amp;gt;%
  select(playerID, lgID, position) %&amp;gt;%
  inner_join(career)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the
## `.groups` argument.
## Joining with `by = join_by(playerID, lgID)`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- People %&amp;gt;%
  tbl_df() %&amp;gt;%
  dplyr::select(playerID, nameFirst, nameLast) %&amp;gt;%
  unite(name, nameFirst, nameLast, sep = &amp;quot; &amp;quot;) %&amp;gt;%
  inner_join(career, by = &amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## ℹ Please use `tibble::as_tibble()` instead.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- career %&amp;gt;%
  mutate(league_dummy = ifelse(lgID == &amp;#39;NL&amp;#39;, 1, 0))

count(career, position)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 2
##   position     n
##   &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1 catcher    415
## 2 dh          83
## 3 infield   1279
## 4 outfield  1165&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_violin(position ~ average, data = career, fill = &amp;#39;gray85&amp;#39;, draw_quantiles = c(0.1, 0.5, 0.9)) %&amp;gt;%
  gf_labs(x = &amp;quot;Batting Average&amp;quot;,
          y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/10-regression-categorical_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;career &amp;lt;- career %&amp;gt;%
  mutate(outfield = ifelse(position == &amp;#39;outfield&amp;#39;, 1, 0),
         infield = ifelse(position == &amp;#39;infield&amp;#39;, 1, 0),
         catcher = ifelse(position == &amp;#39;catcher&amp;#39;, 1, 0))

head(career)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 11
##   playerID  name        lgID  position     H    AB average league_dummy outfield
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 abbotje01 Jeff Abbott AL    outfield   127   459   0.277            0        1
## 2 abbotku01 Kurt Abbott AL    infield     33   123   0.268            0        0
## 3 abbotku01 Kurt Abbott NL    infield    455  1780   0.256            1        0
## 4 abercre01 Reggie Abe… NL    outfield    54   255   0.212            1        1
## 5 abernbr01 Brent Aber… AL    infield    194   767   0.253            0        0
## 6 abnersh01 Shawn Abner AL    outfield    81   309   0.262            0        1
## # ℹ 2 more variables: infield &amp;lt;dbl&amp;gt;, catcher &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_lm &amp;lt;- lm(average ~ 1 + outfield + infield + catcher, data = career)

broom::tidy(position_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 5
##   term         estimate std.error statistic   p.value
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)  0.255      0.00314    81.3   0        
## 2 outfield    -0.000610   0.00325    -0.188 0.851    
## 3 infield     -0.00218    0.00324    -0.672 0.501    
## 4 catcher     -0.0144     0.00344    -4.19  0.0000288&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_stats(average ~ position, data = career, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   response position      mean
## 1  average  catcher 0.2409461
## 2  average       dh 0.2553577
## 3  average  infield 0.2531783
## 4  average outfield 0.2547475&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Quiz 1</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz1/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz1/</guid>
      <description>&lt;p&gt;Quiz 1 can be taken on ICON, due September 18th, 2023. The quiz covers content from &lt;a href=&#34;https://psqf6243.brandonlebeau.org/content/01-week1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Week 1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://uiowa.instructure.com/courses/214414/quizzes/397149&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quiz 1 Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quiz 2</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz2/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz2/</guid>
      <description>&lt;p&gt;Quiz 2 can be taken on ICON, due September 25th, 2023. The quiz covers content from &lt;a href=&#34;https://psqf6243.brandonlebeau.org/content/02-week2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Week 2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://uiowa.instructure.com/courses/214414/quizzes/397152&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quiz 2 Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quiz 3</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz3/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz3/</guid>
      <description>&lt;p&gt;Quiz 3 can be taken on ICON, due October 2nd, 2023. The quiz covers content from &lt;a href=&#34;https://psqf6243.brandonlebeau.org/content/03-week3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Week 3&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://uiowa.instructure.com/courses/214414/quizzes/397153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quiz 3 Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quiz 4</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz4/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz4/</guid>
      <description>&lt;p&gt;Quiz 4 can be taken on ICON, due October 16th, 2023. The quiz covers content from &lt;a href=&#34;https://psqf6243.brandonlebeau.org/content/04-week4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Week 4&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://uiowa.instructure.com/courses/214414/quizzes/397155&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quiz 4 Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quiz 5</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz5/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/quizzes/quiz5/</guid>
      <description>&lt;p&gt;Quiz 4 can be taken on ICON, due October 16th, 2023. The quiz covers content from &lt;a href=&#34;https://psqf6243.brandonlebeau.org/content/06-week6/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Week 6&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://uiowa.instructure.com/courses/214414/quizzes/397154&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quiz 5 Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression - In Class</title>
      <link>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;introduction-to-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;This week will dive into linear regression, the foundation of this course. The exploration into linear regression will first start with the case when we have 2 &lt;strong&gt;continuous&lt;/strong&gt; attributes. One of those attributes will be the &lt;em&gt;outcome&lt;/em&gt; or &lt;em&gt;attribute of interest&lt;/em&gt; whereas the other will used as a &lt;em&gt;predictor&lt;/em&gt;. The outcome or attribute of interest is sometimes referred to as the dependent variable and the predictor is sometimes referred to as the independent variable. One way to think about this is that the dependent variable depends or is a function of the other attributes of interest. In linear regression terms, it could also be said that the independent variable &lt;strong&gt;explains variation&lt;/strong&gt; in the dependent variable (more on this later).&lt;/p&gt;
&lt;p&gt;Of note, variable is a typical word used in statistics, I’ve come to like the word &lt;strong&gt;attribute&lt;/strong&gt; instead of variable. I will tend to use attribute, as in, a data attribute, but these are roughly interchangeable in my terminology.&lt;/p&gt;
&lt;p&gt;We may write this general model as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_{0} + \beta_{1} X + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the outcome attribute. It is also known as the dependent variable. The &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; term is the predictor/covariate attribute. It is also known as the independent variable. The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is a random error term, more on this later. Finally, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; are unknown population coefficients that we are interested in estimating. More on this later too.&lt;/p&gt;
&lt;div id=&#34;specific-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specific example&lt;/h2&gt;
&lt;p&gt;The data used for this section of the course is from the 2019 WNBA season. These data are part of the &lt;a href=&#34;https://www.bayesrulesbook.com/&#34;&gt;&lt;em&gt;bayesrules&lt;/em&gt; package/book&lt;/a&gt;. The data contain 146 rows, one for each WNBA player sampled, and 32 attributes for that player. The R packages are loaded and the first few rows of the data are shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.2     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1
## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0
## ✔ purrr     1.0.1     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mosaic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;mosaic&amp;#39;:
##   method                           from   
##   fortify.SpatialPolygonsDataFrame ggplot2
## 
## The &amp;#39;mosaic&amp;#39; package masks several functions from core packages in order to add 
## additional features.  The original behavior of these functions should not be affected by this.
## 
## Attaching package: &amp;#39;mosaic&amp;#39;
## 
## The following object is masked from &amp;#39;package:Matrix&amp;#39;:
## 
##     mean
## 
## The following objects are masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     count, do, tally
## 
## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     cross
## 
## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     stat
## 
## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,
##     quantile, sd, t.test, var
## 
## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     max, mean, min, prod, range, sample, sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggformula)

basketball &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/basketball.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 146 Columns: 32
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &amp;quot;,&amp;quot;
## chr  (2): player_name, team
## dbl (29): height, weight, year, age, games_played, games_started, avg_minute...
## lgl  (1): starter
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theme_set(theme_bw(base_size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(basketball)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 32
##   player_name     height weight  year team    age games_played games_started
##   &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 Natalie Achonwa     75    190  2019 IND      26           30            18
## 2 Kayla Alexander     76    195  2019 CHI      28            3             0
## 3 Rebecca Allen       74    162  2019 NYL      26           24             2
## 4 Jillian Alleyne     74    193  2019 MIN      24            5             0
## 5 Kristine Anigwe     76    200  2019 TOT      22           27             0
## 6 Kristine Anigwe     76    200  2019 CON      22           17             0
## # ℹ 24 more variables: avg_minutes_played &amp;lt;dbl&amp;gt;, avg_field_goals &amp;lt;dbl&amp;gt;,
## #   avg_field_goal_attempts &amp;lt;dbl&amp;gt;, field_goal_pct &amp;lt;dbl&amp;gt;,
## #   avg_three_pointers &amp;lt;dbl&amp;gt;, avg_three_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   three_pointer_pct &amp;lt;dbl&amp;gt;, avg_two_pointers &amp;lt;dbl&amp;gt;,
## #   avg_two_pointer_attempts &amp;lt;dbl&amp;gt;, two_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_free_throws &amp;lt;dbl&amp;gt;, avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Suppose we are interested in exploring if players tend to score more points by playing more minutes in the season. That is, those that play more may have more opportunities to score more points. More generally, the relationship between average points in each game by the total minutes played across the season.&lt;/p&gt;
&lt;p&gt;One first step in an analysis would be to explore each distribution independently first. I’m going to leave that as an exercise for you to do on your own.&lt;/p&gt;
&lt;p&gt;The next step would be to explore the bivariate figure of these two attributes. As both of these attributes are continuous ratio type attributes, a scatterplot would be one way to visualize this. A scatterplot takes each X,Y pair of data and plots those coordinates. This can be done in R with the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;) |&amp;gt;
  gf_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(avg_points ~ total_minutes, 
    data = basketball) |&amp;gt; 
round(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.848&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(cor(avg_points ~ total_minutes, data = basketball), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.848&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;questions-to-consider&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Questions to consider&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What can be noticed about the relationship between these two attributes?&lt;/li&gt;
&lt;li&gt;Does there appear to be a relationship between the two?&lt;/li&gt;
&lt;li&gt;Is this relationship perfect?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-smoother-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding a smoother line&lt;/h2&gt;
&lt;p&gt;Adding a smoother line to the figure can help to guide how strong the relationship may be. In general, there are two types of smoothers that we will consider in this course. One is flexible and data dependent. This means that the functional form of the relationship is flexible to allow the data to specify if there are in non-linear aspects. The second is a linear or straight-line approach.&lt;/p&gt;
&lt;p&gt;I’m going to add both to the figure below. The flexible (in this case this is a LOESS curve) curve is darker blue, the linear line is lighter blue.&lt;/p&gt;
&lt;p&gt;Does there appear to be much difference in the relationship across the two lines?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(linewidth = 2.5) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linewidth = 2.5, linetype = 2, color = &amp;#39;green&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(basketball, total_minutes &amp;lt;= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 32
##   player_name      height weight  year team    age games_played games_started
##   &amp;lt;chr&amp;gt;             &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 Angel McCoughtry     73    173  2019 ATL      32            1             1
## # ℹ 24 more variables: avg_minutes_played &amp;lt;dbl&amp;gt;, avg_field_goals &amp;lt;dbl&amp;gt;,
## #   avg_field_goal_attempts &amp;lt;dbl&amp;gt;, field_goal_pct &amp;lt;dbl&amp;gt;,
## #   avg_three_pointers &amp;lt;dbl&amp;gt;, avg_three_pointer_attempts &amp;lt;dbl&amp;gt;,
## #   three_pointer_pct &amp;lt;dbl&amp;gt;, avg_two_pointers &amp;lt;dbl&amp;gt;,
## #   avg_two_pointer_attempts &amp;lt;dbl&amp;gt;, two_pointer_pct &amp;lt;dbl&amp;gt;,
## #   avg_free_throws &amp;lt;dbl&amp;gt;, avg_free_throw_attempts &amp;lt;dbl&amp;gt;, free_throw_pct &amp;lt;dbl&amp;gt;,
## #   avg_offensive_rb &amp;lt;dbl&amp;gt;, avg_defensive_rb &amp;lt;dbl&amp;gt;, avg_rb &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-linear-regression-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating linear regression coefficients&lt;/h2&gt;
&lt;p&gt;The linear regression coefficients can be estimated within any statistical software (or by hand, even if tedious). Within R, the primary function is &lt;code&gt;lm()&lt;/code&gt; to estimate a linear regression. The primary argument is a formula similar to the regression formula shown above at the top of the notes.&lt;/p&gt;
&lt;p&gt;This equation could be written more directly for our specific problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Avg\_points = \beta_{0} + \beta_{1} Minutes\_Played + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One way to read this equation is that the number of minutes played for each player helps to understand variation or differences in the average points scored for that player. Or, average points is modeled or explained by minutes played.&lt;/p&gt;
&lt;p&gt;For the R formula, instead of an $ = $, you could insert a ~.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wnba_reg &amp;lt;- lm(avg_points ~ total_minutes, data = basketball)
coef(wnba_reg) |&amp;gt; round(4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept) total_minutes 
##        1.1356        0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;100 * .01&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretting-linear-regression-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretting linear regression terms&lt;/h2&gt;
&lt;p&gt;Now that we have estimates for the linear regression terms, how are these interpretted? The linear regression equation with these estimates plugged in would look like the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{avg\_points} = 1.1356 + .0101 min\_played
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
avg\_points = 1.1356 + .0101 min\_played + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where instead of &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;, the estimated values from this single season were inserted. Note the &lt;span class=&#34;math inline&#34;&gt;\(\hat{avg\_points}\)&lt;/span&gt;, which the caret symbol is read as a hat, that is, average points hat, is a very important small distinction. This now represents the predicted values for the linear regression. That means, that the predicted value for the average number of points is assumed to function solely based on the minutes a player played. We could put in any value for the minutes played and get an estimated average number of points out.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.1457&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 100&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.1456&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * mean(basketball$total_minutes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.342042&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * 5000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51.6356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * -50&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6306&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.1356 + .0101 * -5000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -49.3644&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also notice from the equation above with the estimated coefficients, there is no longer any error. More on this later, but I wanted to point that out now. Back to model interpretations, these can become a bit more obvious with the values computed above by inputting specific values for the total minutes played.&lt;/p&gt;
&lt;p&gt;First, for the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;), notice that for the first computation above when 0 total minutes was input into the equation, that the same value for the intercept estimate was returned. This highlights what the intercept is, the average number of points scored when the X attribute (minutes played) equals 0.&lt;/p&gt;
&lt;p&gt;The slope, (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;), term is the average change in the outcome (average points here) for a one unit change in the predictor attribute (minutes played). Therefore, the slope here is 0.0101, which means that the average points scores increases by about 0.01 points for every additional minute played. This effect is additive, meaning that the 0.01 for a one unit change, say from 100 to 101 minutes, will remain when increasing from 101 to 102 minutes.&lt;/p&gt;
&lt;p&gt;The predictions coming from the linear regression are the same as the light blue dashed line shown in the figure above and recreated here without the dark blue line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, linewidth = 3, color = &amp;#39;blue&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_histogram(~ avg_points, data = basketball)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-the-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about the error?&lt;/h2&gt;
&lt;p&gt;So far the error has been disregarded, but where did it go? The error didn’t disappear, it is actually in the figure just created above. Where can you see the error? Why was it disregarded when creating the predicted values?&lt;/p&gt;
&lt;p&gt;The short answer is that the error in a linear regression is commonly assumed to follow a Normal distribution with a mean of 0 and some variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Sometimes this is written in math notation as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\epsilon \sim N(0, \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From this notation, can you see why the error was disregarded earlier when generating predictions?&lt;/p&gt;
&lt;p&gt;In short, on average, the error is assumed to be 0 across all the sample data. The error will be smaller when the data are more closely clustered around the linear regression line and larger when the data are not clustered around the linear regression line. In the simple case with a single predictor, the error would be minimized when the correlation is closest to 1 in absolute value and largest when the correlation close to or equals 0.&lt;/p&gt;
&lt;div id=&#34;estimating-error-in-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating error in linear regression&lt;/h3&gt;
&lt;p&gt;This comes from partitioning of variance that you maybe heard from a design of experiment or analysis of variance course. More specifically, the variance in the outcome can be partioned or split into two components, those that the independent attribute helped to explain vs those that it can not explain. The part that can be explained is sometimes referred to as the &lt;em&gt;sum of squares regression&lt;/em&gt; (SSR), the portion that is unexplained is referred to as the &lt;em&gt;sum of squares error&lt;/em&gt; (SSE). This could be written in math notation as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum (Y - \bar{Y})^2 = \sum (Y - \hat{Y})^2 + \sum (\hat{Y} - \bar{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{Y} = \beta_{0} + \beta_{1} minutes
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SS_{reg} = \sum (\hat{Y} - \bar{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SS_{error} = \sum (Y - \hat{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SS_{Total} = \sum (Y - \bar{Y})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s try to visualize what this means.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(avg_points + mean(avg_points) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#FF7F7F&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(avg_points + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#65a765&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gf_point(avg_points ~ total_minutes, data = basketball, size = 4, alpha = .5) %&amp;gt;% 
  gf_hline(yintercept = ~mean(avg_points), data = basketball) %&amp;gt;%
  gf_smooth(method = &amp;#39;lm&amp;#39;, linetype = 2, color = &amp;#39;lightblue&amp;#39;) %&amp;gt;%
  gf_segment(mean(avg_points) + fitted(wnba_reg) ~ total_minutes + total_minutes, data = basketball, color = &amp;#39;#FFD580&amp;#39;) %&amp;gt;%
  gf_labs(x = &amp;quot;Total Minutes Played&amp;quot;,
          y = &amp;quot;Average Points Scored&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://psqf6243.brandonlebeau.org/lectures/02-linear-regression-inclass_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;another-related-measure-of-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another related measure of error&lt;/h2&gt;
&lt;p&gt;Another way to get a measure of how well the model is performing, would be a statistic called R-squared. This statistic is a function of the sum of squares described above.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R^{2} = 1 - \frac{SS_{error}}{SS_{total}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R^{2} = \frac{SS_{reg}}{SS_{total}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s compute the sum of square and get a value for &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma ^2 = \frac{SS_{error}}{ n - 2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma = \sqrt{\sigma^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basketball %&amp;gt;%
summarise(ss_total = sum((avg_points - mean(avg_points))^2),
          ss_error = sum((avg_points - fitted(wnba_reg))^2),
          ss_reg = sum((fitted(wnba_reg) - mean(avg_points))^2)) %&amp;gt;%
mutate(r_square = 1 - ss_error / ss_total,
       r_square2 = ss_reg / ss_total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   ss_total ss_error ss_reg r_square r_square2
##      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1    2004.     564.  1440.    0.719     0.719&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(wnba_reg)$r.square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7185315&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(wnba_reg)$sigma&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma_hat_square &amp;lt;- 563.9929 / (nrow(basketball) - 2)
sigma_hat &amp;lt;- sqrt(sigma_hat_square)

sigma_hat_square&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.916617&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sigma_hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.979045&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(basketball$avg_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.717388&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Activity 1</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/activity/activity1/</link>
      <pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/activity/activity1/</guid>
      <description>&lt;p&gt;The following activity is aimed to give you some practice with exploring data on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. &lt;em&gt;Please make sure to add everyone&amp;rsquo;s name to the submission&lt;/em&gt;, this can be a comment on ICON or on the document itself.&lt;/p&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-turn-in&#34;&gt;What to turn in&lt;/h3&gt;
&lt;p&gt;Please turn in a document that contains the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answers to the questions below&lt;/li&gt;
&lt;li&gt;include any relevant statistics/figures that support your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, upload the final document to &lt;a href=&#34;https://uiowa.instructure.com/courses/214414/assignments/2006006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICON&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;due-date&#34;&gt;Due Date&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Due around&lt;/em&gt; &lt;strong&gt;September 18th, 2023&lt;/strong&gt;. No penalty for late submissions as long as it is submitted by December 11th.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this activity comes from the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy Tuesday&lt;/a&gt; project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-08-09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see the Tidy Tuesday page for more information&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The data can be obtained in &lt;a href=&#34;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/tornados.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csv format&lt;/a&gt;. A short description for each attribute is as follows. These data are also found within the &amp;ldquo;data&amp;rdquo; folder inside the IDAS.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;om&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tornado number. Effectively an ID for this tornado in this year.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yr&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Year, 1950-2022.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mo&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Month, 1-12.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;dy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Day of the month, 1-31.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;tz&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_tz_database_time_zones&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Canonical tz database timezone&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime_utc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date and time normalized to UTC.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;st&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stf&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State FIPS (Federal Information Processing Standards) number.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;inj&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of injuries. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of fatalities. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;loss&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;len&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Length in miles.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;wid&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Width in yards.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ns&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of states affected by this tornado. 1, 2, or 3.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;sn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 1st county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 2nd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 3rd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 4th county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;logical&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Was the mag column estimated?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;log_loss.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;The log of the loss attribute&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Explore the distribution of the &lt;code&gt;log_loss&lt;/code&gt; attribute. In a few sentences, summarize key elements of the distribution, for instance discussing elements related to &lt;strong&gt;shape&lt;/strong&gt;, &lt;strong&gt;center&lt;/strong&gt;, &lt;strong&gt;variation&lt;/strong&gt;, and/or &lt;strong&gt;extreme values&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pick a few attributes (i.e., pick your top 2 favorite) that are of the double or integer type and explore the bivariate association between those attributes. That is, estimate what is the bivariate association between the attributes you chose. In a few sentences, summarize key elements of the associations, paying attention to the &lt;strong&gt;strength&lt;/strong&gt; and &lt;strong&gt;direction&lt;/strong&gt; of association. A few attributes could be: &lt;code&gt;len&lt;/code&gt;, &lt;code&gt;wid&lt;/code&gt;, &lt;code&gt;inj&lt;/code&gt;, &lt;code&gt;fat&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore the bivariate associations you picked in #2 visually. Does the relationship appear to be linear for the associations that you picked? Why or why not?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using the &lt;code&gt;log_loss&lt;/code&gt; attribute, compute descriptive statistics for this attribute using one of the following attributes as a grouping attribute:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;mag&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;mo&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;More specifically, compute descriptive statistics for the integer or double outcome for each unique value of either &lt;code&gt;mag&lt;/code&gt; or &lt;code&gt;mo&lt;/code&gt;. If you are using R for this, you may need to force the &lt;code&gt;mag&lt;/code&gt; or &lt;code&gt;mo&lt;/code&gt; attributes to be factors using the &lt;code&gt;factor()&lt;/code&gt; function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Summarize, in a few sentences, any notable differences or similarities in the descriptive statistics computed in #4 across the groups?&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Activity 2</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/activity/activity2/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/activity/activity2/</guid>
      <description>&lt;p&gt;The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. &lt;em&gt;Please make sure to add everyone&amp;rsquo;s name to the submission&lt;/em&gt;, this can be a comment on ICON or on the document itself.&lt;/p&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-turn-in&#34;&gt;What to turn in&lt;/h3&gt;
&lt;p&gt;Please turn in a document that contains the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answers to the questions below&lt;/li&gt;
&lt;li&gt;include any relevant statistics/figures that support your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, upload the final document to &lt;a href=&#34;https://uiowa.instructure.com/courses/214414/assignments/2006007&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICON&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;due-date&#34;&gt;Due Date&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Due around&lt;/em&gt; &lt;strong&gt;October 2nd, 2023&lt;/strong&gt;. No penalty for late submissions as long as it is submitted by December 11th.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this activity comes from the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy Tuesday&lt;/a&gt; project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-08-09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see the Tidy Tuesday page for more information&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The data can be obtained in &lt;a href=&#34;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/tornados.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csv format&lt;/a&gt;. A short description for each attribute is as follows. These data are also found within the &amp;ldquo;data&amp;rdquo; folder inside the IDAS.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;om&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tornado number. Effectively an ID for this tornado in this year.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yr&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Year, 1950-2022.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mo&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Month, 1-12.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;dy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Day of the month, 1-31.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;tz&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_tz_database_time_zones&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Canonical tz database timezone&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime_utc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date and time normalized to UTC.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;st&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stf&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State FIPS (Federal Information Processing Standards) number.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;inj&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of injuries. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of fatalities. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;loss&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;len&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Length in miles.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;wid&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Width in yards.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ns&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of states affected by this tornado. 1, 2, or 3.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;sn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 1st county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 2nd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 3rd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 4th county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;logical&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Was the mag column estimated?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;log_loss.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;The log of the loss attribute&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Does the length or width of the tornado explain significant variation in the log loss of the tornado?&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Refresh your descriptive analyses from the previous activity, if you did not explore the length or width of the tornado, please do that here.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Does the association between length or width of the tornado and log loss of the tornado appear to be linear? Why or why not?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit a linear regression to answer the research question highlighted above. &lt;em&gt;Select only 1 attribute above to start for this activity&lt;/em&gt; Interpret the intercept and slope of the linear regression. That is, what do these terms mean?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the r-squared and sigma estimates from the linear regression? Interpret these two values in the context of the problem. That is, what do these two terms mean in the context of the data and this problem?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, in a couple sentences, provide a summary of the overall model. Does the model appear to be useful to predict or explain variation in the log loss of the tornado with either length or width of the tornado? &lt;em&gt;Use statistics from the analysis steps above to support your answer.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Activity 3</title>
      <link>https://psqf6243.brandonlebeau.org/assignments/activity/activity3/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://psqf6243.brandonlebeau.org/assignments/activity/activity3/</guid>
      <description>&lt;p&gt;The following activity is aimed to give you some practice with exploring data and running a linear regression on your own using statistical software. You are welcome to use any statistical software you wish and you are also free to work in groups of up to 3 for this assignment. If you work in groups, please submit one completed activity per group on ICON. &lt;em&gt;Please make sure to add everyone&amp;rsquo;s name to the submission&lt;/em&gt;, this can be a comment on ICON or on the document itself.&lt;/p&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;h3 id=&#34;what-to-turn-in&#34;&gt;What to turn in&lt;/h3&gt;
&lt;p&gt;Please turn in a document that contains the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;answers to the questions below&lt;/li&gt;
&lt;li&gt;include any relevant statistics/figures that support your answer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally,upload the final document to &lt;a href=&#34;https://uiowa.instructure.com/courses/214414/assignments/2006008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICON&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;due-date&#34;&gt;Due Date&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Due around&lt;/em&gt; &lt;strong&gt;October 16th, 2023&lt;/strong&gt;. No penalty for late submissions as long as it is submitted by December 11th.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The data for this activity comes from the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tidy Tuesday&lt;/a&gt; project. The data contain 19,405 rows and 28 columns about tornados from around the United States between 2007 and 2022. A data description for each column in the data is shown below (&lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-08-09&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see the Tidy Tuesday page for more information&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The data can be obtained in &lt;a href=&#34;https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/tornados.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;csv format&lt;/a&gt;. A short description for each attribute is as follows. These data are also found within the &amp;ldquo;data&amp;rdquo; folder inside the IDAS.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;variable&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;class&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;om&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Tornado number. Effectively an ID for this tornado in this year.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;yr&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Year, 1950-2022.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mo&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Month, 1-12.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;dy&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Day of the month, 1-31.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;date&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;time&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Time.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;tz&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_tz_database_time_zones&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Canonical tz database timezone&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime_utc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;datetime&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Date and time normalized to UTC.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;st&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;character&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Two-letter postal abbreviation for the state (DC = Washington, DC; PR = Puerto Rico; VI = Virgin Islands).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;stf&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State FIPS (Federal Information Processing Standards) number.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;mag&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Magnitude on the F scale (EF beginning in 2007). Some of these values are estimated (see fc).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;inj&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of injuries. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of fatalities. When summing for state totals, use sn == 1 (see below).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;loss&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Estimated property loss information in dollars. Prior to 1996, values were grouped into ranges. The reported number for such years is the maximum of its range.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;slon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Starting longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elat&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending latitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;elon&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Ending longitude in decimal degrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;len&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Length in miles.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;wid&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Width in yards.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ns&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of states affected by this tornado. 1, 2, or 3.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;sn&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;State number for this row. 1 means the row contains the entire track information for this state, 0 means there is at least one more entry for this state for this tornado (om + yr).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f1&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 1st county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f2&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 2nd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f3&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 3rd county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;f4&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;integer&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FIPS code for the 4th county.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;fc&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;logical&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Was the mag column estimated?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;log_loss.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;double.&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;The log of the loss attribute&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;guiding-question&#34;&gt;Guiding Question&lt;/h2&gt;
&lt;p&gt;Does the length or width of the tornado explain significant variation in the log loss of the tornado?&lt;/p&gt;
&lt;h2 id=&#34;questions&#34;&gt;Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;This assignment uses the same data from the &lt;a href=&#34;https://psqf6243.brandonlebeau.org/assignments/activity/activity2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second activity&lt;/a&gt;. Take a few minutes to reacquaint yourself with this analysis, including the linear regression estimates obtained.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Was the intercept from the linear regression fitted in activity 2 using the raw data as it was collected interpretable in the context of the data? Rephrasing slightly, could the intercept term be made more interpretable? If so, how?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fit a modified linear regression that performs a centering. That is, center the distance in some fashion. Options could include, minimum, mean, median, maximum, etc centering. What changed for the estimates obtained from the model?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which model, the centered or the uncentered, do you feel has a stronger justification for its usage? Be as specific as possible in your rationale.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use whichever model you feel is the best from #3, then extract from software or compute the standard errors for the estimated regression coefficients. Interpret these standard errors in the context of the problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute confidence intervals for the two regression coefficients. Justify your choice for level of confidence and interpret the confidence interval.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, perform hypothesis testing for the two regression coefficients. In this, set up the null/alternative hypotheses and then interpret the statistical results to provide a statistical conclusion. More specifically, what does the hypothesis test suggest about the null and alternative hypotheses?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Question 6 talked about the statistical conclusions, now compare/contrast this with practical conclusions. More specifically, would these results be useful in practice? Be as specific as possible in your response.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
